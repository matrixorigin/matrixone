# do not change this part {
parameter-struct-name = "SystemVariables"
config-struct-name = "varsConfig"

operation-file-name = "system_vars"

config-file-name = "system_vars_config"
# }

[[parameter]]
name = "rootname"
scope = ["global"]
access = ["file"]
type = "string"
domain-type = "set"
values = ["root"]
comment = "root name"
update-mode = "fix"

[[parameter]]
name = "rootpassword"
scope = ["global"]
access = ["file"]
type = "string"
domain-type = "set"
values = [""]
comment = "root password"
update-mode = "dynamic"

[[parameter]]
name = "dumpuser"
scope = ["global"]
access = ["file"]
type = "string"
domain-type = "set"
values = ["dump"]
comment = "dump user name"
update-mode = "fix"

[[parameter]]
name = "dumppassword"
scope = ["global"]
access = ["file"]
type = "string"
domain-type = "set"
values = ["111"]
comment = "dump user password"
update-mode = "fix"

[[parameter]]
name = "dumpdatabase"
scope = ["global"]
access = ["file"]
type = "string"
domain-type = "set"
values = ["default"]
comment = "dump database name"
update-mode = "dynamic"

[[parameter]]
name = "port"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "range"
values = ["6001", "6001", "6010"]
comment = "port defines which port the mo-server listens on and clients connect to"
update-mode = "dynamic"

[[parameter]]
name = "host"
scope = ["global"]
access = ["file"]
type = "string"
domain-type = "set"
values = ["0.0.0.0","localhost","127.0.0.1"]
comment = "listening ip"
update-mode = "dynamic"

[[parameter]]
name = "sendRow"
scope = ["global"]
access = ["file"]
type = "bool"
domain-type = "set"
values = []
comment = "send data row while producing"
update-mode = "dynamic"

[[parameter]]
name = "dumpEnv"
scope = ["global"]
access = ["file"]
type = "bool"
domain-type = "set"
values = []
comment = "dump Environment with memEngine Null nodes for testing"
update-mode = "dynamic"

[[parameter]]
name = "hostMmuLimitation"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "set"
values = ["1099511627776"]
comment = "host mmu limitation. default: 1 << 40 = 1099511627776"
update-mode = "dynamic"

[[parameter]]
name = "guestMmuLimitation"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "set"
values = ["1099511627776"]
comment = "guest mmu limitation. default: 1 << 40 = 1099511627776"
update-mode = "dynamic"

[[parameter]]
name = "mempoolMaxSize"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "set"
values = ["1099511627776"]
comment = "mempool maxsize. default: 1 << 40 = 1099511627776"
update-mode = "dynamic"

[[parameter]]
name = "mempoolFactor"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "set"
values = ["8"]
comment = "mempool factor. default: 8"
update-mode = "dynamic"

[[parameter]]
name = "processLimitationSize"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "set"
values = ["42949672960"]
comment = "process.Limitation.Size. default: 10 << 32 = 42949672960"
update-mode = "dynamic"

[[parameter]]
name = "processLimitationBatchRows"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "set"
values = ["42949672960"]
comment = "process.Limitation.BatchRows. default: 10 << 32 = 42949672960"
update-mode = "dynamic"

[[parameter]]
name = "processLimitationBatchSize"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "set"
values = ["0"]
comment = "process.Limitation.BatchSize. default: 0"
update-mode = "dynamic"

[[parameter]]
name = "processLimitationPartitionRows"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "set"
values = ["42949672960"]
comment = "process.Limitation.PartitionRows. default: 10 << 32 = 42949672960"
update-mode = "dynamic"

[[parameter]]
name = "countOfRowsPerSendingToClient"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "range"
values = ["1000","1","10000"]
comment = "send the count of rows to the client"
update-mode = "dynamic"

[[parameter]]
name = "periodOfEpochTimer"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "set"
values = ["5"]
comment = "the period of epoch timer in second"
update-mode = "dynamic"

[[parameter]]
name = "periodOfPersistence"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "set"
values = ["20"]
comment = "the period of persistence in second"
update-mode = "dynamic"

[[parameter]]
name = "periodOfDDLDeleteTimer"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "set"
values = ["20"]
comment = "the period of the ddl delete in second"
update-mode = "dynamic"

[[parameter]]
name = "timeoutOfHeartbeat"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "set"
values = ["20"]
comment = "the timeout of heartbeat in second. In a distributed setting, adjust this setting according to you network status. For poor network connections, set this value larger."
update-mode = "dynamic"

[[parameter]]
name = "rejectWhenHeartbeatFromPDLeaderIsTimeout"
scope = ["global"]
access = ["file"]
type = "bool"
domain-type = "set"
values = ["false"]
comment = "default value is false. the server will reject the connection and sql request when the heartbeat from pdleader is timeout."
update-mode = "dynamic"

[[parameter]]
name = "enableEpochLogging"
scope = ["global"]
access = ["file"]
type = "bool"
domain-type = "set"
values = ["false"]
comment = "default is false. Print logs when the server calls catalog service to run the ddl."
update-mode = "dynamic"

[[parameter]]
name = "recordTimeElapsedOfSqlRequest"
scope = ["global"]
access = ["file"]
type = "bool"
domain-type = "set"
values = ["true"]
comment = "record the time elapsed of executing sql request"
update-mode = "dynamic"

[[parameter]]
name = "nodeID"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "range"
values = ["0", "0", "10"]
comment = "the Node ID of the cube. In a distributed setting, each node in a cluster should have a different nodeID."
update-mode = "dynamic"

[[parameter]]
name = "storePath"
scope = ["global"]
access = ["file"]
type = "string"
domain-type = "set"
values = ["./store"]
comment = "the root directory of the storage and matrixcube's data. The actual dir is cubeDirPrefix + nodeID"
update-mode = "dynamic"


[[parameter]]
name = "lengthOfQueryPrinted"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "range"
values = ["50", "-1", "10000"]
comment = "the length of query printed into console. -1, complete string. 0, empty string. >0 , length of characters at the header of the string."
update-mode = "dynamic"

[[parameter]]
name = "batchSizeInLoadData"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "range"
values = ["40000","10","100000"]
comment = "the count of rows in vector of batch in load data"
update-mode = "dynamic"

[[parameter]]
name = "loadDataConcurrencyCount"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "range"
values = ["16","1","16"]
comment = "default is 16. The count of go routine writing batch into the storage."
update-mode = "dynamic"

[[parameter]]
name = "cubeLogLevel"
scope = ["global"]
access = ["file"]
type = "string"
domain-type = "set"
values = ["debug","info","error","warning","warn","fatal"]
comment = "default is debug. The log level for cube."
update-mode = "dynamic"

[[parameter]]
name = "enableProfileGetDataFromPipeline"
scope = ["global"]
access = ["file"]
type = "bool"
domain-type = "set"
values = []
comment = "defult is false. true for profiling the getDataFromPipeline"
update-mode = "dynamic"

[[parameter]]
name = "maxBytesInOutbufToFlush"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "range"
values = ["1024","32","3096"]
comment = "KB. When the number of bytes in the outbuffer exceeds the it,the outbuffer will be flushed."
update-mode = "dynamic"

[[parameter]]
name = "cubeMaxEntriesBytes"
scope = ["global"]
access = ["file"]
type = "int64"
domain-type = "set"
values = ["314572800"]
comment = "default is 300MB. The max entries bytes for the write batch in the cube."
update-mode = "dynamic"

# Cluster Configs
pre-allocated-group-num = 20
max-group-num           = 0

# Logger Configs
level = "debug" # debug, info, warn, error, fatal.
format = "json" # json, console.

# log file config
filename = "" # log file.
max-size = 512 # maximum log file size.
max-days = 0 # maximum log file days kept.
max-backups = 0 # maximum numbers of old log files to retain.

# Cube Configs
# In a distributed setting, the ip of addr-raft and addr-client should set to the ip address of the machine that mo-server runs on.
# Docker or NAT network environment configuration
# If a client cannot access Cube through the default client URLs listened to by Cube, you must manually set the advertise client URLs
# For example, addr-raft = "0.0.0.0:10000", addr-advertise-raft = "${HOST}:10000". The same to addr-client.
addr-raft = "localhost:10000"                       # the address for raft-group rpc communication
addr-advertise-raft = ""                            # for docker deployment
addr-client = "localhost:20000"                     # the address for cube service
addr-advertise-client = ""                          # for docker deployment
dir-deploy = ""                                     # TODO: ...
version = ""                                        # cube version number
githash = ""                                        # cube git hash
capacity = 0                                        # the maximum storage capacity of this node
use-memory-as-storage = false                       # if this parameter is true, the data will be stored in memory and hence volatile
shard-groups = 1                                    # TODO: ...
dir-data = ""                                       # directory for cube data
labels = [
    ["zone", "default-zone"],
    ["rack", "rack-0"],
    ["host", "node-0"],
]                                                   # label information of this node
# Replication Configs
[replication]
# If pb scheduler doesn't hear from a node for longer than max-peer-down-time, it will ask this node to destroy itself if it comes back.
max-peer-down-time = "30m"                          # the maximum time a node is allowed to leave
shard-heartbeat-duration = "10s"                    # the period for this raft group to report information to scheduler
store-heartbeat-duration = "10s"                     # the period for this node to report information to scheduler
shard-split-check-duration = "30s"                  # the period of shard split checking
shard-state-check-duration = "1m"                   # the period of shard state checking
compact-log-check-duration = "60s"                  # the period of log compaction checking
disable-shard-split = false                         # to disable shard split, set this value to true
allow-remove-leader = false                         # to allow leader removal, set this value to true
shard-capacity-bytes = "96MB"                       # shard capacity
shard-split-check-bytes = "64MB"                    # shard storage split threshold

[snapshot]
max-concurrency-snap-chunks = 8
snap-chunk-size = "4MB"

[raft]
enable-pre-vote = true                              # TODO:
tick-interval = "1s"                                # In a distributed setting, adjust this setting according to you network status. For poor network connections, set this value larger.
heartbeat-ticks = 2                                 # how many ticks one raft heartbeat has
election-timeout-ticks = 10                         # how many ticks a election timeout has
max-size-per-msg = 0                                # maximum number of raft log entry one raft message has
max-inflight-msgs = 512                             # maximum number of raft message on the way of sending(inflight)
max-entry-bytes = 314572800                         # maximum bytes for one cube write proposal
send-raft-batch-size = 64                           # TODO:

[raft.raft-log]
disable-sync = false                                # sync raft log when writing to disk
compact-duration = "30s"                            # TODO:
compact-threshold = 256                             # the threshold for raft log cleaning
max-allow-transfer-lag = 2                          # when raft group transfer leadership, the maximum log lag the next leader can have
CompactProtectLag = 0                               # TODO:
force-compact-log-count = ""                        # the log count for raft leader to force compaction
force-compact-log-bytes = ""                        # the log size for raft leader to force compaction

[worker]
raft-apply-worker = 32                              # TODO:
raft-msg-worker = 8                                 # TODO:
raft-event-worker = 32                              # the number of raft event worker for a shard

# Prophet Configs
[prophet]
# In a distributed setting, there are three nodes act as prophet, each of which should have different names.
name = "node0"
data-dir = ""                                       # TODO:
# RPC port for other client to access prophet.
# Docker or NAT network environment configuration
# For exmaple, rpc-addr = "0.0.0.0:30000", rpc-advertise-addr = "${HOST}:30000"
rpc-addr = "localhost:30000"
rpc-advertise-addr = ""
# In a distributed setting, adjust this setting according to you network status. For poor network connections, set this value larger.
rpc-timeout = "10s"
# In a distributed setting, if a node is not a prophet node(i.e., a pure storage node), set the value storage-node = false.
storage-node = true
# In a distributed setting, if a node is a prophet node, the value external-etcd should be empty.
# If a node is not a prophet node(i.e., the above setting storage-node = false), the value of external-etcd should be the three prophet node's prophet.embed-etcd's client-urls.
external-etcd = ["", "", ""]
# If prophet cluster have 3 nodes (also prophet cluster should have 3 nodes at least), lease should be 3. external-etcd should config nodes advertise-client-urls.
lease = 0

# In a distributed setting, only the three prophet nodes need to adjust this setting
[prophet.embed-etcd]
# For the genesis node in the three prophet, the join value should remain a empty string.
# For the other two nodes in the prophet group, the join value should set to the genesis node's peer-urls.
# see more: https://etcd.io/docs/v3.5/op-guide/clustering/
join = ""
# In a distributed setting, change the localhost to the machine ip to expose the client-urls to other nodes in the cluster.
# When you deploy a cluster, you must specify the IP address of the current host as client-urls (for example, "http://192.168.100.214:40000"). 
# If the cluster runs on Docker, specify the IP address of Docker as "http://0.0.0.0:40000"
client-urls = "http://localhost:40000"
# In a distributed setting, change the localhost to the machine ip to expose the client-urls to other nodes in the cluster.
# The list of peer URLs to be listened to by a prophet node
peer-urls = "http://localhost:50000"
# In some situations such as in the Docker or NAT network environment, 
# if a client cannot access prophet through the default client URLs listened to by prophet, you must manually set the advertise client URLs
# For example, client-url = "http://0.0.0.0:40000", advertise-client-urls = "http://${HOST}:40000", The same to peer-urls.
advertise-client-urls = ""
advertise-peer-urls = ""
initial-cluster = ""                                # TODO:
initial-cluster-state = ""                          # TODO:
tick-interval = "500ms"                             # prophet raft group tick interval
election-interval = "3000ms"                        # prophet raft group election interval
enable-prevote = true                               # enable prophet raft group pre-vote
auto-compaction-mode = "periodic"                   # prophet raft log auto-compaction-mode
auto-compaction-retention = "1h"                    # prophet raft log auto-compaction-retention time
quota-backend-bytes = "8GB"                         # etcd quota backend bytes

[prophet.schedule]
max-snapshot-count = 3                              # maximum snapshot a node that can be scheduled by prophet is handling
max-pending-peer-count = 16                         # maximum pending peers a node that can be scheduled can have
max-merge-resource-size = 0                         # TODO:
max-merge-resource-keys = 0                         # TODO:
split-merge-interval = "0s"                         # TODO:
enable-one-way-merge = false                        # TODO:
enable-cross-table-merge = false                    # TODO:
patrol-resource-interval = "100ms"                  # the period of cube leader patrol
max-container-down-time = "30m"                     # the maximum time a node is allowed to leave
leader-schedule-limit = 4                           # the maximum number of transfer leader
leader-schedule-policy = "count"                    # the rebalance scheduling policy, (count | size), count: every node has the same raft leader. size: every raft leader on a node manage the same data size
resource-schedule-limit = 2048                      # the maximum resource schedule operation can happen simultaneously
replica-schedule-limit = 64                         # the maximum resource replication operation can happen simultaneously
merge-schedule-limit = 0                            # TODO:
hot-resource-schedule-limit = 4                     # the maximum hot resource schedule operation can happen simultaneously
hot-resource-cache-hits-threshold = 3               # threshold to determine whether a shard is hot
tolerant-size-ratio = 0.0                           # TODO:
low-space-ratio = 0.8                               # space ratio to decide whether this node's storage is low
high-space-ratio = 0.7                              # space ratio to decide whether this node's storage is high
resource-score-formula-version = ""                 # TODO:
scheduler-max-waiting-operator = 5                  # the maximum number of waiting state a scheduling operation can have
enable-remove-down-replica = true                   # enable cube scheduler to remove down replica in a cluster
enable-replace-offline-replica = true               # enable cube scheduler to replace offline replica
enable-make-up-replica = true                       # enable cube scheduler to create replica when replica number is short
enable-remove-extra-replica = true                  # enable cube scheduler to remove redundant replica
enable-location-replacement = true                  # enable cube scheduler to relocate replica
enable-debug-metrics = false                        # TODO:
enable-joint-consensus = true                       # TODO:
container-limit-mode = ""                           # TODO:

[prophet.replication]
max-replicas = 1                                    # max number of replica in a prophet group
location-labels = "zone,rack"                       # label information of this prophet node
strictly-match-label = true                         # if this parameter is set to true, `location-labels` must be in the store label set
enable-placement-rules = true                       # enable scheduling based on rule
isolation-level = "rack"                            # replicas isolation level

[metric]
addr = ""                                           # cube uses prometheus push to send metrics, this address is the prometheus-gateway address
interval = 0                                        # the period to send cube metrics
job = ""                                            # prometheus job
instance = ""                                       # prometheus instance

# Storage Configs
[meta.conf]
block-max-rows = 160000
segment-max-blocks = 40

[scheduler-cfg]
block-writers = 8
segment-writers = 4

[cache-cfg]
index-cache-size = 134217728        # 128M
insert-cache-size = 4294967296      # 4G
data-cache-size = 4294967296        # 4G
