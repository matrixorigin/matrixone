# Change this config file according to your usage.
#
# These values must be different from each other:
# port
# nodeID
# prophetEmbedEtcdJoinAddr
#
# addr-raft
# addr-client
# dir-data
#[prophet]
#   name
#   rpc-addr
#   external-etcd
#   [prophet.embed-etcd]
#       client-urls
#       peer-urls
#
#To set up matrixone on a single node, there is no need to change this config file
#
#To set up matrixone on a distributed cluster, change this config file according to the following instruction
#    1. Requirements: at least three nodes
#
#    2. set up the prophet genesis node
#        2.1. make sure the nodeID is unique
#        2.2. change the addr-raft ip to the machine ip
#        2.3. change the addr-client ip to the machine ip
#        2.4. make sure the prophet name is different from the names of other two prophet node
#        2.5. change the rpc-addr ip to the machine ip
#        2.6. change the client-urls ip to the machine ip
#        2.7. change the peer-urls ip to the machine ip
#        2.8. make sure the dir-data is different from the other nodes in the cluster
#
#    3. set up the other two prophet nodes
#        3.1. apply the above 8 steps of prophet genesis node setting
#        3.2. change the prophet join address from empty string to the prophet genesis node's peer-urls
#   
#    4. set up pure prophet node
#        4.1. make sure the nodeID is unique
#        4.2. change prophet-node to false
#        4.3. change the addr-raft ip to the machine ip
#        4.4. change the addr-client ip to the machine ip
#        4.5. In the external-etcd attribute, fill the three empty string with the three client-urls of the three prophet node
#        4.6. make sure the dir-data is different from the other nodes in the cluster
#
#Start MatrixOne cluster on docker or kubernetes, please refer to this repo [matrixorigin/matrixone-operator](https://github.com/matrixorigin/matrixone-operator)




#	Name:	rootpassword
#	Scope:	[global]
#	Access:	[file]
#	DataType:	string
#	DomainType:	set
#	Values:	[]
#	Comment:	root password
#	UpdateMode:	dynamic
	rootpassword = ""

#	Name:	dumpdatabase
#	Scope:	[global]
#	Access:	[file]
#	DataType:	string
#	DomainType:	set
#	Values:	[default]
#	Comment:	dump database name
#	UpdateMode:	dynamic
	dumpdatabase = "default"

#	Name:	port
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	range
#	Values:	[6001 6001 6010]
#	Comment:	port defines which port the mo-server listens on and clients connect to
#	UpdateMode:	dynamic
	port = 6001

#	Name:	host
#	Scope:	[global]
#	Access:	[file]
#	DataType:	string
#	DomainType:	set
#	Values:	[0.0.0.0 localhost 127.0.0.1]
#	Comment:	listening ip
#	UpdateMode:	dynamic
	host = "0.0.0.0"

#	Name:	sendRow
#	Scope:	[global]
#	Access:	[file]
#	DataType:	bool
#	DomainType:	set
#	Values:	[]
#	Comment:	send data row while producing
#	UpdateMode:	dynamic
	sendRow = false

#	Name:	dumpEnv
#	Scope:	[global]
#	Access:	[file]
#	DataType:	bool
#	DomainType:	set
#	Values:	[]
#	Comment:	dump Environment with memEngine Null nodes for testing
#	UpdateMode:	dynamic
	dumpEnv = false

#	Name:	hostMmuLimitation
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[1099511627776]
#	Comment:	host mmu limitation. default: 1 << 40 = 1099511627776
#	UpdateMode:	dynamic
	hostMmuLimitation = 1099511627776

#	Name:	guestMmuLimitation
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[1099511627776]
#	Comment:	guest mmu limitation. default: 1 << 40 = 1099511627776
#	UpdateMode:	dynamic
	guestMmuLimitation = 1099511627776

#	Name:	mempoolMaxSize
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[1099511627776]
#	Comment:	mempool maxsize. default: 1 << 40 = 1099511627776
#	UpdateMode:	dynamic
	mempoolMaxSize = 1099511627776

#	Name:	mempoolFactor
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[8]
#	Comment:	mempool factor. default: 8
#	UpdateMode:	dynamic
	mempoolFactor = 8

#	Name:	processLimitationSize
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[42949672960]
#	Comment:	process.Limitation.Size. default: 10 << 32 = 42949672960
#	UpdateMode:	dynamic
	processLimitationSize = 42949672960

#	Name:	processLimitationBatchRows
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[42949672960]
#	Comment:	process.Limitation.BatchRows. default: 10 << 32 = 42949672960
#	UpdateMode:	dynamic
	processLimitationBatchRows = 42949672960

#	Name:	processLimitationBatchSize
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[0]
#	Comment:	process.Limitation.BatchSize. default: 0
#	UpdateMode:	dynamic
	processLimitationBatchSize = 0

#	Name:	processLimitationPartitionRows
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[42949672960]
#	Comment:	process.Limitation.PartitionRows. default: 10 << 32 = 42949672960
#	UpdateMode:	dynamic
	processLimitationPartitionRows = 42949672960

#	Name:	countOfRowsPerSendingToClient
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	range
#	Values:	[1000 1 10000]
#	Comment:	send the count of rows to the client
#	UpdateMode:	dynamic
	countOfRowsPerSendingToClient = 1000

#	Name:	periodOfEpochTimer
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[5]
#	Comment:	the period of epoch timer in second
#	UpdateMode:	dynamic
	periodOfEpochTimer = 5

#	Name:	periodOfPersistence
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[20]
#	Comment:	the period of persistence in second
#	UpdateMode:	dynamic
	periodOfPersistence = 20

#	Name:	periodOfDDLDeleteTimer
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[20]
#	Comment:	the period of the ddl delete in second
#	UpdateMode:	dynamic
	periodOfDDLDeleteTimer = 20

#	Name:	timeoutOfHeartbeat
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[20]
#	Comment:	the timeout of heartbeat in second. In a distributed setting, adjust this setting according to you network status. For poor network connections, set this value larger.
#	UpdateMode:	dynamic
	timeoutOfHeartbeat = 20

#	Name:	rejectWhenHeartbeatFromPDLeaderIsTimeout
#	Scope:	[global]
#	Access:	[file]
#	DataType:	bool
#	DomainType:	set
#	Values:	[false]
#	Comment:	default value is false. the server will reject the connection and sql request when the heartbeat from pdleader is timeout.
#	UpdateMode:	dynamic
	rejectWhenHeartbeatFromPDLeaderIsTimeout = false

#	Name:	enableEpochLogging
#	Scope:	[global]
#	Access:	[file]
#	DataType:	bool
#	DomainType:	set
#	Values:	[false]
#	Comment:	default is false. Print logs when the server calls catalog service to run the ddl.
#	UpdateMode:	dynamic
	enableEpochLogging = false

#	Name:	recordTimeElapsedOfSqlRequest
#	Scope:	[global]
#	Access:	[file]
#	DataType:	bool
#	DomainType:	set
#	Values:	[true]
#	Comment:	record the time elapsed of executing sql request
#	UpdateMode:	dynamic
	recordTimeElapsedOfSqlRequest = true

#	Name:	nodeID
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	range
#	Values:	[0 0 10]
#	Comment:	the Node ID of the cube. In a distributed setting, each node in a cluster should have a different nodeID.
#	UpdateMode:	dynamic
	nodeID = 0

#	Name:	storePath
#	Scope:	[global]
#	Access:	[file]
#	DataType:	string
#	DomainType:	set
#	Values:	[./store]
#	Comment:	the root directory of the storage and matrixcube's data. The actual dir is cubeDirPrefix + nodeID
#	UpdateMode:	dynamic
	storePath = "./store"

#	Name:	lengthOfQueryPrinted
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	range
#	Values:	[50 -1 10000]
#	Comment:	the length of query printed into console. -1, complete string. 0, empty string. >0 , length of characters at the header of the string.
#	UpdateMode:	dynamic
	lengthOfQueryPrinted = 50

#	Name:	batchSizeInLoadData
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	range
#	Values:	[40000 10 100000]
#	Comment:	the count of rows in vector of batch in load data
#	UpdateMode:	dynamic
	batchSizeInLoadData = 40000

#	Name:	loadDataConcurrencyCount
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	range
#	Values:	[16 1 16]
#	Comment:	default is 16. The count of go routine writing batch into the storage.
#	UpdateMode:	dynamic
	loadDataConcurrencyCount = 16

#	Name:	cubeLogLevel
#	Scope:	[global]
#	Access:	[file]
#	DataType:	string
#	DomainType:	set
#	Values:	[debug info error warning warn fatal]
#	Comment:	default is debug. The log level for cube.
#	UpdateMode:	dynamic
	cubeLogLevel = "debug"

#	Name:	enableProfileGetDataFromPipeline
#	Scope:	[global]
#	Access:	[file]
#	DataType:	bool
#	DomainType:	set
#	Values:	[]
#	Comment:	defult is false. true for profiling the getDataFromPipeline
#	UpdateMode:	dynamic
	enableProfileGetDataFromPipeline = false

#	Name:	maxBytesInOutbufToFlush
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	range
#	Values:	[1024 32 3096]
#	Comment:	KB. When the number of bytes in the outbuffer exceeds the it,the outbuffer will be flushed.
#	UpdateMode:	dynamic
	maxBytesInOutbufToFlush = 1024

#	Name:	cubeMaxEntriesBytes
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[314572800]
#	Comment:	default is 300MB. The max entries bytes for the write batch in the cube.
#	UpdateMode:	dynamic
	cubeMaxEntriesBytes = 314572800

#	Name:	printLogInterVal
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	range
#	Values:	[10 1 1000]
#	Comment:	default printLog Interval is 10s.
#	UpdateMode:	dynamic
	printLogInterVal = 10

#	Name:	exportDataDefaultFlushSize
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[1 2 4 8]
#	Comment:	export data to csv file default flush size
#	UpdateMode:	dynamic
	exportDataDefaultFlushSize = 1

#	Name:	disablePCI
#	Scope:	[global]
#	Access:	[file]
#	DataType:	bool
#	DomainType:	set
#	Values:	[]
#	Comment:	default is false. Disable epochgc pci.
#	UpdateMode:	dynamic
	disablePCI = false

#	Name:	enableTpe
#	Scope:	[global]
#	Access:	[file]
#	DataType:	bool
#	DomainType:	set
#	Values:	[]
#	Comment:	default is false. Enable transactional processing engine.
#	UpdateMode:	dynamic
	enableTpe = false

# Cluster Configs
pre-allocated-group-num = 20
max-group-num           = 0

# Logger Configs
level = "debug" # debug, info, warn, error, fatal.
format = "json" # json, console.

# log file config
filename = "./log/mo.log" # log file.
max-size = 512 # maximum log file size.
max-days = 1   # maximum log file days kept.
max-backups = 0 # maximum numbers of old log files to retain.

# Cube Configs
# In a distributed setting, the ip of addr-raft and addr-client should set to the ip address of the machine that mo-server runs on.
# Docker or NAT network environment configuration
# If a client cannot access Cube through the default client URLs listened to by Cube, you must manually set the advertise client URLs
# For example, addr-raft = "0.0.0.0:10000", addr-advertise-raft = "${HOST}:10000". The same to addr-client.
addr-raft = "0.0.0.0:10000"
addr-advertise-raft = "HOST:10000"
addr-client = "0.0.0.0:20000"
addr-advertise-client = "HOST:20000"
version = ""                                        # cube version number
githash = ""                                        # cube git hash
capacity = 0                                        # the maximum storage capacity of this node
dir-data = ""                                       # directory for cube data
labels = [
    ["zone", "default-zone"],
    ["rack", "rack-0"],
    ["host", "node-0"],
]                                                   # label information of this node
# Replication Configs
[replication]
# If pb scheduler doesn't hear from a node for longer than max-peer-down-time, it will ask this node to destroy itself if it comes back.
max-peer-down-time = "30m"                          # the maximum time a node is allowed to leave
shard-heartbeat-duration = "10s"                    # the period for this raft group to report information to scheduler
store-heartbeat-duration = "10s"                     # the period for this node to report information to scheduler
shard-state-check-duration = "1m"                   # the period of shard state checking
compact-log-check-duration = "60s"                  # the period of log compaction checking
allow-remove-leader = false                         # to allow leader removal, set this value to true

[raft]
tick-interval = "1s"                                # In a distributed setting, adjust this setting according to you network status. For poor network connections, set this value larger.
heartbeat-ticks = 2                                 # how many ticks one raft heartbeat has
election-timeout-ticks = 10                         # how many ticks a election timeout has
max-size-per-msg = 0                                # maximum number of raft log entry one raft message has
max-inflight-msgs = 512                             # maximum number of raft message on the way of sending(inflight)
max-entry-bytes = 16777216                          # maximum bytes for one cube write proposal

# Prophet Configs
[prophet]
# In a distributed setting, there are three nodes act as prophet, each of which should have different names.
name = "NAME"
# RPC port for other client to access prophet.
# Docker or NAT network environment configuration
# For exmaple, rpc-addr = "0.0.0.0:30000", rpc-advertise-addr = "${HOST}:30000"
rpc-addr = "0.0.0.0:30000"
rpc-advertise-addr = "HOST:30000"
# In a distributed setting, adjust this setting according to you network status. For poor network connections, set this value larger.
rpc-timeout = "10s"
# In a distributed setting, if a node is not a prophet node(i.e., a pure prophet node), set the value prophet-node = false.
prophet-node = true
# In a distributed setting, if a node is a prophet node, the value external-etcd should be empty.
# If a node is not a prophet node(i.e., the above setting prophet-node = false), the value of external-etcd should be the three prophet node's prophet.embed-etcd's client-urls.
external-etcd = ["", "", ""]

# In a distributed setting, only the three prophet nodes need to adjust this setting
[prophet.embed-etcd]
# For the genesis node in the three prophet, the join value should remain a empty string.
# For the other two nodes in the prophet group, the join value should set to the genesis node's peer-urls.
# see more: https://etcd.io/docs/v3.5/op-guide/clustering/
# join = ""
# In a distributed setting, change the localhost to the machine ip to expose the client-urls to other nodes in the cluster.
# When you deploy a cluster, you must specify the IP address of the current host as client-urls (for example, "http://192.168.100.214:40000").
# If the cluster runs on Docker, specify the IP address of Docker as "http://0.0.0.0:40000"
client-urls = "http://0.0.0.0:40000"
# In a distributed setting, change the localhost to the machine ip to expose the client-urls to other nodes in the cluster.
# The list of peer URLs to be listened to by a prophet node
peer-urls = "http://0.0.0.0:50000"
# In some situations such as in the Docker or NAT network environment,
# if a client cannot access prophet through the default client URLs listened to by prophet, you must manually set the advertise client URLs
# For example, client-url = "http://0.0.0.0:40000", advertise-client-urls = "http://${HOST}:40000", The same to peer-urls.
advertise-client-urls = "http://HOST:40000"
advertise-peer-urls = "http://HOST:50000"
initial-cluster = "node0=http://node0:50000,node1=http://node1:50000,node2=http://node2:50000"

[prophet.schedule]
max-snapshot-count = 3                              # maximum snapshot a node that can be scheduled by prophet is handling
max-pending-peer-count = 16                         # maximum pending peers a node that can be scheduled can have
patrol-resource-interval = "100ms"                  # the period of cube leader patrol
max-container-down-time = "30m"                     # the maximum time a node is allowed to leave
leader-schedule-limit = 4                           # the maximum number of transfer leader
resource-schedule-limit = 2048                      # the maximum resource schedule operation can happen simultaneously
replica-schedule-limit = 64                         # the maximum resource replication operation can happen simultaneously
low-space-ratio = 0.8                               # space ratio to decide whether this node's storage is low
high-space-ratio = 0.7                              # space ratio to decide whether this node's storage is high

[prophet.replication]
max-replicas = 3                                    # max number of replica in a prophet group
[location-labels]
zone = "rack"                                       # label information of this prophet node
enable-placement-rules = true                       # enable scheduling based on rule
isolation-level = "rack"                            # replicas isolation level

[metric]
addr = ""                                           # cube uses prometheus push to send metrics, this address is the prometheus-gateway address
interval = 0                                        # the period to send cube metrics
job = ""                                            # prometheus job
instance = ""                                       # prometheus instance

# Storage Configs
[meta.conf]
block-max-rows = 160000                             # the maximum rows of a block
segment-max-blocks = 40                             # the maximum blocks of a segment

[scheduler-cfg]
block-writers = 8                                   # the maximum parallelism of block flushing
segment-writers = 4                                 # the maximum parallelism of segment flushing

[cache-cfg]
index-cache-size = 67108864        # 64M          # index shared cache size
insert-cache-size = 104857600      # 100M            # mutable data shared cache size
data-cache-size = 104857600        # 100M            # immutable data shared cache size

[kv-feature]
# duration to check if the Shard needs to be split
shard-split-check-duration = "30s"
# the size of the data managed by each Shard, beyond which the Shard needs to be split
shard-capacity-bytes = "96MB"
# disable shard split
disable-shard-split = false

[aoe-feature]
# duration to check if the Shard needs to be split
shard-split-check-duration = "1m"
# the size of the data managed by each Shard, beyond which the Shard needs to be split
shard-capacity-bytes = "4GB"
# disable shard split
disable-shard-split = false