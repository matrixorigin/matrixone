# Code generated by tool; DO NOT EDIT.
#
# These values must be different from each other:
# port
# nodeID
# raftAddrPort
# clientAddrPort
# prophetRPCAddrPort
# prophetClientUrlPort
# prophetPeerUrlPort
#


#	Name:	rootpassword
#	Scope:	[global]
#	Access:	[file]
#	DataType:	string
#	DomainType:	set
#	Values:	[]
#	Comment:	root password
#	UpdateMode:	dynamic
	rootpassword = ""
			
#	Name:	dumpdatabase
#	Scope:	[global]
#	Access:	[file]
#	DataType:	string
#	DomainType:	set
#	Values:	[default]
#	Comment:	dump database name
#	UpdateMode:	dynamic
	dumpdatabase = "default"
			
#	Name:	port
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	range
#	Values:	[6001 6001 6010]
#	Comment:	port
#	UpdateMode:	dynamic
	port = 6001
			
#	Name:	host
#	Scope:	[global]
#	Access:	[file]
#	DataType:	string
#	DomainType:	set
#	Values:	[localhost 127.0.0.1 0.0.0.0]
#	Comment:	listening ip
#	UpdateMode:	dynamic
	host = "localhost"
			
#	Name:	sendRow
#	Scope:	[global]
#	Access:	[file]
#	DataType:	bool
#	DomainType:	set
#	Values:	[]
#	Comment:	send data row while producing
#	UpdateMode:	dynamic
	sendRow = false
	
#	Name:	dumpEnv
#	Scope:	[global]
#	Access:	[file]
#	DataType:	bool
#	DomainType:	set
#	Values:	[]
#	Comment:	dump Environment with memEngine Null nodes for testing
#	UpdateMode:	dynamic
	dumpEnv = false
	
#	Name:	hostMmuLimitation
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[1099511627776]
#	Comment:	host mmu limitation. default: 1 << 40 = 1099511627776
#	UpdateMode:	dynamic
	hostMmuLimitation = 1099511627776
			
#	Name:	guestMmuLimitation
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[1099511627776]
#	Comment:	guest mmu limitation. default: 1 << 40 = 1099511627776
#	UpdateMode:	dynamic
	guestMmuLimitation = 1099511627776
			
#	Name:	mempoolMaxSize
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[1099511627776]
#	Comment:	mempool maxsize. default: 1 << 40 = 1099511627776
#	UpdateMode:	dynamic
	mempoolMaxSize = 1099511627776
			
#	Name:	mempoolFactor
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[8]
#	Comment:	mempool factor. default: 8
#	UpdateMode:	dynamic
	mempoolFactor = 8
			
#	Name:	processLimitationSize
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[42949672960]
#	Comment:	process.Limitation.Size. default: 10 << 32 = 42949672960
#	UpdateMode:	dynamic
	processLimitationSize = 42949672960
			
#	Name:	processLimitationBatchRows
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[42949672960]
#	Comment:	process.Limitation.BatchRows. default: 10 << 32 = 42949672960
#	UpdateMode:	dynamic
	processLimitationBatchRows = 42949672960
			
#	Name:	processLimitationBatchSize
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[0]
#	Comment:	process.Limitation.BatchSize. default: 0
#	UpdateMode:	dynamic
	processLimitationBatchSize = 0
			
#	Name:	processLimitationPartitionRows
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[42949672960]
#	Comment:	process.Limitation.PartitionRows. default: 10 << 32 = 42949672960
#	UpdateMode:	dynamic
	processLimitationPartitionRows = 42949672960
			
#	Name:	countOfRowsPerSendingToClient
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[10]
#	Comment:	send the count of rows to the client
#	UpdateMode:	dynamic
	countOfRowsPerSendingToClient = 10
			
#	Name:	periodOfEpochTimer
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[5]
#	Comment:	the period of epoch timer in second
#	UpdateMode:	dynamic
	periodOfEpochTimer = 5
			
#	Name:	periodOfPersistence
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[20]
#	Comment:	the period of persistence in second
#	UpdateMode:	dynamic
	periodOfPersistence = 20
			
#	Name:	periodOfDDLDeleteTimer
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[20]
#	Comment:	the period of the ddl delete in second
#	UpdateMode:	dynamic
	periodOfDDLDeleteTimer = 20
			
#	Name:	timeoutOfHeartbeat
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	set
#	Values:	[20]
#	Comment:	the timeout of heartbeat in second
#	UpdateMode:	dynamic
	timeoutOfHeartbeat = 20
			
#	Name:	rejectWhenHeartbeatFromPDLeaderIsTimeout
#	Scope:	[global]
#	Access:	[file]
#	DataType:	bool
#	DomainType:	set
#	Values:	[false]
#	Comment:	default is false. the server will reject the connection and sql request when the heartbeat from pdleader is timeout.
#	UpdateMode:	dynamic
	rejectWhenHeartbeatFromPDLeaderIsTimeout = false
			
#	Name:	recordTimeElapsedOfSqlRequest
#	Scope:	[global]
#	Access:	[file]
#	DataType:	bool
#	DomainType:	set
#	Values:	[true]
#	Comment:	record the time elapsed of executing sql request
#	UpdateMode:	dynamic
	recordTimeElapsedOfSqlRequest = true
			
#	Name:	nodeID
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	range
#	Values:	[0 0 10]
#	Comment:	the Node ID of the cube
#	UpdateMode:	dynamic
	nodeID = 0
			
#	Name:	cubeDir
#	Scope:	[global]
#	Access:	[file]
#	DataType:	string
#	DomainType:	set
#	Values:	[./cube]
#	Comment:	the root direction of the cube
#	UpdateMode:	dynamic
	cubeDir = "./cube"
			
#	Name:	prophetEmbedEtcdJoinAddr
#	Scope:	[global]
#	Access:	[file]
#	DataType:	string
#	DomainType:	set
#	Values:	[http://localhost:40000 http://127.0.0.1:40000]
#	Comment:	the join address of prophet of the cube
#	UpdateMode:	dynamic
	prophetEmbedEtcdJoinAddr = "http://localhost:40000"
			
#	Name:	maxReplicas
#	Scope:	[global]
#	Access:	[file]
#	DataType:	int64
#	DomainType:	range
#	Values:	[1 1 1]
#	Comment:	the number of replicas for each resource
#	UpdateMode:	dynamic
	maxReplicas = 1
			
addr-raft = "localhost:10000"
addr-client = "localhost:20000"
dir-data = "./cube0/node"
dir-deploy = ""
version = ""
githash = ""
capacity = 0
use-memory-as-storage = false
shard-groups = 0

[replication]
    max-peer-down-time = "0s"
    shard-heartbeat-duration = "100ms"
    store-heartbeat-duration = "1s"
    shard-split-check-duration = "0s"
    shard-state-check-duration = "0s"
    disable-shard-split = false
    allow-remove-leader = false
    shard-capacity-bytes = 0
    shard-split-check-bytes = 0

[snapshot]
    max-concurrency-snap-chunks = 0
    snap-chunk-size = 0

[raft]
    enable-pre-vote = false
    tick-interval = "600ms"
    heartbeat-ticks = 0
    election-timeout-ticks = 0
    max-size-per-msg = 0
    max-inflight-msgs = 0
    max-entry-bytes = 314572800
    send-raft-batch-size = 0
    [raft.raft-log]
        disable-sync = false
        compact-duration = "0s"
        compact-threshold = 0
        max-allow-transfer-lag = 0
        ForceCompactCount = 0
        ForceCompactBytes = 0
        CompactProtectLag = 0

[worker]
    raft-apply-worker = 0
    raft-msg-worker = 0
    raft-event-workers = 0

[prophet]
    name = "node0"
    data-dir = ""
    rpc-addr = "localhost:30000"
    rpc-timeout = "0s"
    storage-node = true
    external-etcd = [""]
    lease = 0

    [prophet.embed-etcd]
        join = ""
        client-urls = "http://localhost:40000"
        peer-urls = "http://localhost:50000"
        advertise-client-urls = ""
        advertise-peer-urls = ""
        initial-cluster = ""
        initial-cluster-state = ""
        tick-interval = "0s"
        election-interval = "0s"
        enable-prevote = false
        auto-compaction-mode = ""
        auto-compaction-retention = ""
        quota-backend-bytes = 0

    [prophet.schedule]
        max-snapshot-count = 0
        max-pending-peer-count = 0
        max-merge-resource-size = 0
        max-merge-resource-keys = 0
        split-merge-interval = "0s"
        enable-one-way-merge = false
        enable-cross-table-merge = false
        patrol-resource-interval = "0s"
        max-container-down-time = "0s"
        leader-schedule-limit = 0
        leader-schedule-policy = ""
        resource-schedule-limit = 0
        replica-schedule-limit = 0
        merge-schedule-limit = 0
        hot-resource-schedule-limit = 0
        hot-resource-cache-hits-threshold = 0
        tolerant-size-ratio = 0.0
        low-space-ratio = 0.0
        high-space-ratio = 0.0
        resource-score-formula-version = ""
        scheduler-max-waiting-operator = 0
        enable-remove-down-replica = false
        enable-replace-offline-replica = false
        enable-make-up-replica = false
        enable-remove-extra-replica = false
        enable-location-replacement = false
        enable-debug-metrics = false
        enable-joint-consensus = true
        container-limit-mode = ""

    [prophet.replication]
        max-replicas = 1
        strictly-match-label = false
        enable-placement-rules = false
        isolation-level = ""

[metric]
    addr = ""
    interval = 0
    job = ""
    instance = ""

# Cluster Configs
pre-allocated-group-num = 20
max-group-num           = 0
