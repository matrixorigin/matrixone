// Copyright 2024 Matrix Origin
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package test

import (
	"context"
	"reflect"
	"testing"
	"time"

	"github.com/matrixorigin/matrixone/pkg/catalog"
	"github.com/matrixorigin/matrixone/pkg/container/batch"
	"github.com/matrixorigin/matrixone/pkg/container/types"
	"github.com/matrixorigin/matrixone/pkg/container/vector"
	"github.com/matrixorigin/matrixone/pkg/defines"
	"github.com/matrixorigin/matrixone/pkg/objectio"
	"github.com/matrixorigin/matrixone/pkg/sql/colexec"
	"github.com/matrixorigin/matrixone/pkg/txn/client"
	"github.com/matrixorigin/matrixone/pkg/util/fault"
	"github.com/matrixorigin/matrixone/pkg/vm/engine/disttae"
	catalog2 "github.com/matrixorigin/matrixone/pkg/vm/engine/tae/catalog"
	"github.com/matrixorigin/matrixone/pkg/vm/engine/tae/containers"
	testutil2 "github.com/matrixorigin/matrixone/pkg/vm/engine/tae/db/testutil"
	"github.com/matrixorigin/matrixone/pkg/vm/engine/tae/testutils/config"
	"github.com/matrixorigin/matrixone/pkg/vm/engine/test/testutil"
	"github.com/matrixorigin/matrixone/pkg/vm/process"
	"github.com/stretchr/testify/require"
)

func TestStarCountBasic(t *testing.T) {
	catalog.SetupDefines("")

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	ctx = context.WithValue(ctx, defines.TenantIDKey{}, catalog.System_Account)

	disttaeEngine, taeHandler, rpcAgent, _ := testutil.CreateEngines(ctx, testutil.TestOptions{}, t)
	defer func() {
		disttaeEngine.Close(ctx)
		taeHandler.Close(true)
		rpcAgent.Close()
	}()

	ctx, cancel = context.WithTimeout(ctx, time.Minute)
	defer cancel()

	// Create database
	txn, err := disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	err = disttaeEngine.Engine.Create(ctx, "testdb", txn)
	require.NoError(t, err)

	db, err := disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	// Create table
	schema := catalog2.MockSchemaAll(3, 0)
	schema.Name = "test_table"
	defs, err := testutil.EngineTableDefBySchema(schema)
	require.NoError(t, err)

	err = db.Create(ctx, "test_table", defs)
	require.NoError(t, err)

	err = txn.Commit(ctx)
	require.NoError(t, err)

	// Test 1: Empty table
	txn, err = disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	db, err = disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	rel, err := db.Relation(ctx, "test_table", nil)
	require.NoError(t, err)

	count, err := rel.StarCount(ctx)
	require.NoError(t, err)
	require.Equal(t, uint64(0), count, "Empty table should have 0 rows")

	err = txn.Commit(ctx)
	require.NoError(t, err)

	// Test 2: Insert and count
	txn, err = disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	db, err = disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	rel, err = db.Relation(ctx, "test_table", nil)
	require.NoError(t, err)

	// Insert 100 rows
	bat := catalog2.MockBatch(schema, 100)
	err = rel.Write(ctx, containers.ToCNBatch(bat))
	require.NoError(t, err)

	err = txn.Commit(ctx)
	require.NoError(t, err)

	// Test 2: Count after commit
	txn, err = disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	db, err = disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	rel, err = db.Relation(ctx, "test_table", nil)
	require.NoError(t, err)

	count, err = rel.StarCount(ctx)
	require.NoError(t, err)
	require.Equal(t, uint64(100), count, "Should count committed rows")

	err = txn.Commit(ctx)
	require.NoError(t, err)
}

func TestStarCountWithUncommittedInserts(t *testing.T) {
	catalog.SetupDefines("")

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	ctx = context.WithValue(ctx, defines.TenantIDKey{}, catalog.System_Account)

	disttaeEngine, taeHandler, rpcAgent, _ := testutil.CreateEngines(ctx, testutil.TestOptions{}, t)
	defer func() {
		disttaeEngine.Close(ctx)
		taeHandler.Close(true)
		rpcAgent.Close()
	}()

	ctx, cancel = context.WithTimeout(ctx, time.Minute)
	defer cancel()

	// Setup: Create database and table with 100 rows
	txn, err := disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	err = disttaeEngine.Engine.Create(ctx, "testdb", txn)
	require.NoError(t, err)

	db, err := disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	schema := catalog2.MockSchemaAll(3, -1) // -1 means no primary key
	schema.Name = "test_table"
	defs, err := testutil.EngineTableDefBySchema(schema)
	require.NoError(t, err)

	err = db.Create(ctx, "test_table", defs)
	require.NoError(t, err)

	rel, err := db.Relation(ctx, "test_table", nil)
	require.NoError(t, err)

	bat := catalog2.MockBatch(schema, 100)
	err = rel.Write(ctx, containers.ToCNBatch(bat))
	require.NoError(t, err)

	err = txn.Commit(ctx)
	require.NoError(t, err)

	// Test: Add uncommitted inserts
	txn, err = disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	db, err = disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	rel, err = db.Relation(ctx, "test_table", nil)
	require.NoError(t, err)

	// Insert 50 more rows (uncommitted) - no PK so no conflict
	bat = catalog2.MockBatch(schema, 50)
	err = rel.Write(ctx, containers.ToCNBatch(bat))
	require.NoError(t, err)

	// Count should include uncommitted inserts
	count, err := rel.StarCount(ctx)
	require.NoError(t, err)
	t.Logf("StarCount returned: %d, expected: 150 (100 committed + 50 uncommitted)", count)
	require.Equal(t, uint64(150), count, "Should count 100 committed + 50 uncommitted")

	err = txn.Commit(ctx)
	require.NoError(t, err)
}

func TestStarCountReadonly(t *testing.T) {
	catalog.SetupDefines("")

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	ctx = context.WithValue(ctx, defines.TenantIDKey{}, catalog.System_Account)

	disttaeEngine, taeHandler, rpcAgent, _ := testutil.CreateEngines(ctx, testutil.TestOptions{}, t)
	defer func() {
		disttaeEngine.Close(ctx)
		taeHandler.Close(true)
		rpcAgent.Close()
	}()

	ctx, cancel = context.WithTimeout(ctx, time.Minute)
	defer cancel()

	// Setup: Create database and table with 100 rows
	txn, err := disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	err = disttaeEngine.Engine.Create(ctx, "testdb", txn)
	require.NoError(t, err)

	db, err := disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	schema := catalog2.MockSchemaAll(3, 0)
	schema.Name = "test_table"
	defs, err := testutil.EngineTableDefBySchema(schema)
	require.NoError(t, err)

	err = db.Create(ctx, "test_table", defs)
	require.NoError(t, err)

	rel, err := db.Relation(ctx, "test_table", nil)
	require.NoError(t, err)

	bat := catalog2.MockBatch(schema, 100)
	err = rel.Write(ctx, containers.ToCNBatch(bat))
	require.NoError(t, err)

	err = txn.Commit(ctx)
	require.NoError(t, err)

	// Test: Readonly transaction (use snapshot)
	txn, err = disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	db, err = disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	rel, err = db.Relation(ctx, "test_table", nil)
	require.NoError(t, err)

	// Readonly transaction should only see committed rows
	count, err := rel.StarCount(ctx)
	require.NoError(t, err)
	require.Equal(t, uint64(100), count, "Readonly should only count committed rows")

	err = txn.Commit(ctx)
	require.NoError(t, err)
}

// TestStarCountWithPersistedInserts tests StarCount with persisted uncommitted inserts
func TestStarCountWithPersistedInserts(t *testing.T) {
	catalog.SetupDefines("")

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	ctx = context.WithValue(ctx, defines.TenantIDKey{}, catalog.System_Account)

	// Create engine with very small workspace threshold to trigger persist
	disttaeEngine, taeHandler, rpcAgent, _ := testutil.CreateEngines(
		ctx,
		testutil.TestOptions{
			DisttaeOptions: []testutil.TestDisttaeEngineOptions{
				testutil.WithDisttaeEngineWriteWorkspaceThreshold(1024), // 1KB threshold
			},
		},
		t,
	)
	defer func() {
		disttaeEngine.Close(ctx)
		taeHandler.Close(true)
		rpcAgent.Close()
	}()

	ctx, cancel = context.WithTimeout(ctx, time.Minute)
	defer cancel()

	// Setup: Create database and table with 100 rows
	txn, err := disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	err = disttaeEngine.Engine.Create(ctx, "testdb", txn)
	require.NoError(t, err)

	db, err := disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	schema := catalog2.MockSchemaAll(3, -1) // No PK
	schema.Name = "test_table"
	defs, err := testutil.EngineTableDefBySchema(schema)
	require.NoError(t, err)

	err = db.Create(ctx, "test_table", defs)
	require.NoError(t, err)

	rel, err := db.Relation(ctx, "test_table", nil)
	require.NoError(t, err)

	bat := catalog2.MockBatch(schema, 100)
	err = rel.Write(ctx, containers.ToCNBatch(bat))
	require.NoError(t, err)

	err = txn.Commit(ctx)
	require.NoError(t, err)

	// Test: Add uncommitted inserts that will be persisted
	txn, err = disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	db, err = disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	rel, err = db.Relation(ctx, "test_table", nil)
	require.NoError(t, err)

	// Insert enough data to trigger persist (> 1KB)
	// Each row is ~100 bytes, so 20 rows should be enough
	for i := 0; i < 5; i++ {
		bat = catalog2.MockBatch(schema, 20)
		err = rel.Write(ctx, containers.ToCNBatch(bat))
		require.NoError(t, err)
	}

	// Count should include both committed and persisted uncommitted inserts
	count, err := rel.StarCount(ctx)
	require.NoError(t, err)
	t.Logf("StarCount returned: %d, expected: 200 (100 committed + 100 persisted uncommitted)", count)
	require.Equal(t, uint64(200), count, "Should count 100 committed + 100 persisted uncommitted")

	err = txn.Commit(ctx)
	require.NoError(t, err)
}

// TestStarCountPersistedInsertsMultipleObjectStatsInOneBatch tests that StarCount
// sums rows from all ObjectStats in a single persisted-insert batch, not just the first.
// This covers the bug where one flush produced multiple S3 objects (multiple ObjectStats
// in one workspace entry) and only the first was counted.
//
// Regression: if the bug is reintroduced (only reading entry.bat.Vecs[1].GetBytesAt(0)),
// StarCount would return 100+10=110 instead of 100+10+20+30=160, and this assertion would fail.
//
// Stability: no rand/sleep/parallel; result depends only on controlled workspace state and
// fixed row counts (100+10+20+30). Safe for CI.
func TestStarCountPersistedInsertsMultipleObjectStatsInOneBatch(t *testing.T) {
	catalog.SetupDefines("")

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	ctx = context.WithValue(ctx, defines.TenantIDKey{}, catalog.System_Account)

	disttaeEngine, taeHandler, rpcAgent, _ := testutil.CreateEngines(ctx, testutil.TestOptions{}, t)
	defer func() {
		disttaeEngine.Close(ctx)
		taeHandler.Close(true)
		rpcAgent.Close()
	}()

	ctx, cancel = context.WithTimeout(ctx, time.Minute)
	defer cancel()

	// Setup: Create database and table with 100 committed rows
	txn, err := disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	err = disttaeEngine.Engine.Create(ctx, "testdb", txn)
	require.NoError(t, err)

	db, err := disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	schema := catalog2.MockSchemaAll(3, -1)
	schema.Name = "test_table"
	defs, err := testutil.EngineTableDefBySchema(schema)
	require.NoError(t, err)

	err = db.Create(ctx, "test_table", defs)
	require.NoError(t, err)

	rel, err := db.Relation(ctx, "test_table", nil)
	require.NoError(t, err)

	bat := catalog2.MockBatch(schema, 100)
	err = rel.Write(ctx, containers.ToCNBatch(bat))
	require.NoError(t, err)

	err = txn.Commit(ctx)
	require.NoError(t, err)

	// New txn: inject one persisted-insert entry with multiple ObjectStats in one batch
	txn, err = disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	db, err = disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	rel, err = db.Relation(ctx, "test_table", nil)
	require.NoError(t, err)

	proc := rel.GetProcess().(*process.Process)
	insertBat := colexec.AllocCNS3ResultBat(false, false)
	defer insertBat.Clean(proc.Mp())

	// One batch with 3 ObjectStats (10 + 20 + 30 = 60 rows total)
	rowCounts := []uint32{10, 20, 30}
	for _, cnt := range rowCounts {
		objID := objectio.NewObjectid()
		stats := objectio.NewObjectStatsWithObjectID(&objID, false, false, false)
		require.NoError(t, objectio.SetObjectStatsRowCnt(stats, cnt))
		require.NoError(t, vector.AppendBytes(insertBat.Vecs[1], stats.Marshal(), false, proc.Mp()))
		require.NoError(t, vector.AppendBytes(insertBat.Vecs[0], []byte{}, false, proc.Mp()))
	}
	insertBat.SetRowCount(len(rowCounts))

	dtxn := txn.GetWorkspace().(*disttae.Transaction)
	require.NoError(t, dtxn.WriteFile(
		disttae.INSERT,
		catalog.System_Account,
		rel.GetDBID(ctx),
		rel.GetTableID(ctx),
		"testdb",
		"test_table",
		"fake_multi_obj",
		insertBat,
		disttaeEngine.Engine.GetTNServices()[0],
	))

	// Guarantee workspace state: one persisted INSERT entry with exactly 3 ObjectStats in one batch.
	dbID, tableID := rel.GetDBID(ctx), rel.GetTableID(ctx)
	assertWorkspaceHasPersistedInsertWithMultipleObjectStats(t, txn, dbID, tableID, 3)

	// StarCount must sum all 3 objects: 100 + 10 + 20 + 30 = 160.
	// Bug would only count first ObjectStats -> 100+10=110.
	count, err := rel.StarCount(ctx)
	require.NoError(t, err)
	t.Logf("StarCount with multiple ObjectStats in one batch: %d, expected: 160", count)
	require.Equal(t, uint64(160), count, "StarCount must sum all ObjectStats in the batch (100+10+20+30=160); bug would return 110")

	err = txn.Commit(ctx)
	require.NoError(t, err)
}

// TestStarCountReadonlyLarge tests readonly transaction with large dataset
func TestStarCountReadonlyLarge(t *testing.T) {
	catalog.SetupDefines("")

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	ctx = context.WithValue(ctx, defines.TenantIDKey{}, catalog.System_Account)

	disttaeEngine, taeHandler, rpcAgent, _ := testutil.CreateEngines(ctx, testutil.TestOptions{}, t)
	defer func() {
		disttaeEngine.Close(ctx)
		taeHandler.Close(true)
		rpcAgent.Close()
	}()

	ctx, cancel = context.WithTimeout(ctx, time.Minute*2)
	defer cancel()

	// Create database and table
	txn, err := disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	err = disttaeEngine.Engine.Create(ctx, "testdb", txn)
	require.NoError(t, err)

	db, err := disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	schema := catalog2.MockSchemaAll(3, -1)
	schema.Name = "test_table"
	defs, err := testutil.EngineTableDefBySchema(schema)
	require.NoError(t, err)

	err = db.Create(ctx, "test_table", defs)
	require.NoError(t, err)

	rel, err := db.Relation(ctx, "test_table", nil)
	require.NoError(t, err)

	// Insert 10k rows (reduced from 1M to keep test fast)
	totalRows := 10000
	batchSize := 1000
	for i := 0; i < totalRows/batchSize; i++ {
		bat := catalog2.MockBatch(schema, batchSize)
		err = rel.Write(ctx, containers.ToCNBatch(bat))
		require.NoError(t, err)
	}

	err = txn.Commit(ctx)
	require.NoError(t, err)

	// Readonly transaction
	txn, err = disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	db, err = disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	rel, err = db.Relation(ctx, "test_table", nil)
	require.NoError(t, err)

	count, err := rel.StarCount(ctx)
	require.NoError(t, err)
	require.Equal(t, uint64(totalRows), count)

	err = txn.Commit(ctx)
	require.NoError(t, err)
}

// TestStarCountInMemoryEmpty tests in-memory inserts on empty table
func TestStarCountInMemoryEmpty(t *testing.T) {
	catalog.SetupDefines("")

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	ctx = context.WithValue(ctx, defines.TenantIDKey{}, catalog.System_Account)

	disttaeEngine, taeHandler, rpcAgent, _ := testutil.CreateEngines(ctx, testutil.TestOptions{}, t)
	defer func() {
		disttaeEngine.Close(ctx)
		taeHandler.Close(true)
		rpcAgent.Close()
	}()

	ctx, cancel = context.WithTimeout(ctx, time.Minute)
	defer cancel()

	// Create empty table
	txn, err := disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	err = disttaeEngine.Engine.Create(ctx, "testdb", txn)
	require.NoError(t, err)

	db, err := disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	schema := catalog2.MockSchemaAll(3, -1)
	schema.Name = "test_table"
	defs, err := testutil.EngineTableDefBySchema(schema)
	require.NoError(t, err)

	err = db.Create(ctx, "test_table", defs)
	require.NoError(t, err)

	err = txn.Commit(ctx)
	require.NoError(t, err)

	// Insert 50 rows on empty table
	txn, err = disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	db, err = disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	rel, err := db.Relation(ctx, "test_table", nil)
	require.NoError(t, err)

	bat := catalog2.MockBatch(schema, 50)
	err = rel.Write(ctx, containers.ToCNBatch(bat))
	require.NoError(t, err)

	count, err := rel.StarCount(ctx)
	require.NoError(t, err)
	require.Equal(t, uint64(50), count, "Empty table + 50 uncommitted = 50")

	err = txn.Commit(ctx)
	require.NoError(t, err)
}

// TestStarCountInMemoryLarge tests in-memory inserts with large dataset
func TestStarCountInMemoryLarge(t *testing.T) {
	catalog.SetupDefines("")

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	ctx = context.WithValue(ctx, defines.TenantIDKey{}, catalog.System_Account)

	disttaeEngine, taeHandler, rpcAgent, _ := testutil.CreateEngines(ctx, testutil.TestOptions{}, t)
	defer func() {
		disttaeEngine.Close(ctx)
		taeHandler.Close(true)
		rpcAgent.Close()
	}()

	ctx, cancel = context.WithTimeout(ctx, time.Minute*2)
	defer cancel()

	// Create table with 10k rows
	txn, err := disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	err = disttaeEngine.Engine.Create(ctx, "testdb", txn)
	require.NoError(t, err)

	db, err := disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	schema := catalog2.MockSchemaAll(3, -1)
	schema.Name = "test_table"
	defs, err := testutil.EngineTableDefBySchema(schema)
	require.NoError(t, err)

	err = db.Create(ctx, "test_table", defs)
	require.NoError(t, err)

	rel, err := db.Relation(ctx, "test_table", nil)
	require.NoError(t, err)

	// Insert 10k rows
	totalRows := 10000
	batchSize := 1000
	for i := 0; i < totalRows/batchSize; i++ {
		bat := catalog2.MockBatch(schema, batchSize)
		err = rel.Write(ctx, containers.ToCNBatch(bat))
		require.NoError(t, err)
	}

	err = txn.Commit(ctx)
	require.NoError(t, err)

	// Add 1k uncommitted rows
	txn, err = disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	db, err = disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	rel, err = db.Relation(ctx, "test_table", nil)
	require.NoError(t, err)

	uncommittedRows := 1000
	bat := catalog2.MockBatch(schema, uncommittedRows)
	err = rel.Write(ctx, containers.ToCNBatch(bat))
	require.NoError(t, err)

	count, err := rel.StarCount(ctx)
	require.NoError(t, err)
	require.Equal(t, uint64(totalRows+uncommittedRows), count, "10k committed + 1k uncommitted = 11k")

	err = txn.Commit(ctx)
	require.NoError(t, err)
}

// TestStarCountMixedInMemoryAndPersisted tests mixed in-memory and persisted inserts
func TestStarCountMixedInMemoryAndPersisted(t *testing.T) {
	catalog.SetupDefines("")

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	ctx = context.WithValue(ctx, defines.TenantIDKey{}, catalog.System_Account)

	disttaeEngine, taeHandler, rpcAgent, _ := testutil.CreateEngines(ctx, testutil.TestOptions{}, t)
	defer func() {
		disttaeEngine.Close(ctx)
		taeHandler.Close(true)
		rpcAgent.Close()
	}()

	ctx, cancel = context.WithTimeout(ctx, time.Minute)
	defer cancel()

	// Create table with 100 rows
	txn, err := disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	err = disttaeEngine.Engine.Create(ctx, "testdb", txn)
	require.NoError(t, err)

	db, err := disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	schema := catalog2.MockSchemaAll(3, -1)
	schema.Name = "test_table"
	defs, err := testutil.EngineTableDefBySchema(schema)
	require.NoError(t, err)

	err = db.Create(ctx, "test_table", defs)
	require.NoError(t, err)

	rel, err := db.Relation(ctx, "test_table", nil)
	require.NoError(t, err)

	bat := catalog2.MockBatch(schema, 100)
	err = rel.Write(ctx, containers.ToCNBatch(bat))
	require.NoError(t, err)

	err = txn.Commit(ctx)
	require.NoError(t, err)

	// Start new transaction with fault injection enabled
	txn, err = disttaeEngine.NewTxnOperator(ctx, disttaeEngine.Now())
	require.NoError(t, err)

	// Enable fault injection to force workspace flush
	fault.Enable()
	err = fault.AddFaultPoint(ctx, objectio.FJ_CNWorkspaceForceFlush, ":::", "return", 0, "", false)
	require.NoError(t, err)

	db, err = disttaeEngine.Engine.Database(ctx, "testdb", txn)
	require.NoError(t, err)

	rel, err = db.Relation(ctx, "test_table", nil)
	require.NoError(t, err)

	// Write batches with fault injection enabled - these will be persisted
	for i := 0; i < 3; i++ {
		bat = catalog2.MockBatch(schema, 20)
		err = rel.Write(ctx, containers.ToCNBatch(bat))
		require.NoError(t, err)
	}

	// Disable fault injection
	fault.RemoveFaultPoint(ctx, objectio.FJ_CNWorkspaceForceFlush)
	fault.Disable()

	// Write more batches without fault injection - these will stay in-memory
	for i := 0; i < 2; i++ {
		bat = catalog2.MockBatch(schema, 25)
		err = rel.Write(ctx, containers.ToCNBatch(bat))
		require.NoError(t, err)
	}

	// Total: 100 committed + 60 persisted + 50 in-memory = 210
	count, err := rel.StarCount(ctx)
	require.NoError(t, err)
	require.Equal(t, uint64(210), count, "Should count all rows including mixed in-memory and persisted")

	// Verify workspace state: should have both in-memory and persisted writes
	inMemory, persisted := countWorkspaceWrites(txn)
	t.Logf("Workspace state: in-memory writes=%d, persisted writes=%d", inMemory, persisted)
	require.Greater(t, inMemory, 0, "Should have in-memory writes")
	require.Greater(t, persisted, 0, "Should have persisted writes")

	err = txn.Commit(ctx)
	require.NoError(t, err)
}

// assertWorkspaceHasPersistedInsertWithMultipleObjectStats ensures the txn workspace
// contains at least one persisted INSERT entry for the table whose batch has exactly
// wantObjectStatsRows rows in Vecs[1] (ObjectStats). This guarantees the test scenario
// that triggers the bug: one batch with multiple ObjectStats.
func assertWorkspaceHasPersistedInsertWithMultipleObjectStats(
	t *testing.T,
	txn client.TxnOperator,
	dbID, tableID uint64,
	wantObjectStatsRows int,
) {
	dtxn, ok := txn.GetWorkspace().(*disttae.Transaction)
	require.True(t, ok, "workspace must be *disttae.Transaction")
	txnValue := reflect.ValueOf(dtxn).Elem()
	writesField := txnValue.FieldByName("writes")
	require.True(t, writesField.IsValid(), "writes field must exist")
	writesLen := writesField.Len()

	var found bool
	dtxn.ForEachTableWrites(dbID, tableID, writesLen, func(entry disttae.Entry) {
		if entry.Type() != disttae.INSERT || entry.FileName() == "" || entry.Bat() == nil {
			return
		}
		bat := entry.Bat()
		if len(bat.Attrs) < 2 || bat.Attrs[1] != catalog.ObjectMeta_ObjectStats {
			return
		}
		if bat.Vecs[1].Length() == wantObjectStatsRows {
			found = true
		}
	})
	require.True(t, found,
		"workspace must contain one persisted INSERT entry with exactly %d ObjectStats rows (Vecs[1].Length()=%d); this guarantees the bug scenario is exercised",
		wantObjectStatsRows, wantObjectStatsRows)
}

// assertWorkspaceHasPersistedTombstones verifies that the transaction workspace contains
// at least one uncommitted persisted DELETE entry (typ=DELETE, fileName!="", bat non-empty
// with ObjectStats in column 0). This guarantees StarCount will exercise
// countUncommittedPersistedTombstones for-loop.
func assertWorkspaceHasPersistedTombstones(
	t *testing.T,
	txn client.TxnOperator,
	dbID, tableID uint64,
) {
	dtxn, ok := txn.GetWorkspace().(*disttae.Transaction)
	require.True(t, ok, "workspace must be *disttae.Transaction")
	txnValue := reflect.ValueOf(dtxn).Elem()
	writesField := txnValue.FieldByName("writes")
	require.True(t, writesField.IsValid(), "writes field must exist")
	writesLen := writesField.Len()

	var found bool
	dtxn.ForEachTableWrites(dbID, tableID, writesLen, func(entry disttae.Entry) {
		if entry.Type() != disttae.DELETE || entry.FileName() == "" || entry.Bat() == nil {
			return
		}
		bat := entry.Bat()
		if bat.IsEmpty() || bat.Vecs[0].Length() == 0 {
			return
		}
		if len(bat.Attrs) > 0 && bat.Attrs[0] == catalog.ObjectMeta_ObjectStats {
			found = true
		}
	})
	require.True(t, found,
		"workspace must contain at least one persisted DELETE entry (typ=DELETE, fileName!=empty, bat with ObjectStats in column 0); this guarantees countUncommittedPersistedTombstones path is exercised")
}

// countWorkspaceWrites counts in-memory and persisted writes in the transaction workspace
func countWorkspaceWrites(txn client.TxnOperator) (inMemory, persisted int) {
	// Access the internal transaction workspace
	workspace := txn.GetWorkspace()
	dtxn, ok := workspace.(*disttae.Transaction)
	if !ok {
		return 0, 0
	}

	// Use reflection to access the writes slice since there's no public API
	// to iterate all writes across all tables
	txnValue := reflect.ValueOf(dtxn).Elem()
	writesField := txnValue.FieldByName("writes")
	if !writesField.IsValid() {
		return 0, 0
	}

	// Count all INSERT writes
	for i := 0; i < writesField.Len(); i++ {
		entry := writesField.Index(i)

		// Get typ field
		typField := entry.FieldByName("typ")
		if !typField.IsValid() || typField.Int() != int64(disttae.INSERT) {
			continue
		}

		// Get fileName field
		fileNameField := entry.FieldByName("fileName")
		if !fileNameField.IsValid() {
			continue
		}

		// Get bat field
		batField := entry.FieldByName("bat")
		if !batField.IsValid() {
			continue
		}

		if fileNameField.String() != "" {
			persisted++
		} else if !batField.IsNil() {
			inMemory++
		}
	}

	return inMemory, persisted
}

// TestStarCountWithInMemoryTombstones tests StarCount with uncommitted in-memory tombstones.
//
// Scenario:
// 1. Commit 100 rows via TAE
// 2. Start a new CN transaction
// 3. Delete 20 rows (uncommitted, in-memory tombstones)
// 4. Verify StarCount = 100 - 20 = 80
//
// This tests the formula: StarCount = CommittedRows + UncommittedInserts - UncommittedTombstones
// where UncommittedTombstones are in-memory (no visibility check needed).
func TestStarCountWithInMemoryTombstones(t *testing.T) {
	p := testutil.InitEnginePack(testutil.TestOptions{}, t)
	defer p.Close()

	tae := p.T.GetDB()

	schema := catalog2.MockSchemaAll(3, 0) // column 0 is primary key (int8)
	schema.Name = "test_table"

	// Step 1: Create table via CN and insert 100 rows via TAE
	txnop := p.StartCNTxn()
	_, rel := p.CreateDBAndTable(txnop, "db", schema)
	dbID := rel.GetDBID(p.Ctx)
	tableID := rel.GetTableID(p.Ctx)
	require.NoError(t, txnop.Commit(p.Ctx))

	// Subscribe to the table
	require.NoError(t, p.D.SubscribeTable(p.Ctx, dbID, tableID, "db", schema.Name, false))

	// Insert 100 rows via TAE
	{
		txn, _ := tae.StartTxn(nil)
		db, _ := txn.GetDatabase("db")
		rel, _ := db.GetRelationByName(schema.Name)
		bat := catalog2.MockBatch(schema, 100)
		require.NoError(t, rel.Append(p.Ctx, bat))
		require.NoError(t, txn.Commit(p.Ctx))
	}

	// Wait for logtail to sync
	_, err := p.D.GetPartitionStateStats(p.Ctx, dbID, tableID)
	require.NoError(t, err)

	// Step 2: Collect rowids from TAE for deletion
	var deleteRowIDs []types.Rowid
	{
		txn, _ := tae.StartTxn(nil)
		db, _ := txn.GetDatabase("db")
		rel, _ := db.GetRelationByName(schema.Name)

		it := rel.MakeObjectIt(false)
		for it.Next() {
			obj := it.GetObject()
			objID := obj.GetMeta().(*catalog2.ObjectEntry).ID()

			for blkIdx := 0; blkIdx < obj.BlkCnt(); blkIdx++ {
				blkID := objectio.NewBlockidWithObjectID(objID, uint16(blkIdx))
				for rowIdx := uint32(0); rowIdx < 20 && len(deleteRowIDs) < 20; rowIdx++ {
					rowid := objectio.NewRowid(&blkID, rowIdx)
					deleteRowIDs = append(deleteRowIDs, rowid)
				}
			}
		}
		it.Close()
		require.NoError(t, txn.Commit(p.Ctx))
	}
	require.Equal(t, 20, len(deleteRowIDs), "Should collect 20 rowids for deletion")

	// Step 3: Start CN transaction and delete rows
	txnop = p.StartCNTxn()

	db, err := p.D.Engine.Database(p.Ctx, "db", txnop)
	require.NoError(t, err)

	rel, err = db.Relation(p.Ctx, schema.Name, nil)
	require.NoError(t, err)

	// Verify initial count (should be 100)
	count, err := rel.StarCount(p.Ctx)
	require.NoError(t, err)
	require.Equal(t, uint64(100), count, "Initial count should be 100")

	// Build delete batch with rowids
	vec1 := vector.NewVec(types.T_Rowid.ToType())
	for _, rowid := range deleteRowIDs {
		require.NoError(t, vector.AppendFixed(vec1, rowid, false, p.Mp))
	}

	// Get PK type from schema and create dummy PK values
	// MockSchemaAll(3, 0) creates column 0 as int8 type PK
	pkCol := schema.GetPrimaryKey()
	vec2 := vector.NewVec(pkCol.Type)
	for i := 0; i < len(deleteRowIDs); i++ {
		require.NoError(t, vector.AppendFixed(vec2, int8(i), false, p.Mp))
	}

	delBatch := batch.NewWithSize(2)
	delBatch.SetRowCount(len(deleteRowIDs))
	delBatch.Attrs = []string{catalog.Row_ID, catalog.TableTailAttrPKVal}
	delBatch.Vecs[0] = vec1
	delBatch.Vecs[1] = vec2

	// Delete rows
	require.NoError(t, rel.Delete(p.Ctx, delBatch, catalog.Row_ID))

	// Step 4: Verify StarCount after deletion
	count, err = rel.StarCount(p.Ctx)
	require.NoError(t, err)
	t.Logf("StarCount after delete: %d, expected: 80 (100 committed - 20 uncommitted deletes)", count)
	require.Equal(t, uint64(80), count, "StarCount should be 100 - 20 = 80")

	// Cleanup
	vec1.Free(p.Mp)
	vec2.Free(p.Mp)

	require.NoError(t, txnop.Commit(p.Ctx))
}

// TestStarCountWithMixedInsertsAndTombstones tests StarCount with both
// uncommitted inserts and uncommitted tombstones in the same transaction.
//
// Scenario:
// 1. Commit 100 rows via TAE
// 2. Start a new CN transaction
// 3. Insert 50 new rows (uncommitted inserts)
// 4. Delete 20 committed rows (uncommitted tombstones)
// 5. Verify StarCount = 100 + 50 - 20 = 130
func TestStarCountWithMixedInsertsAndTombstones(t *testing.T) {
	p := testutil.InitEnginePack(testutil.TestOptions{}, t)
	defer p.Close()

	tae := p.T.GetDB()

	schema := catalog2.MockSchemaAll(3, -1) // -1 means fake PK (uint64, no conflict on insert)
	schema.Name = "test_table"

	// Step 1: Create table via CN
	txnop := p.StartCNTxn()
	_, rel := p.CreateDBAndTable(txnop, "db", schema)
	dbID := rel.GetDBID(p.Ctx)
	tableID := rel.GetTableID(p.Ctx)
	require.NoError(t, txnop.Commit(p.Ctx))

	// Subscribe to the table
	require.NoError(t, p.D.SubscribeTable(p.Ctx, dbID, tableID, "db", schema.Name, false))

	// Insert 100 rows via TAE
	{
		txn, _ := tae.StartTxn(nil)
		db, _ := txn.GetDatabase("db")
		rel, _ := db.GetRelationByName(schema.Name)
		bat := catalog2.MockBatch(schema, 100)
		require.NoError(t, rel.Append(p.Ctx, bat))
		require.NoError(t, txn.Commit(p.Ctx))
	}

	// Wait for logtail to sync
	_, err := p.D.GetPartitionStateStats(p.Ctx, dbID, tableID)
	require.NoError(t, err)

	// Step 2: Collect rowids for deletion
	var deleteRowIDs []types.Rowid
	{
		txn, _ := tae.StartTxn(nil)
		db, _ := txn.GetDatabase("db")
		rel, _ := db.GetRelationByName(schema.Name)

		it := rel.MakeObjectIt(false)
		for it.Next() {
			obj := it.GetObject()
			objID := obj.GetMeta().(*catalog2.ObjectEntry).ID()

			for blkIdx := 0; blkIdx < obj.BlkCnt(); blkIdx++ {
				blkID := objectio.NewBlockidWithObjectID(objID, uint16(blkIdx))
				for rowIdx := uint32(0); rowIdx < 20 && len(deleteRowIDs) < 20; rowIdx++ {
					rowid := objectio.NewRowid(&blkID, rowIdx)
					deleteRowIDs = append(deleteRowIDs, rowid)
				}
			}
		}
		it.Close()
		require.NoError(t, txn.Commit(p.Ctx))
	}
	require.Equal(t, 20, len(deleteRowIDs))

	// Step 3: Start CN transaction
	txnop = p.StartCNTxn()

	db, err := p.D.Engine.Database(p.Ctx, "db", txnop)
	require.NoError(t, err)

	rel, err = db.Relation(p.Ctx, schema.Name, nil)
	require.NoError(t, err)

	// Insert 50 new rows
	bat := catalog2.MockBatch(schema, 50)
	require.NoError(t, rel.Write(p.Ctx, containers.ToCNBatch(bat)))

	// Delete 20 rows
	vec1 := vector.NewVec(types.T_Rowid.ToType())
	for _, rowid := range deleteRowIDs {
		require.NoError(t, vector.AppendFixed(vec1, rowid, false, p.Mp))
	}

	// Use fake PK column (uint64 type)
	pkCol := schema.GetPrimaryKey()
	vec2 := vector.NewVec(pkCol.Type)
	for i := 0; i < len(deleteRowIDs); i++ {
		require.NoError(t, vector.AppendFixed(vec2, uint64(i), false, p.Mp))
	}

	delBatch := batch.NewWithSize(2)
	delBatch.SetRowCount(len(deleteRowIDs))
	delBatch.Attrs = []string{catalog.Row_ID, catalog.TableTailAttrPKVal}
	delBatch.Vecs[0] = vec1
	delBatch.Vecs[1] = vec2

	require.NoError(t, rel.Delete(p.Ctx, delBatch, catalog.Row_ID))

	// Step 4: Verify StarCount
	count, err := rel.StarCount(p.Ctx)
	require.NoError(t, err)
	t.Logf("StarCount: %d, expected: 130 (100 committed + 50 uncommitted inserts - 20 uncommitted deletes)", count)
	require.Equal(t, uint64(130), count, "StarCount should be 100 + 50 - 20 = 130")

	vec1.Free(p.Mp)
	vec2.Free(p.Mp)

	require.NoError(t, txnop.Commit(p.Ctx))
}

// TestStarCountWithPersistedTombstones tests StarCount with persisted uncommitted tombstones.
//
// Scenario:
//  1. Commit 100 rows via TAE
//  2. Start a new CN transaction
//  3. Create a tombstone object (20 rowids) via CNS3TombstoneWriter and add it as a persisted
//     DELETE entry via rel.Delete with ObjectMeta_ObjectStats (object-level delete)
//  4. Assert workspace has persisted tombstones (fileName != "", bat with ObjectStats)
//  5. Verify StarCount = 100 - 20 = 80 and that the persisted-tombstone path was exercised (fault getcount)
//
// This tests countUncommittedPersistedTombstones for-loop (entry.fileName != "", Vecs[0] as ObjectStats).
func TestStarCountWithPersistedTombstones(t *testing.T) {
	p := testutil.InitEnginePack(testutil.TestOptions{}, t)
	defer p.Close()

	tae := p.T.GetDB()

	schema := catalog2.MockSchemaAll(3, 0) // column 0 is primary key (int8)
	schema.Name = "test_table"

	// Step 1: Create table via CN and insert 100 rows via TAE
	txnop := p.StartCNTxn()
	_, rel := p.CreateDBAndTable(txnop, "db", schema)
	dbID := rel.GetDBID(p.Ctx)
	tableID := rel.GetTableID(p.Ctx)
	require.NoError(t, txnop.Commit(p.Ctx))

	require.NoError(t, p.D.SubscribeTable(p.Ctx, dbID, tableID, "db", schema.Name, false))

	{
		txn, _ := tae.StartTxn(nil)
		db, _ := txn.GetDatabase("db")
		rel, _ := db.GetRelationByName(schema.Name)
		bat := catalog2.MockBatch(schema, 100)
		require.NoError(t, rel.Append(p.Ctx, bat))
		require.NoError(t, txn.Commit(p.Ctx))
	}

	_, err := p.D.GetPartitionStateStats(p.Ctx, dbID, tableID)
	require.NoError(t, err)

	// Step 2: Collect 20 rowids from TAE for the tombstone object
	var deleteRowIDs []types.Rowid
	{
		txn, _ := tae.StartTxn(nil)
		db, _ := txn.GetDatabase("db")
		rel, _ := db.GetRelationByName(schema.Name)
		it := rel.MakeObjectIt(false)
		for it.Next() {
			obj := it.GetObject()
			objID := obj.GetMeta().(*catalog2.ObjectEntry).ID()
			for blkIdx := 0; blkIdx < obj.BlkCnt(); blkIdx++ {
				blkID := objectio.NewBlockidWithObjectID(objID, uint16(blkIdx))
				for rowIdx := uint32(0); rowIdx < 20 && len(deleteRowIDs) < 20; rowIdx++ {
					rowid := objectio.NewRowid(&blkID, rowIdx)
					deleteRowIDs = append(deleteRowIDs, rowid)
				}
			}
		}
		it.Close()
		require.NoError(t, txn.Commit(p.Ctx))
	}
	require.Equal(t, 20, len(deleteRowIDs), "Should collect 20 rowids")

	// Step 3: Start CN transaction and add persisted tombstones via object-level delete
	txnop = p.StartCNTxn()
	db, err := p.D.Engine.Database(p.Ctx, "db", txnop)
	require.NoError(t, err)
	rel, err = db.Relation(p.Ctx, schema.Name, nil)
	require.NoError(t, err)

	count, err := rel.StarCount(p.Ctx)
	require.NoError(t, err)
	require.Equal(t, uint64(100), count, "Initial count should be 100")

	// Build tombstone batch (rowids + pk) and write to S3 via CNS3TombstoneWriter
	tombstoneBat := batch.NewWithSize(2)
	tombstoneBat.Vecs[0] = vector.NewVec(types.T_Rowid.ToType())
	tombstoneBat.Vecs[1] = vector.NewVec(types.T_int8.ToType())
	tombstoneBat.Attrs = []string{catalog.Row_ID, catalog.TableTailAttrPKVal}
	for _, rowid := range deleteRowIDs {
		require.NoError(t, vector.AppendFixed(tombstoneBat.Vecs[0], rowid, false, p.Mp))
	}
	for i := 0; i < len(deleteRowIDs); i++ {
		require.NoError(t, vector.AppendFixed(tombstoneBat.Vecs[1], int8(i), false, p.Mp))
	}
	tombstoneBat.SetRowCount(len(deleteRowIDs))

	proc := rel.GetProcess().(*process.Process)
	w := colexec.NewCNS3TombstoneWriter(proc.Mp(), proc.GetFileService(), types.T_int8.ToType(), -1)
	require.NoError(t, w.Write(p.Ctx, tombstoneBat))
	ss, err := w.Sync(p.Ctx)
	require.NoError(t, err)
	require.Equal(t, 1, len(ss), "Sync should return one ObjectStats")

	tombstoneBat.Vecs[0].Free(p.Mp)
	tombstoneBat.Vecs[1].Free(p.Mp)

	// Add persisted DELETE entry via object-level delete (ObjectMeta_ObjectStats)
	tbat := batch.NewWithSize(1)
	tbat.Attrs = []string{catalog.ObjectMeta_ObjectStats}
	tbat.Vecs[0] = vector.NewVec(types.T_binary.ToType())
	require.NoError(t, vector.AppendBytes(tbat.Vecs[0], ss[0].Marshal(), false, p.Mp))
	tbat.SetRowCount(1)
	require.NoError(t, rel.Delete(p.Ctx, tbat, catalog.Row_ID))

	// Step 4: Assert workspace has uncommitted persisted tombstones (guarantees the path is exercisable)
	assertWorkspaceHasPersistedTombstones(t, txnop, dbID, tableID)

	// Step 5: StarCount must use persisted tombstones and return 80
	count, err = rel.StarCount(p.Ctx)
	require.NoError(t, err)
	t.Logf("StarCount after delete: %d, expected: 80 (100 committed - 20 persisted uncommitted deletes)", count)
	require.Equal(t, uint64(80), count, "StarCount should be 100 - 20 = 80")

	tbat.Vecs[0].Free(p.Mp)
	require.NoError(t, txnop.Commit(p.Ctx))
}

// TestStarCountDeleteUncommittedInserts tests StarCount when deleting uncommitted inserts.
//
// Scenario:
// 1. Commit 100 rows via TAE
// 2. Start a new CN transaction
// 3. Insert 50 new rows (uncommitted)
// 4. Delete 30 of those uncommitted rows
// 5. Verify StarCount = 100 + 50 - 30 = 120
//
// This tests that inserts and deletes in the same transaction cancel out correctly.
func TestStarCountDeleteUncommittedInserts(t *testing.T) {
	p := testutil.InitEnginePack(testutil.TestOptions{}, t)
	defer p.Close()

	tae := p.T.GetDB()

	schema := catalog2.MockSchemaAll(3, -1) // fake PK (uint64)
	schema.Name = "test_table"

	// Step 1: Create table and insert 100 rows
	txnop := p.StartCNTxn()
	_, rel := p.CreateDBAndTable(txnop, "db", schema)
	dbID := rel.GetDBID(p.Ctx)
	tableID := rel.GetTableID(p.Ctx)
	require.NoError(t, txnop.Commit(p.Ctx))

	require.NoError(t, p.D.SubscribeTable(p.Ctx, dbID, tableID, "db", schema.Name, false))

	{
		txn, _ := tae.StartTxn(nil)
		db, _ := txn.GetDatabase("db")
		rel, _ := db.GetRelationByName(schema.Name)
		bat := catalog2.MockBatch(schema, 100)
		require.NoError(t, rel.Append(p.Ctx, bat))
		require.NoError(t, txn.Commit(p.Ctx))
	}

	_, err := p.D.GetPartitionStateStats(p.Ctx, dbID, tableID)
	require.NoError(t, err)

	// Step 2: Start CN transaction, insert 50 rows, then delete 30 of them
	txnop = p.StartCNTxn()

	db, err := p.D.Engine.Database(p.Ctx, "db", txnop)
	require.NoError(t, err)

	rel, err = db.Relation(p.Ctx, schema.Name, nil)
	require.NoError(t, err)

	// Insert 50 rows
	bat := catalog2.MockBatch(schema, 50)
	require.NoError(t, rel.Write(p.Ctx, containers.ToCNBatch(bat)))

	// Note: We cannot easily get rowids of uncommitted inserts to delete them
	// because they don't have stable rowids yet. This scenario is actually
	// handled by the deleteBatch() mechanism which marks uncommitted inserts
	// for deletion via batchSelectList.
	//
	// For this test, we'll delete committed rows instead to verify the formula.
	// The actual "delete uncommitted inserts" scenario is handled internally
	// by the workspace mechanism, not through the Delete() API.

	// Collect rowids from committed data
	var deleteRowIDs []types.Rowid
	{
		txn, _ := tae.StartTxn(nil)
		db, _ := txn.GetDatabase("db")
		rel, _ := db.GetRelationByName(schema.Name)

		it := rel.MakeObjectIt(false)
		for it.Next() {
			obj := it.GetObject()
			objID := obj.GetMeta().(*catalog2.ObjectEntry).ID()

			for blkIdx := 0; blkIdx < obj.BlkCnt(); blkIdx++ {
				blkID := objectio.NewBlockidWithObjectID(objID, uint16(blkIdx))
				for rowIdx := uint32(0); rowIdx < 30 && len(deleteRowIDs) < 30; rowIdx++ {
					rowid := objectio.NewRowid(&blkID, rowIdx)
					deleteRowIDs = append(deleteRowIDs, rowid)
				}
			}
		}
		it.Close()
		require.NoError(t, txn.Commit(p.Ctx))
	}
	require.Equal(t, 30, len(deleteRowIDs))

	// Delete 30 committed rows
	vec1 := vector.NewVec(types.T_Rowid.ToType())
	for _, rowid := range deleteRowIDs {
		require.NoError(t, vector.AppendFixed(vec1, rowid, false, p.Mp))
	}

	pkCol := schema.GetPrimaryKey()
	vec2 := vector.NewVec(pkCol.Type)
	for i := 0; i < len(deleteRowIDs); i++ {
		require.NoError(t, vector.AppendFixed(vec2, uint64(i), false, p.Mp))
	}

	delBatch := batch.NewWithSize(2)
	delBatch.SetRowCount(len(deleteRowIDs))
	delBatch.Attrs = []string{catalog.Row_ID, catalog.TableTailAttrPKVal}
	delBatch.Vecs[0] = vec1
	delBatch.Vecs[1] = vec2

	require.NoError(t, rel.Delete(p.Ctx, delBatch, catalog.Row_ID))

	// Step 3: Verify StarCount
	count, err := rel.StarCount(p.Ctx)
	require.NoError(t, err)
	t.Logf("StarCount: %d, expected: 120 (100 committed + 50 uncommitted inserts - 30 uncommitted deletes)", count)
	require.Equal(t, uint64(120), count, "StarCount should be 100 + 50 - 30 = 120")

	vec1.Free(p.Mp)
	vec2.Free(p.Mp)

	require.NoError(t, txnop.Commit(p.Ctx))
}

// TestStarCountMixedInMemoryAndPersistedTombstones tests StarCount with both in-memory and persisted tombstones.
//
// Scenario:
// 1. Insert 100 rows via TAE (committed)
// 2. Start CN transaction
// 3. Delete 10 rows via rowid (in-memory tombstones)
// 4. Create a tombstone object for 15 rows and add via object-level delete (persisted tombstones)
// 5. Assert workspace has persisted tombstones
// 6. Verify StarCount = 100 - 10 - 15 = 75
//
// This tests the formula: StarCount = CommittedRows - InMemoryTombstones - PersistedTombstones.
func TestStarCountMixedInMemoryAndPersistedTombstones(t *testing.T) {
	p := testutil.InitEnginePack(testutil.TestOptions{}, t)
	defer p.Close()

	tae := p.T.GetDB()

	schema := catalog2.MockSchemaAll(3, 0)
	schema.Name = "test_table"

	txnop := p.StartCNTxn()
	_, rel := p.CreateDBAndTable(txnop, "db", schema)
	dbID := rel.GetDBID(p.Ctx)
	tableID := rel.GetTableID(p.Ctx)
	require.NoError(t, txnop.Commit(p.Ctx))

	require.NoError(t, p.D.SubscribeTable(p.Ctx, dbID, tableID, "db", schema.Name, false))

	{
		txn, _ := tae.StartTxn(nil)
		db, _ := txn.GetDatabase("db")
		rel, _ := db.GetRelationByName(schema.Name)
		bat := catalog2.MockBatch(schema, 100)
		require.NoError(t, rel.Append(p.Ctx, bat))
		require.NoError(t, txn.Commit(p.Ctx))
	}

	_, err := p.D.GetPartitionStateStats(p.Ctx, dbID, tableID)
	require.NoError(t, err)

	// Collect rowids for first batch (10 rows, in-memory deletes)
	var firstBatchRowIDs []types.Rowid
	{
		txn, _ := tae.StartTxn(nil)
		db, _ := txn.GetDatabase("db")
		rel, _ := db.GetRelationByName(schema.Name)
		it := rel.MakeObjectIt(false)
		for it.Next() {
			obj := it.GetObject()
			objID := obj.GetMeta().(*catalog2.ObjectEntry).ID()
			for blkIdx := 0; blkIdx < obj.BlkCnt(); blkIdx++ {
				blkID := objectio.NewBlockidWithObjectID(objID, uint16(blkIdx))
				for rowIdx := uint32(0); rowIdx < 10 && len(firstBatchRowIDs) < 10; rowIdx++ {
					rowid := objectio.NewRowid(&blkID, rowIdx)
					firstBatchRowIDs = append(firstBatchRowIDs, rowid)
				}
				if len(firstBatchRowIDs) >= 10 {
					break
				}
			}
			if len(firstBatchRowIDs) >= 10 {
				break
			}
		}
		require.NoError(t, txn.Commit(p.Ctx))
	}
	require.Equal(t, 10, len(firstBatchRowIDs))

	// Collect rowids for second batch (15 rows, for persisted tombstone object)
	var secondBatchRowIDs []types.Rowid
	{
		txn, _ := tae.StartTxn(nil)
		db, _ := txn.GetDatabase("db")
		rel, _ := db.GetRelationByName(schema.Name)
		it := rel.MakeObjectIt(false)
		for it.Next() {
			obj := it.GetObject()
			objID := obj.GetMeta().(*catalog2.ObjectEntry).ID()
			for blkIdx := 0; blkIdx < obj.BlkCnt(); blkIdx++ {
				blkID := objectio.NewBlockidWithObjectID(objID, uint16(blkIdx))
				for rowIdx := uint32(10); rowIdx < 25 && len(secondBatchRowIDs) < 15; rowIdx++ {
					rowid := objectio.NewRowid(&blkID, rowIdx)
					secondBatchRowIDs = append(secondBatchRowIDs, rowid)
				}
				if len(secondBatchRowIDs) >= 15 {
					break
				}
			}
			if len(secondBatchRowIDs) >= 15 {
				break
			}
		}
		require.NoError(t, txn.Commit(p.Ctx))
	}
	require.Equal(t, 15, len(secondBatchRowIDs))

	// Start CN transaction
	txnop = p.StartCNTxn()
	db, _ := p.D.Engine.Database(p.Ctx, "db", txnop)
	rel, _ = db.Relation(p.Ctx, schema.Name, nil)

	// Delete first batch (in-memory)
	vec1 := vector.NewVec(types.T_Rowid.ToType())
	vec2 := vector.NewVec(types.T_int8.ToType())
	for _, rowID := range firstBatchRowIDs {
		require.NoError(t, vector.AppendFixed(vec1, rowID, false, p.Mp))
		require.NoError(t, vector.AppendFixed(vec2, int8(0), false, p.Mp))
	}
	delBatch1 := batch.NewWithSize(2)
	delBatch1.SetRowCount(len(firstBatchRowIDs))
	delBatch1.Attrs = []string{catalog.Row_ID, catalog.TableTailAttrPKVal}
	delBatch1.Vecs[0] = vec1
	delBatch1.Vecs[1] = vec2
	require.NoError(t, rel.Delete(p.Ctx, delBatch1, catalog.Row_ID))

	// Create tombstone object for second batch and add as persisted DELETE (object-level delete)
	tombstoneBat := batch.NewWithSize(2)
	tombstoneBat.Vecs[0] = vector.NewVec(types.T_Rowid.ToType())
	tombstoneBat.Vecs[1] = vector.NewVec(types.T_int8.ToType())
	tombstoneBat.Attrs = []string{catalog.Row_ID, catalog.TableTailAttrPKVal}
	for _, rowid := range secondBatchRowIDs {
		require.NoError(t, vector.AppendFixed(tombstoneBat.Vecs[0], rowid, false, p.Mp))
	}
	for i := 0; i < len(secondBatchRowIDs); i++ {
		require.NoError(t, vector.AppendFixed(tombstoneBat.Vecs[1], int8(i+10), false, p.Mp))
	}
	tombstoneBat.SetRowCount(len(secondBatchRowIDs))

	proc := rel.GetProcess().(*process.Process)
	w := colexec.NewCNS3TombstoneWriter(proc.Mp(), proc.GetFileService(), types.T_int8.ToType(), -1)
	require.NoError(t, w.Write(p.Ctx, tombstoneBat))
	ss, err := w.Sync(p.Ctx)
	require.NoError(t, err)
	require.Equal(t, 1, len(ss))

	tombstoneBat.Vecs[0].Free(p.Mp)
	tombstoneBat.Vecs[1].Free(p.Mp)

	tbat := batch.NewWithSize(1)
	tbat.Attrs = []string{catalog.ObjectMeta_ObjectStats}
	tbat.Vecs[0] = vector.NewVec(types.T_binary.ToType())
	require.NoError(t, vector.AppendBytes(tbat.Vecs[0], ss[0].Marshal(), false, p.Mp))
	tbat.SetRowCount(1)
	require.NoError(t, rel.Delete(p.Ctx, tbat, catalog.Row_ID))

	assertWorkspaceHasPersistedTombstones(t, txnop, dbID, tableID)

	count, err := rel.StarCount(p.Ctx)
	require.NoError(t, err)
	t.Logf("StarCount: %d, expected: 75 (100 committed - 10 in-memory deletes - 15 persisted deletes)", count)
	require.Equal(t, uint64(75), count, "StarCount should be 100 - 10 - 15 = 75")

	vec1.Free(p.Mp)
	vec2.Free(p.Mp)
	tbat.Vecs[0].Free(p.Mp)
	require.NoError(t, txnop.Commit(p.Ctx))
}

// TestStarCountEmptyTableWithTombstones tests StarCount with uncommitted inserts and deletes (no initial committed data).
//
// Scenario:
// 1. Create empty table
// 2. Insert 100 rows via TAE (committed)
// 3. Start CN transaction, delete 30 rows (uncommitted)
// 4. Verify StarCount = 100 - 30 = 70
//
// This tests uncommitted deletes on a table that starts empty (no data before the test).
func TestStarCountEmptyTableWithTombstones(t *testing.T) {
	p := testutil.InitEnginePack(testutil.TestOptions{}, t)
	defer p.Close()

	tae := p.T.GetDB()

	schema := catalog2.MockSchemaAll(3, 0)
	schema.Name = "test_table"

	// Step 1: Create empty table via CN
	txnop := p.StartCNTxn()
	_, rel := p.CreateDBAndTable(txnop, "db", schema)
	dbID := rel.GetDBID(p.Ctx)
	tableID := rel.GetTableID(p.Ctx)
	require.NoError(t, txnop.Commit(p.Ctx))

	require.NoError(t, p.D.SubscribeTable(p.Ctx, dbID, tableID, "db", schema.Name, false))

	// Step 2: Insert 100 rows via TAE (committed)
	{
		txn, _ := tae.StartTxn(nil)
		db, _ := txn.GetDatabase("db")
		rel, _ := db.GetRelationByName(schema.Name)
		bat := catalog2.MockBatch(schema, 100)
		require.NoError(t, rel.Append(p.Ctx, bat))
		require.NoError(t, txn.Commit(p.Ctx))
	}

	_, err := p.D.GetPartitionStateStats(p.Ctx, dbID, tableID)
	require.NoError(t, err)

	// Step 3: Collect rowids for deletion
	var deleteRowIDs []types.Rowid
	{
		txn, _ := tae.StartTxn(nil)
		db, _ := txn.GetDatabase("db")
		taeRel, _ := db.GetRelationByName(schema.Name)

		it := taeRel.MakeObjectIt(false)
		for it.Next() {
			obj := it.GetObject()
			objID := obj.GetMeta().(*catalog2.ObjectEntry).ID()

			for blkIdx := 0; blkIdx < obj.BlkCnt(); blkIdx++ {
				blkID := objectio.NewBlockidWithObjectID(objID, uint16(blkIdx))
				for rowIdx := uint32(0); rowIdx < 30 && len(deleteRowIDs) < 30; rowIdx++ {
					rowid := objectio.NewRowid(&blkID, rowIdx)
					deleteRowIDs = append(deleteRowIDs, rowid)
				}
				if len(deleteRowIDs) >= 30 {
					break
				}
			}
			if len(deleteRowIDs) >= 30 {
				break
			}
		}
		it.Close()
		require.NoError(t, txn.Commit(p.Ctx))
	}
	require.Equal(t, 30, len(deleteRowIDs))

	// Step 4: Start CN transaction and delete 30 rows
	txnop = p.StartCNTxn()
	db, err := p.D.Engine.Database(p.Ctx, "db", txnop)
	require.NoError(t, err)
	rel, err = db.Relation(p.Ctx, schema.Name, nil)
	require.NoError(t, err)

	vec1 := vector.NewVec(types.T_Rowid.ToType())
	vec2 := vector.NewVec(types.T_int64.ToType())
	for _, rowID := range deleteRowIDs {
		vector.AppendFixed(vec1, rowID, false, p.Mp)
		vector.AppendFixed(vec2, int64(0), false, p.Mp)
	}

	delBatch := batch.NewWithSize(2)
	delBatch.SetRowCount(len(deleteRowIDs))
	delBatch.Attrs = []string{catalog.Row_ID, catalog.TableTailAttrPKVal}
	delBatch.Vecs[0] = vec1
	delBatch.Vecs[1] = vec2

	require.NoError(t, rel.Delete(p.Ctx, delBatch, catalog.Row_ID))

	// Step 5: Verify StarCount
	count, err := rel.StarCount(p.Ctx)
	require.NoError(t, err)
	t.Logf("StarCount: %d, expected: 70 (100 committed - 30 uncommitted deletes)", count)
	require.Equal(t, uint64(70), count, "StarCount should be 100 - 30 = 70")

	vec1.Free(p.Mp)
	vec2.Free(p.Mp)

	require.NoError(t, txnop.Commit(p.Ctx))
}

// TestStarCountMultipleDeletes tests StarCount with multiple DELETE operations in the same transaction.
//
// Scenario:
// 1. Insert 100 rows via TAE (committed)
// 2. Start CN transaction
// 3. Delete 20 rows (first DELETE)
// 4. Delete another 15 rows (second DELETE)
// 5. Delete another 10 rows (third DELETE)
// 6. Verify StarCount = 100 - 20 - 15 - 10 = 55
//
// This tests that tombstones accumulate correctly across multiple DELETE operations.
func TestStarCountMultipleDeletes(t *testing.T) {
	p := testutil.InitEnginePack(testutil.TestOptions{}, t)
	defer p.Close()

	tae := p.T.GetDB()

	schema := catalog2.MockSchemaAll(3, 0)
	schema.Name = "test_table"

	// Step 1: Create table and insert 100 rows
	txnop := p.StartCNTxn()
	_, rel := p.CreateDBAndTable(txnop, "db", schema)
	dbID := rel.GetDBID(p.Ctx)
	tableID := rel.GetTableID(p.Ctx)
	require.NoError(t, txnop.Commit(p.Ctx))

	require.NoError(t, p.D.SubscribeTable(p.Ctx, dbID, tableID, "db", schema.Name, false))

	{
		txn, _ := tae.StartTxn(nil)
		db, _ := txn.GetDatabase("db")
		rel, _ := db.GetRelationByName(schema.Name)
		bat := catalog2.MockBatch(schema, 100)
		require.NoError(t, rel.Append(p.Ctx, bat))
		require.NoError(t, txn.Commit(p.Ctx))
	}

	_, err := p.D.GetPartitionStateStats(p.Ctx, dbID, tableID)
	require.NoError(t, err)

	// Step 2: Collect rowids for three batches of deletes
	var batch1RowIDs, batch2RowIDs, batch3RowIDs []types.Rowid
	{
		txn, _ := tae.StartTxn(nil)
		db, _ := txn.GetDatabase("db")
		taeRel, _ := db.GetRelationByName(schema.Name)

		it := taeRel.MakeObjectIt(false)
		for it.Next() {
			obj := it.GetObject()
			objID := obj.GetMeta().(*catalog2.ObjectEntry).ID()

			for blkIdx := 0; blkIdx < obj.BlkCnt(); blkIdx++ {
				blkID := objectio.NewBlockidWithObjectID(objID, uint16(blkIdx))

				// Collect rowids for batch 1 (rows 0-19)
				for rowIdx := uint32(0); rowIdx < 20 && len(batch1RowIDs) < 20; rowIdx++ {
					rowid := objectio.NewRowid(&blkID, rowIdx)
					batch1RowIDs = append(batch1RowIDs, rowid)
				}

				// Collect rowids for batch 2 (rows 20-34)
				for rowIdx := uint32(20); rowIdx < 35 && len(batch2RowIDs) < 15; rowIdx++ {
					rowid := objectio.NewRowid(&blkID, rowIdx)
					batch2RowIDs = append(batch2RowIDs, rowid)
				}

				// Collect rowids for batch 3 (rows 35-44)
				for rowIdx := uint32(35); rowIdx < 45 && len(batch3RowIDs) < 10; rowIdx++ {
					rowid := objectio.NewRowid(&blkID, rowIdx)
					batch3RowIDs = append(batch3RowIDs, rowid)
				}

				if len(batch1RowIDs) >= 20 && len(batch2RowIDs) >= 15 && len(batch3RowIDs) >= 10 {
					break
				}
			}
			if len(batch1RowIDs) >= 20 && len(batch2RowIDs) >= 15 && len(batch3RowIDs) >= 10 {
				break
			}
		}
		it.Close()
		require.NoError(t, txn.Commit(p.Ctx))
	}
	require.Equal(t, 20, len(batch1RowIDs))
	require.Equal(t, 15, len(batch2RowIDs))
	require.Equal(t, 10, len(batch3RowIDs))

	// Step 3: Start CN transaction and perform three DELETE operations
	txnop = p.StartCNTxn()
	db, err := p.D.Engine.Database(p.Ctx, "db", txnop)
	require.NoError(t, err)
	rel, err = db.Relation(p.Ctx, schema.Name, nil)
	require.NoError(t, err)

	// First DELETE: 20 rows
	{
		vec1 := vector.NewVec(types.T_Rowid.ToType())
		vec2 := vector.NewVec(types.T_int64.ToType())
		for _, rowID := range batch1RowIDs {
			vector.AppendFixed(vec1, rowID, false, p.Mp)
			vector.AppendFixed(vec2, int64(0), false, p.Mp)
		}

		delBatch := batch.NewWithSize(2)
		delBatch.SetRowCount(len(batch1RowIDs))
		delBatch.Attrs = []string{catalog.Row_ID, catalog.TableTailAttrPKVal}
		delBatch.Vecs[0] = vec1
		delBatch.Vecs[1] = vec2

		require.NoError(t, rel.Delete(p.Ctx, delBatch, catalog.Row_ID))
		vec1.Free(p.Mp)
		vec2.Free(p.Mp)
	}

	// Verify count after first delete
	count, err := rel.StarCount(p.Ctx)
	require.NoError(t, err)
	require.Equal(t, uint64(80), count, "Count should be 80 after first delete")

	// Second DELETE: 15 rows
	{
		vec1 := vector.NewVec(types.T_Rowid.ToType())
		vec2 := vector.NewVec(types.T_int64.ToType())
		for _, rowID := range batch2RowIDs {
			vector.AppendFixed(vec1, rowID, false, p.Mp)
			vector.AppendFixed(vec2, int64(0), false, p.Mp)
		}

		delBatch := batch.NewWithSize(2)
		delBatch.SetRowCount(len(batch2RowIDs))
		delBatch.Attrs = []string{catalog.Row_ID, catalog.TableTailAttrPKVal}
		delBatch.Vecs[0] = vec1
		delBatch.Vecs[1] = vec2

		require.NoError(t, rel.Delete(p.Ctx, delBatch, catalog.Row_ID))
		vec1.Free(p.Mp)
		vec2.Free(p.Mp)
	}

	// Verify count after second delete
	count, err = rel.StarCount(p.Ctx)
	require.NoError(t, err)
	require.Equal(t, uint64(65), count, "Count should be 65 after second delete")

	// Third DELETE: 10 rows
	{
		vec1 := vector.NewVec(types.T_Rowid.ToType())
		vec2 := vector.NewVec(types.T_int64.ToType())
		for _, rowID := range batch3RowIDs {
			vector.AppendFixed(vec1, rowID, false, p.Mp)
			vector.AppendFixed(vec2, int64(0), false, p.Mp)
		}

		delBatch := batch.NewWithSize(2)
		delBatch.SetRowCount(len(batch3RowIDs))
		delBatch.Attrs = []string{catalog.Row_ID, catalog.TableTailAttrPKVal}
		delBatch.Vecs[0] = vec1
		delBatch.Vecs[1] = vec2

		require.NoError(t, rel.Delete(p.Ctx, delBatch, catalog.Row_ID))
		vec1.Free(p.Mp)
		vec2.Free(p.Mp)
	}

	// Step 4: Verify final StarCount
	count, err = rel.StarCount(p.Ctx)
	require.NoError(t, err)
	t.Logf("StarCount: %d, expected: 55 (100 - 20 - 15 - 10)", count)
	require.Equal(t, uint64(55), count, "StarCount should be 100 - 20 - 15 - 10 = 55")

	require.NoError(t, txnop.Commit(p.Ctx))
}

// TestStarCountTransferredTombstones tests StarCount with transferred tombstones after merge.
//
// Scenario:
// 1. Insert 10 rows via TAE (committed, in appendable object)
// 2. Start CN transaction, delete 2 rows
// 3. Force flush tombstones (persisted, pointing to appendable object)
// 4. Trigger merge: appendable object  non-appendable object (new objectID)
// 5. Transfer happens: old tombstones  new tombstones (pointing to new object)
// 6. Verify StarCount = 10 - 2 = 8
//
// This tests that visibility check correctly filters old tombstones (pointing to deleted appendable object)
// and only counts new tombstones (pointing to new non-appendable object).

// TestStarCountTransferredTombstones tests StarCount with transferred tombstones after merge.
//
// Scenario:
// 1. Insert 10 rows via TAE (committed, in appendable object)
// 2. Start CN transaction
// 3. Construct persisted tombstone object (2 rows) and write to workspace
// 4. Verify StarCount = 10 - 2 = 8 before merge
// 5. Trigger merge: appendable object  non-appendable object
// 6. Transfer happens: old tombstones  new tombstones
// 7. Verify StarCount = 10 - 2 = 8 after merge (visibility check filters old tombstones)

// TestStarCountTransferredTombstones tests StarCount with transferred tombstones after merge.
//
// Scenario:
// 1. Insert 10 rows via TAE (committed, in appendable object)
// 2. Start CN transaction
// 3. Construct persisted tombstone object (2 rows) and write to workspace
// 4. Verify StarCount = 10 - 2 = 8 before merge
// 5. Trigger merge: appendable object  non-appendable object
// 6. Transfer happens: old tombstones  new tombstones
// 7. Verify StarCount = 10 - 2 = 8 after merge (visibility check filters old tombstones)
//
// This tests that visibility check correctly handles transferred tombstones:
// - Old tombstones point to deleted appendable object (invisible)  filtered out
// - New tombstones point to new non-appendable object (visible)  counted

// TestStarCountTransferredTombstones tests StarCount with transferred tombstones after merge.
//
// Scenario:
// 1. Insert 10 rows and flush (create appendable object)
// 2. Start CN transaction (snapshot before merge)
// 3. Delete 2 rows in CN transaction (in-memory tombstones)
// 4. Trigger merge in background (appendable  non-appendable, old object deleted)
// 5. Force transfer check on commit
// 6. Verify StarCount = 8 in new transaction
//
// This tests that transfer correctly handles tombstones when data objects change:
// - Old in-memory tombstones point to deleted appendable object
// - Transfer creates new persisted tombstones pointing to new non-appendable object
// - Visibility check filters old tombstones, counts new ones

// TestStarCountWithTransferredTombstones tests StarCount with tombstone object transfer.
// Scenario: tombstone objects point to old data objects, merge creates new data objects,
// transfer updates tombstone objects to point to new data objects.
func TestStarCountWithTransferredTombstones(t *testing.T) {
	opts := config.WithLongScanAndCKPOpts(nil)
	p := testutil.InitEnginePack(testutil.TestOptions{TaeEngineOptions: opts}, t)
	defer p.Close()

	schema := catalog2.MockSchemaEnhanced(1, 0, 2)
	schema.Name = "test_table"

	// Create table
	txnop := p.StartCNTxn()
	_, rel := p.CreateDBAndTable(txnop, "db", schema)
	dbID := rel.GetDBID(p.Ctx)
	tableID := rel.GetTableID(p.Ctx)
	require.NoError(t, txnop.Commit(p.Ctx))

	bat := catalog2.MockBatch(schema, 10)

	// Insert data via TN and compact
	{
		tnTxn, _ := p.T.GetDB().StartTxn(nil)
		tnDB, _ := tnTxn.GetDatabase("db")
		tnRel, _ := tnDB.GetRelationByName(schema.Name)
		require.NoError(t, tnRel.Append(p.Ctx, bat))
		require.NoError(t, tnTxn.Commit(p.Ctx))

		testutil2.CompactBlocks(t, 0, p.T.GetDB(), "db", schema, true)
	}

	// Start CN transaction
	_, _, cnTxnOp, err := p.D.GetTable(p.Ctx, "db", schema.Name)
	require.NoError(t, err)
	cnTxnOp.GetWorkspace().StartStatement()
	require.NoError(t, cnTxnOp.GetWorkspace().IncrStatementID(p.Ctx, false))

	// Read rowids and PKs for tombstones
	tombstoneBat := batch.NewWithSize(2)
	tombstoneBat.Vecs[0] = vector.NewVec(types.T_Rowid.ToType())
	tombstoneBat.Vecs[1] = vector.NewVec(types.T_int32.ToType())
	{
		txn, _, reader, err := testutil.GetTableTxnReader(p.Ctx, p.D, "db", schema.Name, nil, p.Mp, t)
		require.NoError(t, err)
		ret := testutil.EmptyBatchFromSchema(schema)
		for {
			done, err := reader.Read(p.Ctx, []string{schema.GetPrimaryKey().Name, catalog.Row_ID}, nil, p.Mp, ret)
			if done {
				break
			}
			require.NoError(t, err)
			// Only delete 5 rows (half)
			for i := 0; i < ret.RowCount() && tombstoneBat.Vecs[0].Length() < 5; i++ {
				vector.AppendFixed[types.Rowid](tombstoneBat.Vecs[0],
					vector.GetFixedAtNoTypeCheck[types.Rowid](ret.Vecs[1], i), false, p.Mp)
				vector.AppendFixed[int32](tombstoneBat.Vecs[1],
					vector.GetFixedAtNoTypeCheck[int32](ret.Vecs[0], i), false, p.Mp)
			}
		}
		require.NoError(t, txn.Commit(p.Ctx))
		tombstoneBat.SetRowCount(tombstoneBat.Vecs[0].Length())
		require.Equal(t, 5, tombstoneBat.RowCount())
	}

	// Merge data objects (creates new object, deletes old)
	testutil2.MergeBlocks(t, 0, p.T.GetDB(), "db", schema, true)
	_, err = p.D.GetPartitionStateStats(p.Ctx, dbID, tableID)
	require.NoError(t, err)

	// Create tombstone object and write to workspace
	{
		db, _ := p.D.Engine.Database(p.Ctx, "db", cnTxnOp)
		rel, _ := db.Relation(p.Ctx, schema.Name, nil)
		proc := rel.GetProcess().(*process.Process)

		w := colexec.NewCNS3TombstoneWriter(proc.Mp(), proc.GetFileService(), types.T_int32.ToType(), -1)
		require.NoError(t, w.Write(p.Ctx, tombstoneBat))
		ss, err := w.Sync(p.Ctx)
		require.NoError(t, err)
		require.Equal(t, 1, len(ss))

		tbat := batch.NewWithSize(1)
		tbat.Attrs = []string{catalog.ObjectMeta_ObjectStats}
		tbat.Vecs[0] = vector.NewVec(types.T_text.ToType())
		vector.AppendBytes(tbat.Vecs[0], ss[0].Marshal(), false, p.Mp)
		tbat.SetRowCount(1)

		transaction := cnTxnOp.GetWorkspace().(*disttae.Transaction)
		require.NoError(t, transaction.WriteFile(
			disttae.DELETE, 0, dbID, tableID, "db", schema.Name,
			ss[0].ObjectLocation().String(), tbat, p.D.Engine.GetTNServices()[0]))

		require.NoError(t, cnTxnOp.UpdateSnapshot(p.Ctx, p.D.Now()))
	}

	// Note: tombstone objects don't affect StarCount until commit
	// This is different from in-memory tombstones

	// Commit with force transfer
	ctx := context.WithValue(p.Ctx, disttae.UT_ForceTransCheck{}, "yes")
	require.NoError(t, cnTxnOp.Commit(ctx))

	// Verify in new transaction after transfer: 10 - 5 = 5
	{
		txnop := p.StartCNTxn()
		db, _ := p.D.Engine.Database(p.Ctx, "db", txnop)
		rel, _ := db.Relation(p.Ctx, schema.Name, nil)
		count, err := rel.StarCount(p.Ctx)
		require.NoError(t, err)
		require.Equal(t, uint64(5), count)
		require.NoError(t, txnop.Commit(p.Ctx))
	}
}

// TestStarCountWithTransferredInMemoryTombstones tests StarCount with in-memory tombstone transfer.
// Scenario: CN deletes rows (in-memory tombstones), then merge happens, transfer updates rowids.
func TestStarCountWithTransferredInMemoryTombstones(t *testing.T) {
	opts := config.WithLongScanAndCKPOpts(nil)
	p := testutil.InitEnginePack(testutil.TestOptions{TaeEngineOptions: opts}, t)
	defer p.Close()

	schema := catalog2.MockSchemaEnhanced(1, 0, 2)
	schema.Name = "test_table"

	// Create table
	txnop := p.StartCNTxn()
	_, _ = p.CreateDBAndTable(txnop, "db", schema)
	require.NoError(t, txnop.Commit(p.Ctx))

	bat := catalog2.MockBatch(schema, 10)

	// Insert data via TN and compact
	{
		tnTxn, _ := p.T.GetDB().StartTxn(nil)
		tnDB, _ := tnTxn.GetDatabase("db")
		tnRel, _ := tnDB.GetRelationByName(schema.Name)
		require.NoError(t, tnRel.Append(p.Ctx, bat))
		require.NoError(t, tnTxn.Commit(p.Ctx))
		testutil2.CompactBlocks(t, 0, p.T.GetDB(), "db", schema, true)
	}

	// Start CN transaction
	_, _, cnTxnOp, err := p.D.GetTable(p.Ctx, "db", schema.Name)
	require.NoError(t, err)
	cnTxnOp.GetWorkspace().StartStatement()
	require.NoError(t, cnTxnOp.GetWorkspace().IncrStatementID(p.Ctx, false))

	// Delete 5 rows using in-memory tombstones
	{
		txn, _, reader, err := testutil.GetTableTxnReader(p.Ctx, p.D, "db", schema.Name, nil, p.Mp, t)
		require.NoError(t, err)
		ret := testutil.EmptyBatchFromSchema(schema)

		vec1 := vector.NewVec(types.T_Rowid.ToType())
		vec2 := vector.NewVec(types.T_int32.ToType())
		for {
			done, err := reader.Read(p.Ctx, []string{schema.GetPrimaryKey().Name, catalog.Row_ID}, nil, p.Mp, ret)
			if done {
				break
			}
			require.NoError(t, err)
			for i := 0; i < ret.RowCount() && vec1.Length() < 5; i++ {
				vector.AppendFixed[types.Rowid](vec1,
					vector.GetFixedAtNoTypeCheck[types.Rowid](ret.Vecs[1], i), false, p.Mp)
				vector.AppendFixed[int32](vec2,
					vector.GetFixedAtNoTypeCheck[int32](ret.Vecs[0], i), false, p.Mp)
			}
		}
		require.NoError(t, txn.Commit(p.Ctx))

		delBatch := batch.NewWithSize(2)
		delBatch.SetRowCount(5)
		delBatch.Attrs = []string{catalog.Row_ID, catalog.TableTailAttrPKVal}
		delBatch.Vecs[0] = vec1
		delBatch.Vecs[1] = vec2

		db, _ := p.D.Engine.Database(p.Ctx, "db", cnTxnOp)
		rel, _ := db.Relation(p.Ctx, schema.Name, nil)
		require.NoError(t, rel.Delete(p.Ctx, delBatch, catalog.Row_ID))

		// Verify StarCount before merge: 10 - 5 = 5
		count, err := rel.StarCount(p.Ctx)
		require.NoError(t, err)
		require.Equal(t, uint64(5), count)
	}

	// Merge data objects (creates new object, deletes old)
	testutil2.MergeBlocks(t, 0, p.T.GetDB(), "db", schema, true)

	// Wait for logtail sync and update snapshot to see the merge
	{
		db, _ := p.D.Engine.Database(p.Ctx, "db", cnTxnOp)
		rel, _ := db.Relation(p.Ctx, schema.Name, nil)
		_, err := p.D.GetPartitionStateStats(p.Ctx, rel.GetDBID(p.Ctx), rel.GetTableID(p.Ctx))
		require.NoError(t, err)
	}
	require.NoError(t, cnTxnOp.UpdateSnapshot(p.Ctx, p.D.Now()))

	// Commit with force transfer
	ctx := context.WithValue(p.Ctx, disttae.UT_ForceTransCheck{}, "yes")
	require.NoError(t, cnTxnOp.Commit(ctx))

	// Verify in new transaction after transfer: 10 - 5 = 5
	{
		txnop := p.StartCNTxn()
		db, _ := p.D.Engine.Database(p.Ctx, "db", txnop)
		rel, _ := db.Relation(p.Ctx, schema.Name, nil)
		count, err := rel.StarCount(p.Ctx)
		require.NoError(t, err)
		require.Equal(t, uint64(5), count)
		require.NoError(t, txnop.Commit(p.Ctx))
	}
}
