// Copyright 2021 Matrix Origin
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#include "textflag.h"

// func Crc32BytesHash(data unsafe.Pointer, length int) uint64
// Requires: SSE4.2
TEXT ·Crc32BytesHash(SB), NOSPLIT, $0-24
	MOVQ data+0(FP), SI
	MOVQ length+8(FP), CX
	MOVQ CX, BX
	MOVQ $-1, DI
	ADDQ SI, CX
	SUBQ $8, CX

loop:
	CMPQ   SI, CX
	JGE    done
	CRC32Q (SI), DI
	ADDQ   $8, SI
	JMP    loop

done:
	CRC32Q (CX), DI
	MOVL   DI, ret+16(FP)
	MOVL   BX, ret+20(FP)
	RET

// func Crc32Int64Hash(data uint64) uint64
// Requires: SSE4.2
TEXT ·Crc32Int64Hash(SB), NOSPLIT, $0-16
	MOVQ   $-1, SI
	CRC32Q data+0(FP), SI
	MOVQ   SI, ret+8(FP)
	RET

// func Crc32Int64BatchHash(data *uint64, hashes *uint64, length int)
// Requires: SSE4.2
TEXT ·Crc32Int64BatchHash(SB), NOSPLIT, $0-24
	MOVQ data+0(FP), SI
	MOVQ hashes+8(FP), DI
	MOVQ length+16(FP), CX

loop:
	SUBQ $8, CX
	JL   tail

	MOVQ $-1, R8
	MOVQ $-1, R9
	MOVQ $-1, R10
	MOVQ $-1, R11
	MOVQ $-1, R12
	MOVQ $-1, R13
	MOVQ $-1, R14
	MOVQ $-1, R15

	CRC32Q (SI), R8
	CRC32Q 8(SI), R9
	CRC32Q 16(SI), R10
	CRC32Q 24(SI), R11
	CRC32Q 32(SI), R12
	CRC32Q 40(SI), R13
	CRC32Q 48(SI), R14
	CRC32Q 56(SI), R15

	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ R13, 40(DI)
	MOVQ R14, 48(DI)
	MOVQ R15, 56(DI)

	ADDQ $64, SI
	ADDQ $64, DI
	JMP  loop

tail:
	ADDQ $8, CX
	JE   done

tailLoop:
	MOVQ   $-1, R8
	CRC32Q (SI), R8
	MOVQ   R8, (DI)

	ADDQ $8, SI
	ADDQ $8, DI
	LOOP tailLoop

done:
	RET

// func Crc32Int64CellBatchHash(data *uint64, hashes *uint64, length int)
// Requires: SSE4.2
TEXT ·Crc32Int64CellBatchHash(SB), NOSPLIT, $0-24
	MOVQ data+0(FP), SI
	MOVQ hashes+8(FP), DI
	MOVQ length+16(FP), CX

loop:
	SUBQ $8, CX
	JL   tail

	MOVQ $-1, R8
	MOVQ $-1, R9
	MOVQ $-1, R10
	MOVQ $-1, R11
	MOVQ $-1, R12
	MOVQ $-1, R13
	MOVQ $-1, R14
	MOVQ $-1, R15

	CRC32Q (SI), R8
	CRC32Q 16(SI), R9
	CRC32Q 32(SI), R10
	CRC32Q 48(SI), R11
	CRC32Q 64(SI), R12
	CRC32Q 80(SI), R13
	CRC32Q 96(SI), R14
	CRC32Q 112(SI), R15

	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ R13, 40(DI)
	MOVQ R14, 48(DI)
	MOVQ R15, 56(DI)

	ADDQ $128, SI
	ADDQ $64, DI
	JMP  loop

tail:
	ADDQ $8, CX
	JE   done

tailLoop:
	MOVQ   $-1, R8
	CRC32Q (SI), R8
	MOVQ   R8, (DI)

	ADDQ $16, SI
	ADDQ $8, DI
	LOOP tailLoop

done:
	RET

// func Crc32Int192Hash(data *[3]uint64) uint64
// Requires: SSE4.2
TEXT ·Crc32Int192Hash(SB), NOSPLIT, $0-16
	MOVQ   data+0(FP), SI
	MOVQ   $-1, DI
	CRC32Q (SI), DI
	CRC32Q 8(SI), DI
	CRC32Q 16(SI), DI
	MOVQ   DI, ret+8(FP)
	RET

// func Crc32Int256Hash(data *[4]uint64) uint64
// Requires: SSE4.2
TEXT ·Crc32Int256Hash(SB), NOSPLIT, $0-16
	MOVQ   ata+0(FP), SI
	MOVQ   $-1, DI
	CRC32Q (SI), DI
	CRC32Q 8(SI), DI
	CRC32Q 16(SI), DI
	CRC32Q 24(SI), DI
	MOVQ   DI, ret+8(FP)
	RET

// func Crc32Int320Hash(data *[4]uint64) uint64
// Requires: SSE4.2
TEXT ·Crc32Int320Hash(SB), NOSPLIT, $0-16
	MOVQ   data+0(FP), SI
	MOVQ   $-1, DI
	CRC32Q (SI), DI
	CRC32Q 8(SI), DI
	CRC32Q 16(SI), DI
	CRC32Q 24(SI), DI
	CRC32Q 32(SI), DI
	MOVQ   DI, ret+8(FP)
	RET

// func Crc32Int192BatchHash(data *[3]uint64, hashes *uint64, length int)
// Requires: SSE4.2
TEXT ·Crc32Int192BatchHash(SB), NOSPLIT, $0-24
	MOVQ data+0(FP), SI
	MOVQ hashes+8(FP), DI
	MOVQ length+16(FP), CX

loop:
	SUBQ $8, CX
	JL   tail

	MOVQ $-1, R8
	MOVQ $-1, R9
	MOVQ $-1, R10
	MOVQ $-1, R11
	MOVQ $-1, R12
	MOVQ $-1, R13
	MOVQ $-1, R14
	MOVQ $-1, R15

	CRC32Q (SI), R8
	CRC32Q 8(SI), R8
	CRC32Q 16(SI), R8
	CRC32Q 24(SI), R9
	CRC32Q 32(SI), R9
	CRC32Q 40(SI), R9
	CRC32Q 48(SI), R10
	CRC32Q 56(SI), R10
	CRC32Q 64(SI), R10
	CRC32Q 72(SI), R11
	CRC32Q 80(SI), R11
	CRC32Q 88(SI), R11
	CRC32Q 96(SI), R12
	CRC32Q 104(SI), R12
	CRC32Q 112(SI), R12
	CRC32Q 120(SI), R13
	CRC32Q 128(SI), R13
	CRC32Q 136(SI), R13
	CRC32Q 144(SI), R14
	CRC32Q 152(SI), R14
	CRC32Q 160(SI), R14
	CRC32Q 168(SI), R15
	CRC32Q 176(SI), R15
	CRC32Q 184(SI), R15

	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ R13, 40(DI)
	MOVQ R14, 48(DI)
	MOVQ R15, 56(DI)

	ADDQ $192, SI
	ADDQ $64, DI
	JMP  loop

tail:
	ADDQ $8, CX
	JE   done

tailLoop:
	MOVQ $-1, R8

	CRC32Q (SI), R8
	CRC32Q 8(SI), R8
	CRC32Q 16(SI), R8

	MOVQ R8, (DI)

	ADDQ $24, SI
	ADDQ $8, DI
	LOOP tailLoop

done:
	RET

// func Crc32Int256BatchHash(data *[4]uint64, hashes *uint64, length int)
// Requires: SSE4.2
TEXT ·Crc32Int256BatchHash(SB), NOSPLIT, $0-24
	MOVQ data+0(FP), SI
	MOVQ hashes+8(FP), DI
	MOVQ length+16(FP), CX

loop:
	SUBQ $8, CX
	JL   tail

	MOVQ $-1, R8
	MOVQ $-1, R9
	MOVQ $-1, R10
	MOVQ $-1, R11
	MOVQ $-1, R12
	MOVQ $-1, R13
	MOVQ $-1, R14
	MOVQ $-1, R15

	CRC32Q (SI), R8
	CRC32Q 8(SI), R8
	CRC32Q 16(SI), R8
	CRC32Q 24(SI), R8
	CRC32Q 32(SI), R9
	CRC32Q 40(SI), R9
	CRC32Q 48(SI), R9
	CRC32Q 56(SI), R9
	CRC32Q 64(SI), R10
	CRC32Q 72(SI), R10
	CRC32Q 80(SI), R10
	CRC32Q 88(SI), R10
	CRC32Q 96(SI), R11
	CRC32Q 104(SI), R11
	CRC32Q 112(SI), R11
	CRC32Q 120(SI), R11
	CRC32Q 128(SI), R12
	CRC32Q 136(SI), R12
	CRC32Q 144(SI), R12
	CRC32Q 152(SI), R12
	CRC32Q 160(SI), R13
	CRC32Q 168(SI), R13
	CRC32Q 176(SI), R13
	CRC32Q 184(SI), R13
	CRC32Q 192(SI), R14
	CRC32Q 200(SI), R14
	CRC32Q 208(SI), R14
	CRC32Q 216(SI), R14
	CRC32Q 224(SI), R15
	CRC32Q 232(SI), R15
	CRC32Q 240(SI), R15
	CRC32Q 248(SI), R15

	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ R13, 40(DI)
	MOVQ R14, 48(DI)
	MOVQ R15, 56(DI)

	ADDQ $256, SI
	ADDQ $64, DI
	JMP  loop

tail:
	ADDQ $8, CX
	JE   done

tailLoop:
	MOVQ $-1, R8

	CRC32Q (SI), R8
	CRC32Q 8(SI), R8
	CRC32Q 16(SI), R8
	CRC32Q 24(SI), R8

	MOVQ R8, (DI)

	ADDQ $32, SI
	ADDQ $8, DI
	LOOP tailLoop

done:
	RET

// func Crc32Int320BatchHash(data *[5]uint64, hashes *uint64, length int)
// Requires: SSE4.2
TEXT ·Crc32Int320BatchHash(SB), NOSPLIT, $0-24
	MOVQ data+0(FP), SI
	MOVQ hashes+8(FP), DI
	MOVQ length+16(FP), CX

loop:
	SUBQ $8, CX
	JL   tail

	MOVQ $-1, R8
	MOVQ $-1, R9
	MOVQ $-1, R10
	MOVQ $-1, R11
	MOVQ $-1, R12
	MOVQ $-1, R13
	MOVQ $-1, R14
	MOVQ $-1, R15

	CRC32Q (SI), R8
	CRC32Q 8(SI), R8
	CRC32Q 16(SI), R8
	CRC32Q 24(SI), R8
	CRC32Q 32(SI), R8
	CRC32Q 40(SI), R9
	CRC32Q 48(SI), R9
	CRC32Q 56(SI), R9
	CRC32Q 64(SI), R9
	CRC32Q 72(SI), R9
	CRC32Q 80(SI), R10
	CRC32Q 88(SI), R10
	CRC32Q 96(SI), R10
	CRC32Q 104(SI), R10
	CRC32Q 112(SI), R10
	CRC32Q 120(SI), R11
	CRC32Q 128(SI), R11
	CRC32Q 136(SI), R11
	CRC32Q 144(SI), R11
	CRC32Q 152(SI), R11
	CRC32Q 160(SI), R12
	CRC32Q 168(SI), R12
	CRC32Q 176(SI), R12
	CRC32Q 184(SI), R12
	CRC32Q 192(SI), R12
	CRC32Q 200(SI), R13
	CRC32Q 208(SI), R13
	CRC32Q 216(SI), R13
	CRC32Q 224(SI), R13
	CRC32Q 232(SI), R13
	CRC32Q 240(SI), R14
	CRC32Q 248(SI), R14
	CRC32Q 256(SI), R14
	CRC32Q 264(SI), R14
	CRC32Q 272(SI), R14
	CRC32Q 280(SI), R15
	CRC32Q 288(SI), R15
	CRC32Q 296(SI), R15
	CRC32Q 304(SI), R15
	CRC32Q 312(SI), R15

	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ R13, 40(DI)
	MOVQ R14, 48(DI)
	MOVQ R15, 56(DI)

	ADDQ $320, SI
	ADDQ $64, DI
	JMP  loop

tail:
	ADDQ $8, CX
	JE   done

tailLoop:
	MOVQ $-1, R8

	CRC32Q (SI), R8
	CRC32Q 8(SI), R8
	CRC32Q 16(SI), R8
	CRC32Q 24(SI), R8
	CRC32Q 32(SI), R8

	MOVQ R8, (DI)

	ADDQ $40, SI
	ADDQ $8, DI
	LOOP tailLoop

done:
	RET

////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////

DATA aesIV<>+0x00(SB)/8, $0x5A8279996ED9EBA1
DATA aesIV<>+0x08(SB)/8, $0x8F1BBCDCCA62C1D6
GLOBL aesIV<>(SB), (NOPTR+RODATA), $16

// func AesBytesHash(data unsafe.Pointer, length int) [2]uint64
// Requires: AES
TEXT ·AesBytesHash(SB), NOSPLIT, $0-32
	MOVQ data+0(FP), SI
	MOVQ length+8(FP), CX
	ADDQ SI, CX
	SUBQ $64, CX

	VMOVDQU aesIV<>+0(SB), X0
	VMOVDQU X0, X1
	VMOVDQU X0, X2
	VMOVDQU X1, X3

loop:
	CMPQ SI, CX
	JGE  tail0

	VAESENC (SI), X0, X0
	VAESENC 16(SI), X1, X1
	VAESENC 32(SI), X2, X2
	VAESENC 48(SI), X3, X3

	ADDQ $64, SI
	JMP  loop

tail0:
	ADDQ $48, CX

	CMPQ SI, CX
	JGE  tail1

	VAESENC (SI), X0, X0
	ADDQ    $16, SI

tail1:
	CMPQ SI, CX
	JGE  tail2

	VAESENC (SI), X1, X1
	ADDQ    $16, SI

tail2:
	CMPQ SI, CX
	JGE  tail3

	VAESENC (SI), X2, X2

tail3:
	VAESENC (CX), X3, X3

	VAESENC X1, X0, X0
	VAESENC X2, X3, X3
	VAESENC X3, X0, X0

	VMOVDQU X0, ret+16(FP)

	RET

// func AesInt192BatchHash(data *[3]uint64, hashes *uint64, length int)
// Requires: AES
TEXT ·AesInt192BatchHash(SB), NOSPLIT, $0-24
	MOVQ data+0(FP), SI
	MOVQ hashes+8(FP), DI
	MOVQ length+16(FP), CX

	VMOVDQU aesIV<>+0(SB), X0
	VAESENC X0, X0, X1

loop:
	SUBQ $7, CX
	JL   tail

	VMOVDQU X0, X2
	VMOVDQU X0, X3
	VAESENC (SI), X2, X2
	VAESENC 8(SI), X3, X3
	VAESENC X2, X3, X3
	VAESENC X1, X3, X3
	VAESENC X3, X3, X3
	VPSHUFD $0x4e, X3, X2
	VPXOR   X2, X3, X2
	MOVQ    X2, (DI)

	VMOVDQU X0, X4
	VMOVDQU X0, X5
	VAESENC 24(SI), X4, X4
	VAESENC 32(SI), X5, X5
	VAESENC X4, X5, X5
	VAESENC X1, X5, X5
	VAESENC X5, X5, X5
	VPSHUFD $0x4e, X5, X4
	VPXOR   X4, X5, X4
	MOVQ    X4, 8(DI)

	VMOVDQU X0, X6
	VMOVDQU X0, X7
	VAESENC 48(SI), X6, X6
	VAESENC 56(SI), X7, X7
	VAESENC X6, X7, X7
	VAESENC X1, X7, X7
	VAESENC X7, X7, X7
	VPSHUFD $0x4e, X7, X6
	VPXOR   X6, X7, X6
	MOVQ    X6, 16(DI)

	VMOVDQU X0, X8
	VMOVDQU X0, X9
	VAESENC 72(SI), X8, X8
	VAESENC 80(SI), X9, X9
	VAESENC X8, X9, X9
	VAESENC X1, X9, X9
	VAESENC X9, X9, X9
	VPSHUFD $0x4e, X9, X8
	VPXOR   X8, X9, X8
	MOVQ    X8, 24(DI)

	VMOVDQU X0, X10
	VMOVDQU X0, X11
	VAESENC 96(SI), X10, X10
	VAESENC 104(SI), X11, X11
	VAESENC X10, X11, X11
	VAESENC X1, X11, X11
	VAESENC X11, X11, X11
	VPSHUFD $0x4e, X11, X10
	VPXOR   X10, X11, X10
	MOVQ    X10, 32(DI)

	VMOVDQU X0, X12
	VMOVDQU X0, X13
	VAESENC 120(SI), X12, X12
	VAESENC 128(SI), X13, X13
	VAESENC X12, X13, X13
	VAESENC X1, X13, X13
	VAESENC X13, X13, X13
	VPSHUFD $0x4e, X13, X12
	VPXOR   X12, X13, X12
	MOVQ    X12, 40(DI)

	VMOVDQU X0, X14
	VMOVDQU X0, X15
	VAESENC 144(SI), X14, X14
	VAESENC 152(SI), X15, X15
	VAESENC X14, X15, X15
	VAESENC X1, X15, X15
	VAESENC X15, X15, X15
	VPSHUFD $0x4e, X15, X14
	VPXOR   X14, X15, X14
	MOVQ    X14, 48(DI)

	ADDQ $168, SI
	ADDQ $56, DI
	JMP  loop

tail:
	ADDQ $7, CX
	JE   done


tailLoop:
	VMOVDQU X0, X2
	VMOVDQU X0, X3
	VAESENC (SI), X2, X2
	VAESENC 8(SI), X3, X3
	VAESENC X2, X3, X3
	VAESENC X1, X3, X3
	VAESENC X3, X3, X3
	VPSHUFD $0x4e, X3, X2
	VPXOR   X2, X3, X2
	MOVQ    X2, (DI)

	ADDQ $24, SI
	ADDQ $8, DI
	LOOP tailLoop

done:
	RET

// func AesInt192BatchGenKey(data *[3]uint64, hashes *[2]uint64, length int)
// Requires: AES
TEXT ·AesInt192BatchGenKey(SB), NOSPLIT, $0-24
	MOVQ data+0(FP), SI
	MOVQ hashes+8(FP), DI
	MOVQ length+16(FP), CX

	VMOVDQU aesIV<>+0(SB), X0
	VAESENC X0, X0, X1

loop:
	SUBQ $7, CX
	JL   tail

	VMOVDQU X0, X2
	VMOVDQU X0, X3
	VAESENC (SI), X2, X2
	VAESENC 8(SI), X3, X3
	VAESENC X3, X2, X2
	VAESENC X1, X2, X2
	VMOVDQU X2, (DI)

	VMOVDQU X0, X4
	VMOVDQU X0, X5
	VAESENC 24(SI), X4, X4
	VAESENC 32(SI), X5, X5
	VAESENC X5, X4, X4
	VAESENC X1, X4, X4
	VMOVDQU X4, 16(DI)

	VMOVDQU X0, X6
	VMOVDQU X0, X7
	VAESENC 48(SI), X6, X6
	VAESENC 56(SI), X7, X7
	VAESENC X7, X6, X6
	VAESENC X1, X6, X6
	VMOVDQU X6, 32(DI)

	VMOVDQU X0, X8
	VMOVDQU X0, X9
	VAESENC 72(SI), X8, X8
	VAESENC 80(SI), X9, X9
	VAESENC X9, X8, X8
	VAESENC X1, X8, X8
	VMOVDQU X8, 48(DI)

	VMOVDQU X0, X10
	VMOVDQU X0, X11
	VAESENC 96(SI), X10, X10
	VAESENC 104(SI), X11, X11
	VAESENC X11, X10, X10
	VAESENC X1, X10, X10
	VMOVDQU X10, 64(DI)

	VMOVDQU X0, X12
	VMOVDQU X0, X13
	VAESENC 120(SI), X12, X12
	VAESENC 128(SI), X13, X13
	VAESENC X13, X12, X12
	VAESENC X1, X12, X12
	VMOVDQU X12, 80(DI)

	VMOVDQU X0, X14
	VMOVDQU X0, X15
	VAESENC 144(SI), X14, X14
	VAESENC 152(SI), X15, X15
	VAESENC X15, X14, X14
	VAESENC X1, X14, X14
	VMOVDQU X14, 96(DI)

	ADDQ $168, SI
	ADDQ $112, DI
	JMP  loop

tail:
	ADDQ $7, CX
	JE   done


tailLoop:
	VMOVDQU X0, X2
	VMOVDQU X0, X3
	VAESENC (SI), X2, X2
	VAESENC 8(SI), X3, X3
	VAESENC X3, X2, X2
	VAESENC X1, X2, X2
	VMOVDQU X2, (DI)

	ADDQ $24, SI
	ADDQ $16, DI
	LOOP tailLoop

done:
	RET

// func AesInt256BatchHash(data *[4]uint64, hashes *uint64, length int)
// Requires: AES
TEXT ·AesInt256BatchHash(SB), NOSPLIT, $0-24
	MOVQ data+0(FP), SI
	MOVQ hashes+8(FP), DI
	MOVQ length+16(FP), CX

	VMOVDQU aesIV<>+0(SB), X0
	VAESENC X0, X0, X1

loop:
	SUBQ $7, CX
	JL   tail

	VMOVDQU X0, X2
	VMOVDQU X0, X3
	VAESENC (SI), X2, X2
	VAESENC 16(SI), X3, X3
	VAESENC X2, X3, X3
	VAESENC X1, X3, X3
	VAESENC X3, X3, X3
	VPSHUFD $0x4e, X3, X2
	VPXOR   X2, X3, X2
	MOVQ    X2, (DI)

	VMOVDQU X0, X4
	VMOVDQU X0, X5
	VAESENC 32(SI), X4, X4
	VAESENC 48(SI), X5, X5
	VAESENC X4, X5, X5
	VAESENC X1, X5, X5
	VAESENC X5, X5, X5
	VPSHUFD $0x4e, X5, X4
	VPXOR   X4, X5, X4
	MOVQ    X4, 8(DI)

	VMOVDQU X0, X6
	VMOVDQU X0, X7
	VAESENC 64(SI), X6, X6
	VAESENC 80(SI), X7, X7
	VAESENC X6, X7, X7
	VAESENC X1, X7, X7
	VAESENC X7, X7, X7
	VPSHUFD $0x4e, X7, X6
	VPXOR   X6, X7, X6
	MOVQ    X6, 16(DI)

	VMOVDQU X0, X8
	VMOVDQU X0, X9
	VAESENC 96(SI), X8, X8
	VAESENC 112(SI), X9, X9
	VAESENC X8, X9, X9
	VAESENC X1, X9, X9
	VAESENC X9, X9, X9
	VPSHUFD $0x4e, X9, X8
	VPXOR   X8, X9, X8
	MOVQ    X8, 24(DI)

	VMOVDQU X0, X10
	VMOVDQU X0, X11
	VAESENC 128(SI), X10, X10
	VAESENC 144(SI), X11, X11
	VAESENC X10, X11, X11
	VAESENC X1, X11, X11
	VAESENC X11, X11, X11
	VPSHUFD $0x4e, X11, X10
	VPXOR   X10, X11, X10
	MOVQ    X10, 32(DI)

	VMOVDQU X0, X12
	VMOVDQU X0, X13
	VAESENC 160(SI), X12, X12
	VAESENC 176(SI), X13, X13
	VAESENC X12, X13, X13
	VAESENC X1, X13, X13
	VAESENC X13, X13, X13
	VPSHUFD $0x4e, X13, X12
	VPXOR   X12, X13, X12
	MOVQ    X12, 40(DI)

	VMOVDQU X0, X14
	VMOVDQU X0, X15
	VAESENC 192(SI), X14, X14
	VAESENC 208(SI), X15, X15
	VAESENC X14, X15, X15
	VAESENC X1, X15, X15
	VAESENC X15, X15, X15
	VPSHUFD $0x4e, X15, X14
	VPXOR   X14, X15, X14
	MOVQ    X14, 48(DI)

	ADDQ $224, SI
	ADDQ $56, DI
	JMP  loop

tail:
	ADDQ $7, CX
	JE   done


tailLoop:
	VMOVDQU X0, X2
	VMOVDQU X0, X3
	VAESENC (SI), X2, X2
	VAESENC 16(SI), X3, X3
	VAESENC X2, X3, X3
	VAESENC X1, X3, X3
	VAESENC X3, X3, X3
	VPSHUFD $0x4e, X3, X2
	VPXOR   X2, X3, X2
	MOVQ    X2, (DI)

	ADDQ $32, SI
	ADDQ $8, DI
	LOOP tailLoop

done:
	RET

// func AesInt256BatchGenKey(data *[4]uint64, hashes *[2]uint64, length int)
// Requires: AES
TEXT ·AesInt256BatchGenKey(SB), NOSPLIT, $0-24
	MOVQ data+0(FP), SI
	MOVQ hashes+8(FP), DI
	MOVQ length+16(FP), CX

	VMOVDQU aesIV<>+0(SB), X0
	VAESENC X0, X0, X1

loop:
	SUBQ $7, CX
	JL   tail

	VMOVDQU X0, X2
	VMOVDQU X0, X3
	VAESENC (SI), X2, X2
	VAESENC 16(SI), X3, X3
	VAESENC X3, X2, X2
	VAESENC X1, X2, X2
	VMOVDQU X2, (DI)

	VMOVDQU X0, X4
	VMOVDQU X0, X5
	VAESENC 32(SI), X4, X4
	VAESENC 48(SI), X5, X5
	VAESENC X5, X4, X4
	VAESENC X1, X4, X4
	VMOVDQU X4, 16(DI)

	VMOVDQU X0, X6
	VMOVDQU X0, X7
	VAESENC 64(SI), X6, X6
	VAESENC 80(SI), X7, X7
	VAESENC X7, X6, X6
	VAESENC X1, X6, X6
	VMOVDQU X6, 32(DI)

	VMOVDQU X0, X8
	VMOVDQU X0, X9
	VAESENC 96(SI), X8, X8
	VAESENC 112(SI), X9, X9
	VAESENC X9, X8, X8
	VAESENC X1, X8, X8
	VMOVDQU X8, 48(DI)

	VMOVDQU X0, X10
	VMOVDQU X0, X11
	VAESENC 128(SI), X10, X10
	VAESENC 144(SI), X11, X11
	VAESENC X11, X10, X10
	VAESENC X1, X10, X10
	VMOVDQU X10, 64(DI)

	VMOVDQU X0, X12
	VMOVDQU X0, X13
	VAESENC 160(SI), X12, X12
	VAESENC 176(SI), X13, X13
	VAESENC X13, X12, X12
	VAESENC X1, X12, X12
	VMOVDQU X12, 80(DI)

	VMOVDQU X0, X14
	VMOVDQU X0, X15
	VAESENC 192(SI), X14, X14
	VAESENC 208(SI), X15, X15
	VAESENC X15, X14, X14
	VAESENC X1, X14, X14
	VMOVDQU X14, 96(DI)

	ADDQ $224, SI
	ADDQ $112, DI
	JMP  loop

tail:
	ADDQ $7, CX
	JE   done


tailLoop:
	VMOVDQU X0, X2
	VMOVDQU X0, X3
	VAESENC (SI), X2, X2
	VAESENC 16(SI), X3, X3
	VAESENC X3, X2, X2
	VAESENC X1, X2, X2
	VMOVDQU X2, (DI)

	ADDQ $32, SI
	ADDQ $16, DI
	LOOP tailLoop

done:
	RET

// func AesInt320BatchHash(data *[5]uint64, hashes *uint64, length int)
// Requires: AES
TEXT ·AesInt320BatchHash(SB), NOSPLIT, $0-24
	MOVQ data+0(FP), SI
	MOVQ hashes+8(FP), DI
	MOVQ length+16(FP), CX

	VMOVDQU aesIV<>+0(SB), X0

loop:
	SUBQ $5, CX
	JL   tail

	VMOVDQU X0, X1
	VMOVDQU X0, X2
	VMOVDQU X0, X3
	VAESENC (SI), X1, X1
	VAESENC 16(SI), X2, X2
	VAESENC 24(SI), X3, X3
	VAESENC X0, X1, X1
	VAESENC X2, X3, X3
	VAESENC X3, X1, X1
	VAESENC X1, X1, X1
	VAESENC X1, X1, X1
	VAESENC X1, X1, X1
	VPSHUFD $0x4e, X1, X2
	VPXOR   X1, X2, X1
	MOVQ    X1, (DI)

	VMOVDQU X0, X4
	VMOVDQU X0, X5
	VMOVDQU X0, X6
	VAESENC 40(SI), X4, X4
	VAESENC 56(SI), X5, X5
	VAESENC 64(SI), X6, X6
	VAESENC X0, X4, X4
	VAESENC X5, X6, X6
	VAESENC X6, X4, X4
	VAESENC X4, X4, X4
	VAESENC X4, X4, X4
	VAESENC X4, X4, X4
	VPSHUFD $0x4e, X4, X5
	VPXOR   X4, X5, X4
	MOVQ    X4, 8(DI)

	VMOVDQU X0, X7
	VMOVDQU X0, X8
	VMOVDQU X0, X9
	VAESENC 80(SI), X7, X7
	VAESENC 96(SI), X8, X8
	VAESENC 104(SI), X9, X9
	VAESENC X0, X7, X7
	VAESENC X8, X9, X9
	VAESENC X9, X7, X7
	VAESENC X7, X7, X7
	VAESENC X7, X7, X7
	VAESENC X7, X7, X7
	VPSHUFD $0x4e, X7, X8
	VPXOR   X7, X8, X7
	MOVQ    X7, 16(DI)

	VMOVDQU X0, X10
	VMOVDQU X0, X11
	VMOVDQU X0, X12
	VAESENC 120(SI), X10, X10
	VAESENC 136(SI), X11, X11
	VAESENC 144(SI), X12, X12
	VAESENC X0, X10, X10
	VAESENC X11, X12, X12
	VAESENC X12, X10, X10
	VAESENC X10, X10, X10
	VAESENC X10, X10, X10
	VAESENC X10, X10, X10
	VPSHUFD $0x4e, X10, X11
	VPXOR   X10, X11, X10
	MOVQ    X10, 24(DI)

	VMOVDQU X0, X13
	VMOVDQU X0, X14
	VMOVDQU X0, X15
	VAESENC 160(SI), X13, X13
	VAESENC 176(SI), X14, X14
	VAESENC 184(SI), X15, X15
	VAESENC X0, X13, X13
	VAESENC X14, X15, X15
	VAESENC X15, X13, X13
	VAESENC X13, X13, X13
	VAESENC X13, X13, X13
	VAESENC X13, X13, X13
	VPSHUFD $0x4e, X13, X14
	VPXOR   X13, X14, X13
	MOVQ    X13, 32(DI)

	ADDQ $200, SI
	ADDQ $40, DI
	JMP  loop

tail:
	ADDQ $5, CX
	JE   done


tailLoop:
	VMOVDQU X0, X1
	VMOVDQU X0, X2
	VMOVDQU X0, X3
	VAESENC (SI), X1, X1
	VAESENC 16(SI), X2, X2
	VAESENC 24(SI), X3, X3
	VAESENC X0, X1, X1
	VAESENC X2, X3, X3
	VAESENC X3, X1, X1
	VAESENC X1, X1, X1
	VAESENC X1, X1, X1
	VAESENC X1, X1, X1
	VPSHUFD $0x4e, X1, X2
	VPXOR   X1, X2, X1
	MOVQ    X1, (DI)

	ADDQ $40, SI
	ADDQ $8, DI
	LOOP tailLoop

done:
	RET

// func AesInt320BatchGenKey(data *[5]uint64, hashes *[2]uint64, length int)
// Requires: AES
TEXT ·AesInt320BatchGenKey(SB), NOSPLIT, $0-24
	MOVQ data+0(FP), SI
	MOVQ hashes+8(FP), DI
	MOVQ length+16(FP), CX

	VMOVDQU aesIV<>+0(SB), X0

loop:
	SUBQ $5, CX
	JL   tail

	VMOVDQU X0, X1
	VMOVDQU X0, X2
	VMOVDQU X0, X3
	VAESENC (SI), X1, X1
	VAESENC 16(SI), X2, X2
	VAESENC 24(SI), X3, X3
	VAESENC X2, X1, X1
	VAESENC X0, X3, X3
	VAESENC X3, X1, X1
	VMOVDQU X1, (DI)

	VMOVDQU X0, X4
	VMOVDQU X0, X5
	VMOVDQU X0, X6
	VAESENC 40(SI), X4, X4
	VAESENC 56(SI), X5, X5
	VAESENC 64(SI), X6, X6
	VAESENC X5, X4, X4
	VAESENC X0, X6, X6
	VAESENC X6, X4, X4
	VMOVDQU X4, 16(DI)

	VMOVDQU X0, X7
	VMOVDQU X0, X8
	VMOVDQU X0, X9
	VAESENC 80(SI), X7, X7
	VAESENC 96(SI), X8, X8
	VAESENC 104(SI), X9, X9
	VAESENC X8, X7, X7
	VAESENC X0, X9, X9
	VAESENC X9, X7, X7
	VMOVDQU X7, 32(DI)

	VMOVDQU X0, X10
	VMOVDQU X0, X11
	VMOVDQU X0, X12
	VAESENC 120(SI), X10, X10
	VAESENC 136(SI), X11, X11
	VAESENC 144(SI), X12, X12
	VAESENC X11, X10, X10
	VAESENC X0, X12, X12
	VAESENC X12, X10, X10
	VMOVDQU X10, 48(DI)

	VMOVDQU X0, X13
	VMOVDQU X0, X14
	VMOVDQU X0, X15
	VAESENC 160(SI), X13, X13
	VAESENC 176(SI), X14, X14
	VAESENC 184(SI), X15, X15
	VAESENC X14, X13, X13
	VAESENC X0, X15, X15
	VAESENC X15, X13, X13
	VMOVDQU X13, 64(DI)

	ADDQ $200, SI
	ADDQ $80, DI
	JMP  loop

tail:
	ADDQ $5, CX
	JE   done


tailLoop:
	VMOVDQU X0, X1
	VMOVDQU X0, X2
	VMOVDQU X0, X3
	VAESENC (SI), X1, X1
	VAESENC 16(SI), X2, X2
	VAESENC 24(SI), X3, X3
	VAESENC X2, X1, X1
	VAESENC X0, X3, X3
	VAESENC X3, X1, X1
	VMOVDQU X1, (DI)

	ADDQ $40, SI
	ADDQ $16, DI
	LOOP tailLoop

done:
	RET
