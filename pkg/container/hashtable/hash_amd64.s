// Copyright 2021 Matrix Origin
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#include "textflag.h"

// func Crc32Int64BatchHash(data *uint64, hashes *uint64, length int)
// Requires: SSE4.2
TEXT ·Crc32Int64BatchHash(SB), NOSPLIT, $0-24
	MOVQ data+0(FP), SI
	MOVQ hashes+8(FP), DI
	MOVQ length+16(FP), CX

loop:
	SUBQ $8, CX
	JL   tail

	MOVQ $-1, R8
	MOVQ $-1, R9
	MOVQ $-1, R10
	MOVQ $-1, R11
	MOVQ $-1, R12
	MOVQ $-1, R13
	MOVQ $-1, R14
	MOVQ $-1, R15

	CRC32Q 0x00(SI), R8
	CRC32Q 0x08(SI), R9
	CRC32Q 0x10(SI), R10
	CRC32Q 0x18(SI), R11
	CRC32Q 0x20(SI), R12
	CRC32Q 0x28(SI), R13
	CRC32Q 0x30(SI), R14
	CRC32Q 0x38(SI), R15

	MOVQ R8, 0x00(DI)
	MOVQ R9, 0x08(DI)
	MOVQ R10, 0x10(DI)
	MOVQ R11, 0x18(DI)
	MOVQ R12, 0x20(DI)
	MOVQ R13, 0x28(DI)
	MOVQ R14, 0x30(DI)
	MOVQ R15, 0x38(DI)

	ADDQ $0x40, SI
	ADDQ $0x40, DI
	JMP  loop

tail:
	ADDQ $8, CX
	JE   done

tailLoop:
	MOVQ   $-1, R8
	CRC32Q (SI), R8
	MOVQ   R8, (DI)

	ADDQ $0x08, SI
	ADDQ $0x08, DI
	LOOP tailLoop

done:
	RET

// func Crc32Int64CellBatchHash(data *uint64, hashes *uint64, length int)
// Requires: SSE4.2
TEXT ·Crc32Int64CellBatchHash(SB), NOSPLIT, $0-24
	MOVQ data+0(FP), SI
	MOVQ hashes+8(FP), DI
	MOVQ length+16(FP), CX

loop:
	SUBQ $8, CX
	JL   tail

	MOVQ $-1, R8
	MOVQ $-1, R9
	MOVQ $-1, R10
	MOVQ $-1, R11
	MOVQ $-1, R12
	MOVQ $-1, R13
	MOVQ $-1, R14
	MOVQ $-1, R15

	CRC32Q 0x00(SI), R8
	CRC32Q 0x10(SI), R9
	CRC32Q 0x20(SI), R10
	CRC32Q 0x30(SI), R11
	CRC32Q 0x40(SI), R12
	CRC32Q 0x50(SI), R13
	CRC32Q 0x60(SI), R14
	CRC32Q 0x70(SI), R15

	MOVQ R8, 0x00(DI)
	MOVQ R9, 0x08(DI)
	MOVQ R10, 0x10(DI)
	MOVQ R11, 0x18(DI)
	MOVQ R12, 0x20(DI)
	MOVQ R13, 0x28(DI)
	MOVQ R14, 0x30(DI)
	MOVQ R15, 0x38(DI)

	ADDQ $0x80, SI
	ADDQ $0x40, DI
	JMP  loop

tail:
	ADDQ $8, CX
	JE   done

tailLoop:
	MOVQ   $-1, R8
	CRC32Q (SI), R8
	MOVQ   R8, (DI)

	ADDQ $0x10, SI
	ADDQ $0x08, DI
	LOOP tailLoop

done:
	RET

////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////

DATA Pi<>+0x00(SB)/8, $0x3243f6a8885a308d
DATA Pi<>+0x08(SB)/8, $0x313198a2e0370734
DATA Pi<>+0x10(SB)/8, $0x4a4093822299f31d
DATA Pi<>+0x18(SB)/8, $0x0082efa98ec4e6c8
DATA Pi<>+0x20(SB)/8, $0x9452821e638d0137
DATA Pi<>+0x28(SB)/8, $0x7be5466cf34e90c6
DATA Pi<>+0x30(SB)/8, $0xcc0ac29b7c97c50d
DATA Pi<>+0x38(SB)/8, $0xd3f84d5b5b547091
DATA Pi<>+0x40(SB)/8, $0x79216d5d98979fb1
DATA Pi<>+0x48(SB)/8, $0xbd1310ba698dfb5a
DATA Pi<>+0x50(SB)/8, $0xc2ffd72dbd01adfb
DATA Pi<>+0x58(SB)/8, $0x7b8e1afed6a267e9
DATA Pi<>+0x60(SB)/8, $0x6ba7c9045f12c7f9
DATA Pi<>+0x68(SB)/8, $0x924a19947b3916cf
DATA Pi<>+0x70(SB)/8, $0x70801f2e2858efc1
DATA Pi<>+0x78(SB)/8, $0x6636920d871574e6
GLOBL Pi<>(SB), (NOPTR+RODATA), $0x80

DATA CryptedPi<>+0x00(SB)/8, $0x822233b93c11087c
DATA CryptedPi<>+0x08(SB)/8, $0xd2b32f4adde873da
DATA CryptedPi<>+0x10(SB)/8, $0xae9c2fc7dd17bcdb
DATA CryptedPi<>+0x18(SB)/8, $0x859110441a1569fc
DATA CryptedPi<>+0x20(SB)/8, $0x47087d794fffb5c9
DATA CryptedPi<>+0x28(SB)/8, $0xb7b6c8f565414445
DATA CryptedPi<>+0x30(SB)/8, $0xfd260edabb308f8d
DATA CryptedPi<>+0x38(SB)/8, $0x3ddefc67bc565a13
DATA CryptedPi<>+0x40(SB)/8, $0xe4c1d50223544f10
DATA CryptedPi<>+0x48(SB)/8, $0xaf40e05725c3192b
DATA CryptedPi<>+0x50(SB)/8, $0x281d8ab9a16382e9
DATA CryptedPi<>+0x58(SB)/8, $0xddc10c903b63a6cf
DATA CryptedPi<>+0x60(SB)/8, $0x852d3ad603e8df72
DATA CryptedPi<>+0x68(SB)/8, $0xa6642b57d1011deb
DATA CryptedPi<>+0x70(SB)/8, $0x5063d25a1cb7b6b9
DATA CryptedPi<>+0x78(SB)/8, $0xb2623e6241e8e46e
GLOBL CryptedPi<>(SB), (NOPTR+RODATA), $0x80

// func AesBytesBatchGenHashStates(data *[]byte, states *[3]uint64, length int)
// Requires: AES
TEXT ·AesBytesBatchGenHashStates(SB), NOSPLIT, $0-24
	MOVQ data+0(FP), SI
	MOVQ states+8(FP), DI
	MOVQ length+16(FP), CX

	VMOVDQU CryptedPi<>+0x00(SB), X0
	VMOVDQU CryptedPi<>+0x10(SB), X1
	VMOVDQU CryptedPi<>+0x20(SB), X2
	VMOVDQU CryptedPi<>+0x30(SB), X3

loop:
	MOVQ (SI), AX
	MOVQ 8(SI), DX
	MOVQ DX, BX

	ADDQ AX, DX
	SUBQ $0x40, DX

	VMOVDQU X0, X4
	VMOVDQU X1, X5
	VMOVDQU X2, X6
	VMOVDQU X3, X7

innerLoop:
	CMPQ AX, DX
	JGE  tail

	VAESENC 0x00(AX), X4, X4
	VAESENC 0x10(AX), X5, X5
	VAESENC 0x20(AX), X6, X6
	VAESENC 0x30(AX), X7, X7

	ADDQ $0x40, AX
	JMP  innerLoop

tail:
	ADDQ $0x30, DX
	CMPQ AX, DX
	JGE  done

	VAESENC (AX), X4, X4

	ADDQ $0x10, AX
	CMPQ AX, DX
	JGE  done

	VAESENC (AX), X5, X5

	ADDQ $0x10, AX
	CMPQ AX, DX
	JGE  done

	VAESENC (AX), X6, X6

done:
	VAESENC (DX), X7, X7

	VAESENC X6, X5, X9
	VAESENC X7, X4, X8
	VAESENC X9, X8, X8

	VAESENC X5, X4, X4
	VAESENC X6, X7, X7
	VAESENC X7, X4, X4

	VAESENC X5, X4, X4
	VAESENC X6, X4, X4

	VPSHUFD $0x4e, X4, X5
	VPXOR   X4, X5, X4
	MOVQ    X4, R8
	XORQ    BX, R8
	MOVQ    R8, (DI)

	VMOVDQU X8, 8(DI)

	ADDQ $24, SI
	ADDQ $24, DI
	DECQ CX
	JNZ  loop

	RET

// func AesInt192BatchGenHashStates(data *[3]uint64, states *[3]uint64, length int)
// Requires: AES
TEXT ·AesInt192BatchGenHashStates(SB), NOSPLIT, $0-24
	MOVQ data+0(FP), SI
	MOVQ states+8(FP), DI
	MOVQ length+16(FP), CX

	VMOVDQU CryptedPi<>+0x20(SB), X0
	VMOVDQU CryptedPi<>+0x30(SB), X1
	VMOVDQU CryptedPi<>+0x40(SB), X2
	VMOVDQU CryptedPi<>+0x50(SB), X3
	VAESENC X2, X3, X4
	VAESENC X3, X2, X2
	VMOVDQU X4, X3

loop:
	SUBQ $6, CX
	JL   tail

	VAESENC 0x00(SI), X0, X4
	VAESENC 0x08(SI), X1, X5
	VAESENC X4, X5, X10
	VAESENC X2, X10, X10
	VAESENC X0, X10, X10
	VPSHUFD $0x4e, X10, X11
	VPXOR   X10, X11, X10
	MOVQ    X10, 0x00(DI)
	VAESENC X5, X4, X4
	VAESENC X3, X4, X4
	VMOVDQU X4, 0x08(DI)

	VAESENC 0x18(SI), X0, X6
	VAESENC 0x20(SI), X1, X7
	VAESENC X6, X7, X12
	VAESENC X2, X12, X12
	VAESENC X0, X12, X12
	VPSHUFD $0x4e, X12, X13
	VPXOR   X12, X13, X12
	MOVQ    X12, 0x18(DI)
	VAESENC X7, X6, X6
	VAESENC X3, X6, X6
	VMOVDQU X6, 0x20(DI)

	VAESENC 0x30(SI), X0, X8
	VAESENC 0x38(SI), X1, X9
	VAESENC X8, X9, X14
	VAESENC X2, X14, X14
	VAESENC X0, X14, X14
	VPSHUFD $0x4e, X14, X15
	VPXOR   X14, X15, X14
	MOVQ    X14, 0x30(DI)
	VAESENC X9, X8, X8
	VAESENC X3, X8, X8
	VMOVDQU X8, 0x38(DI)

	VAESENC 0x48(SI), X0, X10
	VAESENC 0x50(SI), X1, X11
	VAESENC X10, X11, X4
	VAESENC X2, X4, X4
	VAESENC X0, X4, X4
	VPSHUFD $0x4e, X4, X5
	VPXOR   X4, X5, X4
	MOVQ    X4, 0x48(DI)
	VAESENC X11, X10, X10
	VAESENC X3, X10, X10
	VMOVDQU X10, 0x50(DI)

	VAESENC 0x60(SI), X0, X12
	VAESENC 0x68(SI), X1, X13
	VAESENC X12, X13, X6
	VAESENC X2, X6, X6
	VAESENC X0, X6, X6
	VPSHUFD $0x4e, X6, X7
	VPXOR   X6, X7, X6
	MOVQ    X6, 0x60(DI)
	VAESENC X13, X12, X12
	VAESENC X3, X12, X12
	VMOVDQU X12, 0x68(DI)

	VAESENC 0x78(SI), X0, X14
	VAESENC 0x80(SI), X1, X15
	VAESENC X14, X15, X8
	VAESENC X2, X8, X8
	VAESENC X0, X8, X8
	VPSHUFD $0x4e, X8, X9
	VPXOR   X8, X9, X8
	MOVQ    X8, 0x78(DI)
	VAESENC X15, X14, X14
	VAESENC X3, X14, X14
	VMOVDQU X14, 0x80(DI)

	ADDQ $0x90, SI
	ADDQ $0x90, DI
	JMP  loop

tail:
	ADDQ $6, CX
	JE   done

tailLoop:
	VAESENC 0x00(SI), X0, X4
	VAESENC 0x08(SI), X1, X5
	VAESENC X4, X5, X10
	VAESENC X2, X10, X10
	VAESENC X0, X10, X10
	VPSHUFD $0x4e, X10, X11
	VPXOR   X10, X11, X10
	MOVQ    X10, 0x00(DI)
	VAESENC X5, X4, X4
	VAESENC X3, X4, X4
	VMOVDQU X4, 0x08(DI)

	ADDQ $0x18, SI
	ADDQ $0x18, DI
	LOOP tailLoop

done:
	RET

// func AesInt256BatchGenHashStates(data *[4]uint64, states *[3]uint64, length int)
// Requires: AES
TEXT ·AesInt256BatchGenHashStates(SB), NOSPLIT, $0-24
	MOVQ data+0(FP), SI
	MOVQ states+8(FP), DI
	MOVQ length+16(FP), CX

	VMOVDQU CryptedPi<>+0x20(SB), X0
	VMOVDQU CryptedPi<>+0x30(SB), X1
	VMOVDQU CryptedPi<>+0x40(SB), X2
	VMOVDQU CryptedPi<>+0x50(SB), X3
	VAESENC X2, X3, X4
	VAESENC X3, X2, X2
	VMOVDQU X4, X3

loop:
	SUBQ $6, CX
	JL   tail

	VAESENC 0x00(SI), X0, X4
	VAESENC 0x10(SI), X1, X5
	VAESENC X4, X5, X10
	VAESENC X2, X10, X10
	VAESENC X0, X10, X10
	VPSHUFD $0x4e, X10, X11
	VPXOR   X10, X11, X10
	MOVQ    X10, 0x00(DI)
	VAESENC X5, X4, X4
	VAESENC X3, X4, X4
	VMOVDQU X4, 0x08(DI)

	VAESENC 0x20(SI), X0, X6
	VAESENC 0x30(SI), X1, X7
	VAESENC X6, X7, X12
	VAESENC X2, X12, X12
	VAESENC X0, X12, X12
	VPSHUFD $0x4e, X12, X13
	VPXOR   X12, X13, X12
	MOVQ    X12, 0x18(DI)
	VAESENC X7, X6, X6
	VAESENC X3, X6, X6
	VMOVDQU X6, 0x20(DI)

	VAESENC 0x40(SI), X0, X8
	VAESENC 0x50(SI), X1, X9
	VAESENC X8, X9, X14
	VAESENC X2, X14, X14
	VAESENC X0, X14, X14
	VPSHUFD $0x4e, X14, X15
	VPXOR   X14, X15, X14
	MOVQ    X14, 0x30(DI)
	VAESENC X9, X8, X8
	VAESENC X3, X8, X8
	VMOVDQU X8, 0x38(DI)

	VAESENC 0x60(SI), X0, X10
	VAESENC 0x70(SI), X1, X11
	VAESENC X10, X11, X4
	VAESENC X2, X4, X4
	VAESENC X0, X4, X4
	VPSHUFD $0x4e, X4, X5
	VPXOR   X4, X5, X4
	MOVQ    X4, 0x48(DI)
	VAESENC X11, X10, X10
	VAESENC X3, X10, X10
	VMOVDQU X10, 0x50(DI)

	VAESENC 0x80(SI), X0, X12
	VAESENC 0x90(SI), X1, X13
	VAESENC X12, X13, X6
	VAESENC X2, X6, X6
	VAESENC X0, X6, X6
	VPSHUFD $0x4e, X6, X7
	VPXOR   X6, X7, X6
	MOVQ    X6, 0x60(DI)
	VAESENC X13, X12, X12
	VAESENC X3, X12, X12
	VMOVDQU X12, 0x68(DI)

	VAESENC 0xa0(SI), X0, X14
	VAESENC 0xb0(SI), X1, X15
	VAESENC X14, X15, X8
	VAESENC X2, X8, X8
	VAESENC X0, X8, X8
	VPSHUFD $0x4e, X8, X9
	VPXOR   X8, X9, X8
	MOVQ    X8, 0x78(DI)
	VAESENC X15, X14, X14
	VAESENC X3, X14, X14
	VMOVDQU X14, 0x80(DI)

	ADDQ $0xc0, SI
	ADDQ $0x90, DI
	JMP  loop

tail:
	ADDQ $6, CX
	JE   done

tailLoop:
	VAESENC 0x00(SI), X0, X4
	VAESENC 0x10(SI), X1, X5
	VAESENC X4, X5, X10
	VAESENC X2, X10, X10
	VAESENC X0, X10, X10
	VPSHUFD $0x4e, X10, X11
	VPXOR   X10, X11, X10
	MOVQ    X10, 0x00(DI)
	VAESENC X5, X4, X4
	VAESENC X3, X4, X4
	VMOVDQU X4, 0x08(DI)

	ADDQ $0x20, SI
	ADDQ $0x18, DI
	LOOP tailLoop

done:
	RET

// func AesInt320BatchGenHashStates(data *[5]uint64, states *[3]uint64, length int)
// Requires: AES
TEXT ·AesInt320BatchGenHashStates(SB), NOSPLIT, $0-24
	MOVQ data+0(FP), SI
	MOVQ states+8(FP), DI
	MOVQ length+16(FP), CX

	VMOVDQU CryptedPi<>+0x00(SB), X0
	VMOVDQU CryptedPi<>+0x10(SB), X1
	VMOVDQU CryptedPi<>+0x20(SB), X2
	VMOVDQU CryptedPi<>+0x30(SB), X3

loop:
	SUBQ $4, CX
	JL   tail

	VAESENC 0x00(SI), X0, X4
	VAESENC 0x10(SI), X1, X5
	VAESENC 0x18(SI), X3, X6
	VAESENC X2, X4, X11
	VAESENC X5, X6, X12
	VAESENC X12, X11, X11
	VAESENC X0, X11, X11
	VAESENC X0, X11, X11
	VPSHUFD $0x4e, X11, X12
	VPXOR   X11, X12, X11
	MOVQ    X11, 0x00(DI)
	VAESENC X5, X4, X4
	VAESENC X2, X6, X6
	VAESENC X6, X4, X4
	VMOVDQU X4, 0x08(DI)

	VAESENC 0x28(SI), X0, X7
	VAESENC 0x38(SI), X1, X8
	VAESENC 0x40(SI), X3, X9
	VAESENC X2, X7, X14
	VAESENC X8, X9, X15
	VAESENC X15, X14, X14
	VAESENC X0, X14, X14
	VAESENC X0, X14, X14
	VPSHUFD $0x4e, X14, X15
	VPXOR   X14, X15, X14
	MOVQ    X14, 0x18(DI)
	VAESENC X8, X7, X7
	VAESENC X2, X9, X9
	VAESENC X9, X7, X7
	VMOVDQU X7, 0x20(DI)

	VAESENC 0x50(SI), X0, X10
	VAESENC 0x60(SI), X1, X11
	VAESENC 0x68(SI), X3, X12
	VAESENC X2, X10, X5
	VAESENC X11, X12, X6
	VAESENC X6, X5, X5
	VAESENC X0, X5, X5
	VAESENC X0, X5, X5
	VPSHUFD $0x4e, X5, X6
	VPXOR   X5, X6, X5
	MOVQ    X5, 0x30(DI)
	VAESENC X11, X10, X10
	VAESENC X2, X12, X12
	VAESENC X12, X10, X10
	VMOVDQU X10, 0x38(DI)

	VAESENC 0x78(SI), X0, X13
	VAESENC 0x88(SI), X1, X14
	VAESENC 0x90(SI), X3, X15
	VAESENC X2, X13, X8
	VAESENC X14, X15, X9
	VAESENC X9, X8, X8
	VAESENC X0, X8, X8
	VAESENC X0, X8, X8
	VPSHUFD $0x4e, X8, X9
	VPXOR   X8, X9, X8
	MOVQ    X8, 0x48(DI)
	VAESENC X14, X13, X13
	VAESENC X2, X15, X15
	VAESENC X15, X13, X13
	VMOVDQU X13, 0x50(DI)

	ADDQ $0xa0, SI
	ADDQ $0x60, DI
	JMP  loop

tail:
	ADDQ $4, CX
	JE   done

tailLoop:
	VAESENC 0x00(SI), X0, X4
	VAESENC 0x10(SI), X1, X5
	VAESENC 0x18(SI), X3, X6
	VAESENC X2, X4, X11
	VAESENC X5, X6, X12
	VAESENC X12, X11, X11
	VAESENC X0, X11, X11
	VAESENC X0, X11, X11
	VPSHUFD $0x4e, X11, X12
	VPXOR   X11, X12, X11
	MOVQ    X11, 0x00(DI)
	VAESENC X5, X4, X4
	VAESENC X2, X6, X6
	VAESENC X6, X4, X4
	VMOVDQU X4, 0x08(DI)

	ADDQ $0x28, SI
	ADDQ $0x18, DI
	LOOP tailLoop

done:
	RET

TEXT genCryptedPi(SB), NOSPLIT, $0-8
	MOVQ dst+0(FP), DI

	VMOVDQU Pi<>+0x00(SB), X0
	VMOVDQU Pi<>+0x10(SB), X1
	VMOVDQU Pi<>+0x20(SB), X2
	VMOVDQU Pi<>+0x30(SB), X3
	VMOVDQU Pi<>+0x40(SB), X4
	VMOVDQU Pi<>+0x50(SB), X5
	VMOVDQU Pi<>+0x60(SB), X6
	VMOVDQU Pi<>+0x70(SB), X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENCLAST X0, X0
	AESENCLAST X1, X1
	AESENCLAST X2, X2
	AESENCLAST X3, X3
	AESENCLAST X4, X4
	AESENCLAST X5, X5
	AESENCLAST X6, X6
	AESENCLAST X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENC X0, X0
	AESENC X1, X1
	AESENC X2, X2
	AESENC X3, X3
	AESENC X4, X4
	AESENC X5, X5
	AESENC X6, X6
	AESENC X7, X7

	AESENCLAST X0, X0
	AESENCLAST X1, X1
	AESENCLAST X2, X2
	AESENCLAST X3, X3
	AESENCLAST X4, X4
	AESENCLAST X5, X5
	AESENCLAST X6, X6
	AESENCLAST X7, X7

	VMOVDQU X0, 0x00(DI)
	VMOVDQU X1, 0x10(DI)
	VMOVDQU X2, 0x20(DI)
	VMOVDQU X3, 0x30(DI)
	VMOVDQU X4, 0x40(DI)
	VMOVDQU X5, 0x50(DI)
	VMOVDQU X6, 0x60(DI)
	VMOVDQU X7, 0x70(DI)

	RET
