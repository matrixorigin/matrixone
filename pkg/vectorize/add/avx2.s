// Code generated by command: go run avx2.go -out add/avx2.s -stubs add/avx2_stubs.go. DO NOT EDIT.

#include "textflag.h"

// func int8AddAvx2Asm(x []int8, y []int8, r []int8)
// Requires: AVX, AVX2
TEXT ·int8AddAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int8AddBlockLoop:
	CMPQ    BX, $0x000000c0
	JL      int8AddTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VPADDB  (CX), Y0, Y0
	VPADDB  32(CX), Y1, Y1
	VPADDB  64(CX), Y2, Y2
	VPADDB  96(CX), Y3, Y3
	VPADDB  128(CX), Y4, Y4
	VPADDB  160(CX), Y5, Y5
	VMOVDQU Y0, (DX)
	VMOVDQU Y1, 32(DX)
	VMOVDQU Y2, 64(DX)
	VMOVDQU Y3, 96(DX)
	VMOVDQU Y4, 128(DX)
	VMOVDQU Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x000000c0, BX
	JMP     int8AddBlockLoop

int8AddTailLoop:
	CMPQ    BX, $0x00000020
	JL      int8AddDone
	VMOVDQU (AX), Y0
	VPADDB  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000020, BX
	JMP     int8AddTailLoop

int8AddDone:
	RET

// func int8AddScalarAvx2Asm(x int8, y []int8, r []int8)
// Requires: AVX, AVX2, SSE2
TEXT ·int8AddScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVBLSX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTB X0, Y0

int8AddScalarBlockLoop:
	CMPQ    BX, $0x000000c0
	JL      int8AddScalarTailLoop
	VPADDB  (CX), Y0, Y1
	VPADDB  32(CX), Y0, Y2
	VPADDB  64(CX), Y0, Y3
	VPADDB  96(CX), Y0, Y4
	VPADDB  128(CX), Y0, Y5
	VPADDB  160(CX), Y0, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x000000c0, BX
	JMP     int8AddScalarBlockLoop

int8AddScalarTailLoop:
	CMPQ    BX, $0x00000020
	JL      int8AddScalarDone
	VPADDB  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000020, BX
	JMP     int8AddScalarTailLoop

int8AddScalarDone:
	RET

// func int16AddAvx2Asm(x []int16, y []int16, r []int16)
// Requires: AVX, AVX2
TEXT ·int16AddAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int16AddBlockLoop:
	CMPQ    BX, $0x00000060
	JL      int16AddTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VPADDW  (CX), Y0, Y0
	VPADDW  32(CX), Y1, Y1
	VPADDW  64(CX), Y2, Y2
	VPADDW  96(CX), Y3, Y3
	VPADDW  128(CX), Y4, Y4
	VPADDW  160(CX), Y5, Y5
	VMOVDQU Y0, (DX)
	VMOVDQU Y1, 32(DX)
	VMOVDQU Y2, 64(DX)
	VMOVDQU Y3, 96(DX)
	VMOVDQU Y4, 128(DX)
	VMOVDQU Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000060, BX
	JMP     int16AddBlockLoop

int16AddTailLoop:
	CMPQ    BX, $0x00000010
	JL      int16AddDone
	VMOVDQU (AX), Y0
	VPADDW  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000010, BX
	JMP     int16AddTailLoop

int16AddDone:
	RET

// func int16AddScalarAvx2Asm(x int16, y []int16, r []int16)
// Requires: AVX, AVX2, SSE2
TEXT ·int16AddScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVWLSX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTW X0, Y0

int16AddScalarBlockLoop:
	CMPQ    BX, $0x00000060
	JL      int16AddScalarTailLoop
	VPADDW  (CX), Y0, Y1
	VPADDW  32(CX), Y0, Y2
	VPADDW  64(CX), Y0, Y3
	VPADDW  96(CX), Y0, Y4
	VPADDW  128(CX), Y0, Y5
	VPADDW  160(CX), Y0, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000060, BX
	JMP     int16AddScalarBlockLoop

int16AddScalarTailLoop:
	CMPQ    BX, $0x00000010
	JL      int16AddScalarDone
	VPADDW  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000010, BX
	JMP     int16AddScalarTailLoop

int16AddScalarDone:
	RET

// func int32AddAvx2Asm(x []int32, y []int32, r []int32)
// Requires: AVX, AVX2
TEXT ·int32AddAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int32AddBlockLoop:
	CMPQ    BX, $0x00000030
	JL      int32AddTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VPADDD  (CX), Y0, Y0
	VPADDD  32(CX), Y1, Y1
	VPADDD  64(CX), Y2, Y2
	VPADDD  96(CX), Y3, Y3
	VPADDD  128(CX), Y4, Y4
	VPADDD  160(CX), Y5, Y5
	VMOVDQU Y0, (DX)
	VMOVDQU Y1, 32(DX)
	VMOVDQU Y2, 64(DX)
	VMOVDQU Y3, 96(DX)
	VMOVDQU Y4, 128(DX)
	VMOVDQU Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000030, BX
	JMP     int32AddBlockLoop

int32AddTailLoop:
	CMPQ    BX, $0x00000008
	JL      int32AddDone
	VMOVDQU (AX), Y0
	VPADDD  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     int32AddTailLoop

int32AddDone:
	RET

// func int32AddScalarAvx2Asm(x int32, y []int32, r []int32)
// Requires: AVX, AVX2, SSE2
TEXT ·int32AddScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVL         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTD X0, Y0

int32AddScalarBlockLoop:
	CMPQ    BX, $0x00000030
	JL      int32AddScalarTailLoop
	VPADDD  (CX), Y0, Y1
	VPADDD  32(CX), Y0, Y2
	VPADDD  64(CX), Y0, Y3
	VPADDD  96(CX), Y0, Y4
	VPADDD  128(CX), Y0, Y5
	VPADDD  160(CX), Y0, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000030, BX
	JMP     int32AddScalarBlockLoop

int32AddScalarTailLoop:
	CMPQ    BX, $0x00000008
	JL      int32AddScalarDone
	VPADDD  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     int32AddScalarTailLoop

int32AddScalarDone:
	RET

// func int64AddAvx2Asm(x []int64, y []int64, r []int64)
// Requires: AVX, AVX2
TEXT ·int64AddAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int64AddBlockLoop:
	CMPQ    BX, $0x00000018
	JL      int64AddTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VPADDQ  (CX), Y0, Y0
	VPADDQ  32(CX), Y1, Y1
	VPADDQ  64(CX), Y2, Y2
	VPADDQ  96(CX), Y3, Y3
	VPADDQ  128(CX), Y4, Y4
	VPADDQ  160(CX), Y5, Y5
	VMOVDQU Y0, (DX)
	VMOVDQU Y1, 32(DX)
	VMOVDQU Y2, 64(DX)
	VMOVDQU Y3, 96(DX)
	VMOVDQU Y4, 128(DX)
	VMOVDQU Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000018, BX
	JMP     int64AddBlockLoop

int64AddTailLoop:
	CMPQ    BX, $0x00000004
	JL      int64AddDone
	VMOVDQU (AX), Y0
	VPADDQ  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     int64AddTailLoop

int64AddDone:
	RET

// func int64AddScalarAvx2Asm(x int64, y []int64, r []int64)
// Requires: AVX, AVX2, SSE2
TEXT ·int64AddScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVQ         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVQ         AX, X0
	VPBROADCASTQ X0, Y0

int64AddScalarBlockLoop:
	CMPQ    BX, $0x00000018
	JL      int64AddScalarTailLoop
	VPADDQ  (CX), Y0, Y1
	VPADDQ  32(CX), Y0, Y2
	VPADDQ  64(CX), Y0, Y3
	VPADDQ  96(CX), Y0, Y4
	VPADDQ  128(CX), Y0, Y5
	VPADDQ  160(CX), Y0, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000018, BX
	JMP     int64AddScalarBlockLoop

int64AddScalarTailLoop:
	CMPQ    BX, $0x00000004
	JL      int64AddScalarDone
	VPADDQ  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     int64AddScalarTailLoop

int64AddScalarDone:
	RET

// func uint8AddAvx2Asm(x []uint8, y []uint8, r []uint8)
// Requires: AVX, AVX2
TEXT ·uint8AddAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint8AddBlockLoop:
	CMPQ    BX, $0x000000c0
	JL      uint8AddTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VPADDB  (CX), Y0, Y0
	VPADDB  32(CX), Y1, Y1
	VPADDB  64(CX), Y2, Y2
	VPADDB  96(CX), Y3, Y3
	VPADDB  128(CX), Y4, Y4
	VPADDB  160(CX), Y5, Y5
	VMOVDQU Y0, (DX)
	VMOVDQU Y1, 32(DX)
	VMOVDQU Y2, 64(DX)
	VMOVDQU Y3, 96(DX)
	VMOVDQU Y4, 128(DX)
	VMOVDQU Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x000000c0, BX
	JMP     uint8AddBlockLoop

uint8AddTailLoop:
	CMPQ    BX, $0x00000020
	JL      uint8AddDone
	VMOVDQU (AX), Y0
	VPADDB  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000020, BX
	JMP     uint8AddTailLoop

uint8AddDone:
	RET

// func uint8AddScalarAvx2Asm(x uint8, y []uint8, r []uint8)
// Requires: AVX, AVX2, SSE2
TEXT ·uint8AddScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVBLZX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTB X0, Y0

uint8AddScalarBlockLoop:
	CMPQ    BX, $0x000000c0
	JL      uint8AddScalarTailLoop
	VPADDB  (CX), Y0, Y1
	VPADDB  32(CX), Y0, Y2
	VPADDB  64(CX), Y0, Y3
	VPADDB  96(CX), Y0, Y4
	VPADDB  128(CX), Y0, Y5
	VPADDB  160(CX), Y0, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x000000c0, BX
	JMP     uint8AddScalarBlockLoop

uint8AddScalarTailLoop:
	CMPQ    BX, $0x00000020
	JL      uint8AddScalarDone
	VPADDB  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000020, BX
	JMP     uint8AddScalarTailLoop

uint8AddScalarDone:
	RET

// func uint16AddAvx2Asm(x []uint16, y []uint16, r []uint16)
// Requires: AVX, AVX2
TEXT ·uint16AddAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint16AddBlockLoop:
	CMPQ    BX, $0x00000060
	JL      uint16AddTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VPADDW  (CX), Y0, Y0
	VPADDW  32(CX), Y1, Y1
	VPADDW  64(CX), Y2, Y2
	VPADDW  96(CX), Y3, Y3
	VPADDW  128(CX), Y4, Y4
	VPADDW  160(CX), Y5, Y5
	VMOVDQU Y0, (DX)
	VMOVDQU Y1, 32(DX)
	VMOVDQU Y2, 64(DX)
	VMOVDQU Y3, 96(DX)
	VMOVDQU Y4, 128(DX)
	VMOVDQU Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000060, BX
	JMP     uint16AddBlockLoop

uint16AddTailLoop:
	CMPQ    BX, $0x00000010
	JL      uint16AddDone
	VMOVDQU (AX), Y0
	VPADDW  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000010, BX
	JMP     uint16AddTailLoop

uint16AddDone:
	RET

// func uint16AddScalarAvx2Asm(x uint16, y []uint16, r []uint16)
// Requires: AVX, AVX2, SSE2
TEXT ·uint16AddScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVWLZX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTW X0, Y0

uint16AddScalarBlockLoop:
	CMPQ    BX, $0x00000060
	JL      uint16AddScalarTailLoop
	VPADDW  (CX), Y0, Y1
	VPADDW  32(CX), Y0, Y2
	VPADDW  64(CX), Y0, Y3
	VPADDW  96(CX), Y0, Y4
	VPADDW  128(CX), Y0, Y5
	VPADDW  160(CX), Y0, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000060, BX
	JMP     uint16AddScalarBlockLoop

uint16AddScalarTailLoop:
	CMPQ    BX, $0x00000010
	JL      uint16AddScalarDone
	VPADDW  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000010, BX
	JMP     uint16AddScalarTailLoop

uint16AddScalarDone:
	RET

// func uint32AddAvx2Asm(x []uint32, y []uint32, r []uint32)
// Requires: AVX, AVX2
TEXT ·uint32AddAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint32AddBlockLoop:
	CMPQ    BX, $0x00000030
	JL      uint32AddTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VPADDD  (CX), Y0, Y0
	VPADDD  32(CX), Y1, Y1
	VPADDD  64(CX), Y2, Y2
	VPADDD  96(CX), Y3, Y3
	VPADDD  128(CX), Y4, Y4
	VPADDD  160(CX), Y5, Y5
	VMOVDQU Y0, (DX)
	VMOVDQU Y1, 32(DX)
	VMOVDQU Y2, 64(DX)
	VMOVDQU Y3, 96(DX)
	VMOVDQU Y4, 128(DX)
	VMOVDQU Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000030, BX
	JMP     uint32AddBlockLoop

uint32AddTailLoop:
	CMPQ    BX, $0x00000008
	JL      uint32AddDone
	VMOVDQU (AX), Y0
	VPADDD  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     uint32AddTailLoop

uint32AddDone:
	RET

// func uint32AddScalarAvx2Asm(x uint32, y []uint32, r []uint32)
// Requires: AVX, AVX2, SSE2
TEXT ·uint32AddScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVL         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTD X0, Y0

uint32AddScalarBlockLoop:
	CMPQ    BX, $0x00000030
	JL      uint32AddScalarTailLoop
	VPADDD  (CX), Y0, Y1
	VPADDD  32(CX), Y0, Y2
	VPADDD  64(CX), Y0, Y3
	VPADDD  96(CX), Y0, Y4
	VPADDD  128(CX), Y0, Y5
	VPADDD  160(CX), Y0, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000030, BX
	JMP     uint32AddScalarBlockLoop

uint32AddScalarTailLoop:
	CMPQ    BX, $0x00000008
	JL      uint32AddScalarDone
	VPADDD  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     uint32AddScalarTailLoop

uint32AddScalarDone:
	RET

// func uint64AddAvx2Asm(x []uint64, y []uint64, r []uint64)
// Requires: AVX, AVX2
TEXT ·uint64AddAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint64AddBlockLoop:
	CMPQ    BX, $0x00000018
	JL      uint64AddTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VPADDQ  (CX), Y0, Y0
	VPADDQ  32(CX), Y1, Y1
	VPADDQ  64(CX), Y2, Y2
	VPADDQ  96(CX), Y3, Y3
	VPADDQ  128(CX), Y4, Y4
	VPADDQ  160(CX), Y5, Y5
	VMOVDQU Y0, (DX)
	VMOVDQU Y1, 32(DX)
	VMOVDQU Y2, 64(DX)
	VMOVDQU Y3, 96(DX)
	VMOVDQU Y4, 128(DX)
	VMOVDQU Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000018, BX
	JMP     uint64AddBlockLoop

uint64AddTailLoop:
	CMPQ    BX, $0x00000004
	JL      uint64AddDone
	VMOVDQU (AX), Y0
	VPADDQ  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     uint64AddTailLoop

uint64AddDone:
	RET

// func uint64AddScalarAvx2Asm(x uint64, y []uint64, r []uint64)
// Requires: AVX, AVX2, SSE2
TEXT ·uint64AddScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVQ         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVQ         AX, X0
	VPBROADCASTQ X0, Y0

uint64AddScalarBlockLoop:
	CMPQ    BX, $0x00000018
	JL      uint64AddScalarTailLoop
	VPADDQ  (CX), Y0, Y1
	VPADDQ  32(CX), Y0, Y2
	VPADDQ  64(CX), Y0, Y3
	VPADDQ  96(CX), Y0, Y4
	VPADDQ  128(CX), Y0, Y5
	VPADDQ  160(CX), Y0, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000018, BX
	JMP     uint64AddScalarBlockLoop

uint64AddScalarTailLoop:
	CMPQ    BX, $0x00000004
	JL      uint64AddScalarDone
	VPADDQ  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     uint64AddScalarTailLoop

uint64AddScalarDone:
	RET

// func float32AddAvx2Asm(x []float32, y []float32, r []float32)
// Requires: AVX
TEXT ·float32AddAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

float32AddBlockLoop:
	CMPQ    BX, $0x00000030
	JL      float32AddTailLoop
	VMOVUPS (AX), Y0
	VMOVUPS 32(AX), Y1
	VMOVUPS 64(AX), Y2
	VMOVUPS 96(AX), Y3
	VMOVUPS 128(AX), Y4
	VMOVUPS 160(AX), Y5
	VADDPS  (CX), Y0, Y0
	VADDPS  32(CX), Y1, Y1
	VADDPS  64(CX), Y2, Y2
	VADDPS  96(CX), Y3, Y3
	VADDPS  128(CX), Y4, Y4
	VADDPS  160(CX), Y5, Y5
	VMOVUPS Y0, (DX)
	VMOVUPS Y1, 32(DX)
	VMOVUPS Y2, 64(DX)
	VMOVUPS Y3, 96(DX)
	VMOVUPS Y4, 128(DX)
	VMOVUPS Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000030, BX
	JMP     float32AddBlockLoop

float32AddTailLoop:
	CMPQ    BX, $0x00000008
	JL      float32AddDone
	VMOVUPS (AX), Y0
	VADDPS  (CX), Y0, Y0
	VMOVUPS Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     float32AddTailLoop

float32AddDone:
	RET

// func float32AddScalarAvx2Asm(x float32, y []float32, r []float32)
// Requires: AVX, AVX2, SSE
TEXT ·float32AddScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVSS        x+0(FP), X0
	MOVQ         y_base+8(FP), AX
	MOVQ         r_base+32(FP), CX
	MOVQ         y_len+16(FP), DX
	VBROADCASTSS X0, Y0

float32AddScalarBlockLoop:
	CMPQ    DX, $0x00000030
	JL      float32AddScalarTailLoop
	VADDPS  (AX), Y0, Y1
	VADDPS  32(AX), Y0, Y2
	VADDPS  64(AX), Y0, Y3
	VADDPS  96(AX), Y0, Y4
	VADDPS  128(AX), Y0, Y5
	VADDPS  160(AX), Y0, Y6
	VMOVUPS Y1, (CX)
	VMOVUPS Y2, 32(CX)
	VMOVUPS Y3, 64(CX)
	VMOVUPS Y4, 96(CX)
	VMOVUPS Y5, 128(CX)
	VMOVUPS Y6, 160(CX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	SUBQ    $0x00000030, DX
	JMP     float32AddScalarBlockLoop

float32AddScalarTailLoop:
	CMPQ    DX, $0x00000008
	JL      float32AddScalarDone
	VADDPS  (AX), Y0, Y1
	VMOVUPS Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000008, DX
	JMP     float32AddScalarTailLoop

float32AddScalarDone:
	RET

// func float64AddAvx2Asm(x []float64, y []float64, r []float64)
// Requires: AVX
TEXT ·float64AddAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

float64AddBlockLoop:
	CMPQ    BX, $0x00000018
	JL      float64AddTailLoop
	VMOVUPD (AX), Y0
	VMOVUPD 32(AX), Y1
	VMOVUPD 64(AX), Y2
	VMOVUPD 96(AX), Y3
	VMOVUPD 128(AX), Y4
	VMOVUPD 160(AX), Y5
	VADDPD  (CX), Y0, Y0
	VADDPD  32(CX), Y1, Y1
	VADDPD  64(CX), Y2, Y2
	VADDPD  96(CX), Y3, Y3
	VADDPD  128(CX), Y4, Y4
	VADDPD  160(CX), Y5, Y5
	VMOVUPD Y0, (DX)
	VMOVUPD Y1, 32(DX)
	VMOVUPD Y2, 64(DX)
	VMOVUPD Y3, 96(DX)
	VMOVUPD Y4, 128(DX)
	VMOVUPD Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000018, BX
	JMP     float64AddBlockLoop

float64AddTailLoop:
	CMPQ    BX, $0x00000004
	JL      float64AddDone
	VMOVUPD (AX), Y0
	VADDPD  (CX), Y0, Y0
	VMOVUPD Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     float64AddTailLoop

float64AddDone:
	RET

// func float64AddScalarAvx2Asm(x float64, y []float64, r []float64)
// Requires: AVX, AVX2, SSE2
TEXT ·float64AddScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVSD        x+0(FP), X0
	MOVQ         y_base+8(FP), AX
	MOVQ         r_base+32(FP), CX
	MOVQ         y_len+16(FP), DX
	VBROADCASTSD X0, Y0

float64AddScalarBlockLoop:
	CMPQ    DX, $0x00000018
	JL      float64AddScalarTailLoop
	VADDPD  (AX), Y0, Y1
	VADDPD  32(AX), Y0, Y2
	VADDPD  64(AX), Y0, Y3
	VADDPD  96(AX), Y0, Y4
	VADDPD  128(AX), Y0, Y5
	VADDPD  160(AX), Y0, Y6
	VMOVUPD Y1, (CX)
	VMOVUPD Y2, 32(CX)
	VMOVUPD Y3, 64(CX)
	VMOVUPD Y4, 96(CX)
	VMOVUPD Y5, 128(CX)
	VMOVUPD Y6, 160(CX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	SUBQ    $0x00000018, DX
	JMP     float64AddScalarBlockLoop

float64AddScalarTailLoop:
	CMPQ    DX, $0x00000004
	JL      float64AddScalarDone
	VADDPD  (AX), Y0, Y1
	VMOVUPD Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000004, DX
	JMP     float64AddScalarTailLoop

float64AddScalarDone:
	RET
