// Code generated by command: go run avx512.go -out add/avx512.s -stubs add/avx512_stubs.go. DO NOT EDIT.

#include "textflag.h"

// func int8AddAvx512Asm(x []int8, y []int8, r []int8)
// Requires: AVX512BW, AVX512F
TEXT ·int8AddAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int8AddBlockLoop:
	CMPQ      BX, $0x00000300
	JL        int8AddTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPADDB    (CX), Z0, Z0
	VPADDB    64(CX), Z1, Z1
	VPADDB    128(CX), Z2, Z2
	VPADDB    192(CX), Z3, Z3
	VPADDB    256(CX), Z4, Z4
	VPADDB    320(CX), Z5, Z5
	VPADDB    384(CX), Z6, Z6
	VPADDB    448(CX), Z7, Z7
	VPADDB    512(CX), Z8, Z8
	VPADDB    576(CX), Z9, Z9
	VPADDB    640(CX), Z10, Z10
	VPADDB    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000300, BX
	JMP       int8AddBlockLoop

int8AddTailLoop:
	CMPQ      BX, $0x00000040
	JL        int8AddDone
	VMOVDQU32 (AX), Z0
	VPADDB    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000040, BX
	JMP       int8AddTailLoop

int8AddDone:
	RET

// func int8AddScalarAvx512Asm(x int8, y []int8, r []int8)
// Requires: AVX512BW, AVX512F, SSE2
TEXT ·int8AddScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVBLSX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTB X0, Z0

int8AddScalarBlockLoop:
	CMPQ      BX, $0x00000300
	JL        int8AddScalarTailLoop
	VPADDB    (CX), Z0, Z1
	VPADDB    64(CX), Z0, Z2
	VPADDB    128(CX), Z0, Z3
	VPADDB    192(CX), Z0, Z4
	VPADDB    256(CX), Z0, Z5
	VPADDB    320(CX), Z0, Z6
	VPADDB    384(CX), Z0, Z7
	VPADDB    448(CX), Z0, Z8
	VPADDB    512(CX), Z0, Z9
	VPADDB    576(CX), Z0, Z10
	VPADDB    640(CX), Z0, Z11
	VPADDB    704(CX), Z0, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000300, BX
	JMP       int8AddScalarBlockLoop

int8AddScalarTailLoop:
	CMPQ      BX, $0x00000040
	JL        int8AddScalarDone
	VPADDB    (CX), Z0, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000040, BX
	JMP       int8AddScalarTailLoop

int8AddScalarDone:
	RET

// func int16AddAvx512Asm(x []int16, y []int16, r []int16)
// Requires: AVX512BW, AVX512F
TEXT ·int16AddAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int16AddBlockLoop:
	CMPQ      BX, $0x00000180
	JL        int16AddTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPADDW    (CX), Z0, Z0
	VPADDW    64(CX), Z1, Z1
	VPADDW    128(CX), Z2, Z2
	VPADDW    192(CX), Z3, Z3
	VPADDW    256(CX), Z4, Z4
	VPADDW    320(CX), Z5, Z5
	VPADDW    384(CX), Z6, Z6
	VPADDW    448(CX), Z7, Z7
	VPADDW    512(CX), Z8, Z8
	VPADDW    576(CX), Z9, Z9
	VPADDW    640(CX), Z10, Z10
	VPADDW    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000180, BX
	JMP       int16AddBlockLoop

int16AddTailLoop:
	CMPQ      BX, $0x00000020
	JL        int16AddDone
	VMOVDQU32 (AX), Z0
	VPADDW    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000020, BX
	JMP       int16AddTailLoop

int16AddDone:
	RET

// func int16AddScalarAvx512Asm(x int16, y []int16, r []int16)
// Requires: AVX512BW, AVX512F, SSE2
TEXT ·int16AddScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVWLSX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTW X0, Z0

int16AddScalarBlockLoop:
	CMPQ      BX, $0x00000180
	JL        int16AddScalarTailLoop
	VPADDW    (CX), Z0, Z1
	VPADDW    64(CX), Z0, Z2
	VPADDW    128(CX), Z0, Z3
	VPADDW    192(CX), Z0, Z4
	VPADDW    256(CX), Z0, Z5
	VPADDW    320(CX), Z0, Z6
	VPADDW    384(CX), Z0, Z7
	VPADDW    448(CX), Z0, Z8
	VPADDW    512(CX), Z0, Z9
	VPADDW    576(CX), Z0, Z10
	VPADDW    640(CX), Z0, Z11
	VPADDW    704(CX), Z0, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000180, BX
	JMP       int16AddScalarBlockLoop

int16AddScalarTailLoop:
	CMPQ      BX, $0x00000020
	JL        int16AddScalarDone
	VPADDW    (CX), Z0, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000020, BX
	JMP       int16AddScalarTailLoop

int16AddScalarDone:
	RET

// func int32AddAvx512Asm(x []int32, y []int32, r []int32)
// Requires: AVX512F
TEXT ·int32AddAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int32AddBlockLoop:
	CMPQ      BX, $0x000000c0
	JL        int32AddTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPADDD    (CX), Z0, Z0
	VPADDD    64(CX), Z1, Z1
	VPADDD    128(CX), Z2, Z2
	VPADDD    192(CX), Z3, Z3
	VPADDD    256(CX), Z4, Z4
	VPADDD    320(CX), Z5, Z5
	VPADDD    384(CX), Z6, Z6
	VPADDD    448(CX), Z7, Z7
	VPADDD    512(CX), Z8, Z8
	VPADDD    576(CX), Z9, Z9
	VPADDD    640(CX), Z10, Z10
	VPADDD    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x000000c0, BX
	JMP       int32AddBlockLoop

int32AddTailLoop:
	CMPQ      BX, $0x00000010
	JL        int32AddDone
	VMOVDQU32 (AX), Z0
	VPADDD    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000010, BX
	JMP       int32AddTailLoop

int32AddDone:
	RET

// func int32AddScalarAvx512Asm(x int32, y []int32, r []int32)
// Requires: AVX512F, SSE2
TEXT ·int32AddScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVL         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTD X0, Z0

int32AddScalarBlockLoop:
	CMPQ      BX, $0x000000c0
	JL        int32AddScalarTailLoop
	VPADDD    (CX), Z0, Z1
	VPADDD    64(CX), Z0, Z2
	VPADDD    128(CX), Z0, Z3
	VPADDD    192(CX), Z0, Z4
	VPADDD    256(CX), Z0, Z5
	VPADDD    320(CX), Z0, Z6
	VPADDD    384(CX), Z0, Z7
	VPADDD    448(CX), Z0, Z8
	VPADDD    512(CX), Z0, Z9
	VPADDD    576(CX), Z0, Z10
	VPADDD    640(CX), Z0, Z11
	VPADDD    704(CX), Z0, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x000000c0, BX
	JMP       int32AddScalarBlockLoop

int32AddScalarTailLoop:
	CMPQ      BX, $0x00000010
	JL        int32AddScalarDone
	VPADDD    (CX), Z0, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000010, BX
	JMP       int32AddScalarTailLoop

int32AddScalarDone:
	RET

// func int64AddAvx512Asm(x []int64, y []int64, r []int64)
// Requires: AVX512F
TEXT ·int64AddAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int64AddBlockLoop:
	CMPQ      BX, $0x00000060
	JL        int64AddTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPADDQ    (CX), Z0, Z0
	VPADDQ    64(CX), Z1, Z1
	VPADDQ    128(CX), Z2, Z2
	VPADDQ    192(CX), Z3, Z3
	VPADDQ    256(CX), Z4, Z4
	VPADDQ    320(CX), Z5, Z5
	VPADDQ    384(CX), Z6, Z6
	VPADDQ    448(CX), Z7, Z7
	VPADDQ    512(CX), Z8, Z8
	VPADDQ    576(CX), Z9, Z9
	VPADDQ    640(CX), Z10, Z10
	VPADDQ    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000060, BX
	JMP       int64AddBlockLoop

int64AddTailLoop:
	CMPQ      BX, $0x00000008
	JL        int64AddDone
	VMOVDQU32 (AX), Z0
	VPADDQ    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000008, BX
	JMP       int64AddTailLoop

int64AddDone:
	RET

// func int64AddScalarAvx512Asm(x int64, y []int64, r []int64)
// Requires: AVX512F, SSE2
TEXT ·int64AddScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVQ         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVQ         AX, X0
	VPBROADCASTQ X0, Z0

int64AddScalarBlockLoop:
	CMPQ      BX, $0x00000060
	JL        int64AddScalarTailLoop
	VPADDQ    (CX), Z0, Z1
	VPADDQ    64(CX), Z0, Z2
	VPADDQ    128(CX), Z0, Z3
	VPADDQ    192(CX), Z0, Z4
	VPADDQ    256(CX), Z0, Z5
	VPADDQ    320(CX), Z0, Z6
	VPADDQ    384(CX), Z0, Z7
	VPADDQ    448(CX), Z0, Z8
	VPADDQ    512(CX), Z0, Z9
	VPADDQ    576(CX), Z0, Z10
	VPADDQ    640(CX), Z0, Z11
	VPADDQ    704(CX), Z0, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000060, BX
	JMP       int64AddScalarBlockLoop

int64AddScalarTailLoop:
	CMPQ      BX, $0x00000008
	JL        int64AddScalarDone
	VPADDQ    (CX), Z0, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000008, BX
	JMP       int64AddScalarTailLoop

int64AddScalarDone:
	RET

// func uint8AddAvx512Asm(x []uint8, y []uint8, r []uint8)
// Requires: AVX512BW, AVX512F
TEXT ·uint8AddAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint8AddBlockLoop:
	CMPQ      BX, $0x00000300
	JL        uint8AddTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPADDB    (CX), Z0, Z0
	VPADDB    64(CX), Z1, Z1
	VPADDB    128(CX), Z2, Z2
	VPADDB    192(CX), Z3, Z3
	VPADDB    256(CX), Z4, Z4
	VPADDB    320(CX), Z5, Z5
	VPADDB    384(CX), Z6, Z6
	VPADDB    448(CX), Z7, Z7
	VPADDB    512(CX), Z8, Z8
	VPADDB    576(CX), Z9, Z9
	VPADDB    640(CX), Z10, Z10
	VPADDB    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000300, BX
	JMP       uint8AddBlockLoop

uint8AddTailLoop:
	CMPQ      BX, $0x00000040
	JL        uint8AddDone
	VMOVDQU32 (AX), Z0
	VPADDB    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000040, BX
	JMP       uint8AddTailLoop

uint8AddDone:
	RET

// func uint8AddScalarAvx512Asm(x uint8, y []uint8, r []uint8)
// Requires: AVX512BW, AVX512F, SSE2
TEXT ·uint8AddScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVBLZX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTB X0, Z0

uint8AddScalarBlockLoop:
	CMPQ      BX, $0x00000300
	JL        uint8AddScalarTailLoop
	VPADDB    (CX), Z0, Z1
	VPADDB    64(CX), Z0, Z2
	VPADDB    128(CX), Z0, Z3
	VPADDB    192(CX), Z0, Z4
	VPADDB    256(CX), Z0, Z5
	VPADDB    320(CX), Z0, Z6
	VPADDB    384(CX), Z0, Z7
	VPADDB    448(CX), Z0, Z8
	VPADDB    512(CX), Z0, Z9
	VPADDB    576(CX), Z0, Z10
	VPADDB    640(CX), Z0, Z11
	VPADDB    704(CX), Z0, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000300, BX
	JMP       uint8AddScalarBlockLoop

uint8AddScalarTailLoop:
	CMPQ      BX, $0x00000040
	JL        uint8AddScalarDone
	VPADDB    (CX), Z0, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000040, BX
	JMP       uint8AddScalarTailLoop

uint8AddScalarDone:
	RET

// func uint16AddAvx512Asm(x []uint16, y []uint16, r []uint16)
// Requires: AVX512BW, AVX512F
TEXT ·uint16AddAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint16AddBlockLoop:
	CMPQ      BX, $0x00000180
	JL        uint16AddTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPADDW    (CX), Z0, Z0
	VPADDW    64(CX), Z1, Z1
	VPADDW    128(CX), Z2, Z2
	VPADDW    192(CX), Z3, Z3
	VPADDW    256(CX), Z4, Z4
	VPADDW    320(CX), Z5, Z5
	VPADDW    384(CX), Z6, Z6
	VPADDW    448(CX), Z7, Z7
	VPADDW    512(CX), Z8, Z8
	VPADDW    576(CX), Z9, Z9
	VPADDW    640(CX), Z10, Z10
	VPADDW    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000180, BX
	JMP       uint16AddBlockLoop

uint16AddTailLoop:
	CMPQ      BX, $0x00000020
	JL        uint16AddDone
	VMOVDQU32 (AX), Z0
	VPADDW    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000020, BX
	JMP       uint16AddTailLoop

uint16AddDone:
	RET

// func uint16AddScalarAvx512Asm(x uint16, y []uint16, r []uint16)
// Requires: AVX512BW, AVX512F, SSE2
TEXT ·uint16AddScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVWLZX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTW X0, Z0

uint16AddScalarBlockLoop:
	CMPQ      BX, $0x00000180
	JL        uint16AddScalarTailLoop
	VPADDW    (CX), Z0, Z1
	VPADDW    64(CX), Z0, Z2
	VPADDW    128(CX), Z0, Z3
	VPADDW    192(CX), Z0, Z4
	VPADDW    256(CX), Z0, Z5
	VPADDW    320(CX), Z0, Z6
	VPADDW    384(CX), Z0, Z7
	VPADDW    448(CX), Z0, Z8
	VPADDW    512(CX), Z0, Z9
	VPADDW    576(CX), Z0, Z10
	VPADDW    640(CX), Z0, Z11
	VPADDW    704(CX), Z0, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000180, BX
	JMP       uint16AddScalarBlockLoop

uint16AddScalarTailLoop:
	CMPQ      BX, $0x00000020
	JL        uint16AddScalarDone
	VPADDW    (CX), Z0, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000020, BX
	JMP       uint16AddScalarTailLoop

uint16AddScalarDone:
	RET

// func uint32AddAvx512Asm(x []uint32, y []uint32, r []uint32)
// Requires: AVX512F
TEXT ·uint32AddAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint32AddBlockLoop:
	CMPQ      BX, $0x000000c0
	JL        uint32AddTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPADDD    (CX), Z0, Z0
	VPADDD    64(CX), Z1, Z1
	VPADDD    128(CX), Z2, Z2
	VPADDD    192(CX), Z3, Z3
	VPADDD    256(CX), Z4, Z4
	VPADDD    320(CX), Z5, Z5
	VPADDD    384(CX), Z6, Z6
	VPADDD    448(CX), Z7, Z7
	VPADDD    512(CX), Z8, Z8
	VPADDD    576(CX), Z9, Z9
	VPADDD    640(CX), Z10, Z10
	VPADDD    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x000000c0, BX
	JMP       uint32AddBlockLoop

uint32AddTailLoop:
	CMPQ      BX, $0x00000010
	JL        uint32AddDone
	VMOVDQU32 (AX), Z0
	VPADDD    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000010, BX
	JMP       uint32AddTailLoop

uint32AddDone:
	RET

// func uint32AddScalarAvx512Asm(x uint32, y []uint32, r []uint32)
// Requires: AVX512F, SSE2
TEXT ·uint32AddScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVL         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTD X0, Z0

uint32AddScalarBlockLoop:
	CMPQ      BX, $0x000000c0
	JL        uint32AddScalarTailLoop
	VPADDD    (CX), Z0, Z1
	VPADDD    64(CX), Z0, Z2
	VPADDD    128(CX), Z0, Z3
	VPADDD    192(CX), Z0, Z4
	VPADDD    256(CX), Z0, Z5
	VPADDD    320(CX), Z0, Z6
	VPADDD    384(CX), Z0, Z7
	VPADDD    448(CX), Z0, Z8
	VPADDD    512(CX), Z0, Z9
	VPADDD    576(CX), Z0, Z10
	VPADDD    640(CX), Z0, Z11
	VPADDD    704(CX), Z0, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x000000c0, BX
	JMP       uint32AddScalarBlockLoop

uint32AddScalarTailLoop:
	CMPQ      BX, $0x00000010
	JL        uint32AddScalarDone
	VPADDD    (CX), Z0, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000010, BX
	JMP       uint32AddScalarTailLoop

uint32AddScalarDone:
	RET

// func uint64AddAvx512Asm(x []uint64, y []uint64, r []uint64)
// Requires: AVX512F
TEXT ·uint64AddAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint64AddBlockLoop:
	CMPQ      BX, $0x00000060
	JL        uint64AddTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPADDQ    (CX), Z0, Z0
	VPADDQ    64(CX), Z1, Z1
	VPADDQ    128(CX), Z2, Z2
	VPADDQ    192(CX), Z3, Z3
	VPADDQ    256(CX), Z4, Z4
	VPADDQ    320(CX), Z5, Z5
	VPADDQ    384(CX), Z6, Z6
	VPADDQ    448(CX), Z7, Z7
	VPADDQ    512(CX), Z8, Z8
	VPADDQ    576(CX), Z9, Z9
	VPADDQ    640(CX), Z10, Z10
	VPADDQ    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000060, BX
	JMP       uint64AddBlockLoop

uint64AddTailLoop:
	CMPQ      BX, $0x00000008
	JL        uint64AddDone
	VMOVDQU32 (AX), Z0
	VPADDQ    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000008, BX
	JMP       uint64AddTailLoop

uint64AddDone:
	RET

// func uint64AddScalarAvx512Asm(x uint64, y []uint64, r []uint64)
// Requires: AVX512F, SSE2
TEXT ·uint64AddScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVQ         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVQ         AX, X0
	VPBROADCASTQ X0, Z0

uint64AddScalarBlockLoop:
	CMPQ      BX, $0x00000060
	JL        uint64AddScalarTailLoop
	VPADDQ    (CX), Z0, Z1
	VPADDQ    64(CX), Z0, Z2
	VPADDQ    128(CX), Z0, Z3
	VPADDQ    192(CX), Z0, Z4
	VPADDQ    256(CX), Z0, Z5
	VPADDQ    320(CX), Z0, Z6
	VPADDQ    384(CX), Z0, Z7
	VPADDQ    448(CX), Z0, Z8
	VPADDQ    512(CX), Z0, Z9
	VPADDQ    576(CX), Z0, Z10
	VPADDQ    640(CX), Z0, Z11
	VPADDQ    704(CX), Z0, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000060, BX
	JMP       uint64AddScalarBlockLoop

uint64AddScalarTailLoop:
	CMPQ      BX, $0x00000008
	JL        uint64AddScalarDone
	VPADDQ    (CX), Z0, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000008, BX
	JMP       uint64AddScalarTailLoop

uint64AddScalarDone:
	RET

// func float32AddAvx512Asm(x []float32, y []float32, r []float32)
// Requires: AVX512F
TEXT ·float32AddAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

float32AddBlockLoop:
	CMPQ    BX, $0x000000c0
	JL      float32AddTailLoop
	VMOVUPS (AX), Z0
	VMOVUPS 64(AX), Z1
	VMOVUPS 128(AX), Z2
	VMOVUPS 192(AX), Z3
	VMOVUPS 256(AX), Z4
	VMOVUPS 320(AX), Z5
	VMOVUPS 384(AX), Z6
	VMOVUPS 448(AX), Z7
	VMOVUPS 512(AX), Z8
	VMOVUPS 576(AX), Z9
	VMOVUPS 640(AX), Z10
	VMOVUPS 704(AX), Z11
	VADDPS  (CX), Z0, Z0
	VADDPS  64(CX), Z1, Z1
	VADDPS  128(CX), Z2, Z2
	VADDPS  192(CX), Z3, Z3
	VADDPS  256(CX), Z4, Z4
	VADDPS  320(CX), Z5, Z5
	VADDPS  384(CX), Z6, Z6
	VADDPS  448(CX), Z7, Z7
	VADDPS  512(CX), Z8, Z8
	VADDPS  576(CX), Z9, Z9
	VADDPS  640(CX), Z10, Z10
	VADDPS  704(CX), Z11, Z11
	VMOVUPS Z0, (DX)
	VMOVUPS Z1, 64(DX)
	VMOVUPS Z2, 128(DX)
	VMOVUPS Z3, 192(DX)
	VMOVUPS Z4, 256(DX)
	VMOVUPS Z5, 320(DX)
	VMOVUPS Z6, 384(DX)
	VMOVUPS Z7, 448(DX)
	VMOVUPS Z8, 512(DX)
	VMOVUPS Z9, 576(DX)
	VMOVUPS Z10, 640(DX)
	VMOVUPS Z11, 704(DX)
	ADDQ    $0x00000300, AX
	ADDQ    $0x00000300, CX
	ADDQ    $0x00000300, DX
	SUBQ    $0x000000c0, BX
	JMP     float32AddBlockLoop

float32AddTailLoop:
	CMPQ    BX, $0x00000010
	JL      float32AddDone
	VMOVUPS (AX), Z0
	VADDPS  (CX), Z0, Z0
	VMOVUPS Z0, (DX)
	ADDQ    $0x00000040, AX
	ADDQ    $0x00000040, CX
	ADDQ    $0x00000040, DX
	SUBQ    $0x00000010, BX
	JMP     float32AddTailLoop

float32AddDone:
	RET

// func float32AddScalarAvx512Asm(x float32, y []float32, r []float32)
// Requires: AVX512F, SSE
TEXT ·float32AddScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVSS        x+0(FP), X0
	MOVQ         y_base+8(FP), AX
	MOVQ         r_base+32(FP), CX
	MOVQ         y_len+16(FP), DX
	VBROADCASTSS X0, Z0

float32AddScalarBlockLoop:
	CMPQ    DX, $0x000000c0
	JL      float32AddScalarTailLoop
	VADDPS  (AX), Z0, Z1
	VADDPS  64(AX), Z0, Z2
	VADDPS  128(AX), Z0, Z3
	VADDPS  192(AX), Z0, Z4
	VADDPS  256(AX), Z0, Z5
	VADDPS  320(AX), Z0, Z6
	VADDPS  384(AX), Z0, Z7
	VADDPS  448(AX), Z0, Z8
	VADDPS  512(AX), Z0, Z9
	VADDPS  576(AX), Z0, Z10
	VADDPS  640(AX), Z0, Z11
	VADDPS  704(AX), Z0, Z12
	VMOVUPS Z1, (CX)
	VMOVUPS Z2, 64(CX)
	VMOVUPS Z3, 128(CX)
	VMOVUPS Z4, 192(CX)
	VMOVUPS Z5, 256(CX)
	VMOVUPS Z6, 320(CX)
	VMOVUPS Z7, 384(CX)
	VMOVUPS Z8, 448(CX)
	VMOVUPS Z9, 512(CX)
	VMOVUPS Z10, 576(CX)
	VMOVUPS Z11, 640(CX)
	VMOVUPS Z12, 704(CX)
	ADDQ    $0x00000300, AX
	ADDQ    $0x00000300, CX
	SUBQ    $0x000000c0, DX
	JMP     float32AddScalarBlockLoop

float32AddScalarTailLoop:
	CMPQ    DX, $0x00000010
	JL      float32AddScalarDone
	VADDPS  (AX), Z0, Z1
	VMOVUPS Z1, (CX)
	ADDQ    $0x00000040, AX
	ADDQ    $0x00000040, CX
	SUBQ    $0x00000010, DX
	JMP     float32AddScalarTailLoop

float32AddScalarDone:
	RET

// func float64AddAvx512Asm(x []float64, y []float64, r []float64)
// Requires: AVX512F
TEXT ·float64AddAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

float64AddBlockLoop:
	CMPQ    BX, $0x00000060
	JL      float64AddTailLoop
	VMOVUPD (AX), Z0
	VMOVUPD 64(AX), Z1
	VMOVUPD 128(AX), Z2
	VMOVUPD 192(AX), Z3
	VMOVUPD 256(AX), Z4
	VMOVUPD 320(AX), Z5
	VMOVUPD 384(AX), Z6
	VMOVUPD 448(AX), Z7
	VMOVUPD 512(AX), Z8
	VMOVUPD 576(AX), Z9
	VMOVUPD 640(AX), Z10
	VMOVUPD 704(AX), Z11
	VADDPD  (CX), Z0, Z0
	VADDPD  64(CX), Z1, Z1
	VADDPD  128(CX), Z2, Z2
	VADDPD  192(CX), Z3, Z3
	VADDPD  256(CX), Z4, Z4
	VADDPD  320(CX), Z5, Z5
	VADDPD  384(CX), Z6, Z6
	VADDPD  448(CX), Z7, Z7
	VADDPD  512(CX), Z8, Z8
	VADDPD  576(CX), Z9, Z9
	VADDPD  640(CX), Z10, Z10
	VADDPD  704(CX), Z11, Z11
	VMOVUPD Z0, (DX)
	VMOVUPD Z1, 64(DX)
	VMOVUPD Z2, 128(DX)
	VMOVUPD Z3, 192(DX)
	VMOVUPD Z4, 256(DX)
	VMOVUPD Z5, 320(DX)
	VMOVUPD Z6, 384(DX)
	VMOVUPD Z7, 448(DX)
	VMOVUPD Z8, 512(DX)
	VMOVUPD Z9, 576(DX)
	VMOVUPD Z10, 640(DX)
	VMOVUPD Z11, 704(DX)
	ADDQ    $0x00000300, AX
	ADDQ    $0x00000300, CX
	ADDQ    $0x00000300, DX
	SUBQ    $0x00000060, BX
	JMP     float64AddBlockLoop

float64AddTailLoop:
	CMPQ    BX, $0x00000008
	JL      float64AddDone
	VMOVUPD (AX), Z0
	VADDPD  (CX), Z0, Z0
	VMOVUPD Z0, (DX)
	ADDQ    $0x00000040, AX
	ADDQ    $0x00000040, CX
	ADDQ    $0x00000040, DX
	SUBQ    $0x00000008, BX
	JMP     float64AddTailLoop

float64AddDone:
	RET

// func float64AddScalarAvx512Asm(x float64, y []float64, r []float64)
// Requires: AVX512F, SSE2
TEXT ·float64AddScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVSD        x+0(FP), X0
	MOVQ         y_base+8(FP), AX
	MOVQ         r_base+32(FP), CX
	MOVQ         y_len+16(FP), DX
	VBROADCASTSD X0, Z0

float64AddScalarBlockLoop:
	CMPQ    DX, $0x00000060
	JL      float64AddScalarTailLoop
	VADDPD  (AX), Z0, Z1
	VADDPD  64(AX), Z0, Z2
	VADDPD  128(AX), Z0, Z3
	VADDPD  192(AX), Z0, Z4
	VADDPD  256(AX), Z0, Z5
	VADDPD  320(AX), Z0, Z6
	VADDPD  384(AX), Z0, Z7
	VADDPD  448(AX), Z0, Z8
	VADDPD  512(AX), Z0, Z9
	VADDPD  576(AX), Z0, Z10
	VADDPD  640(AX), Z0, Z11
	VADDPD  704(AX), Z0, Z12
	VMOVUPD Z1, (CX)
	VMOVUPD Z2, 64(CX)
	VMOVUPD Z3, 128(CX)
	VMOVUPD Z4, 192(CX)
	VMOVUPD Z5, 256(CX)
	VMOVUPD Z6, 320(CX)
	VMOVUPD Z7, 384(CX)
	VMOVUPD Z8, 448(CX)
	VMOVUPD Z9, 512(CX)
	VMOVUPD Z10, 576(CX)
	VMOVUPD Z11, 640(CX)
	VMOVUPD Z12, 704(CX)
	ADDQ    $0x00000300, AX
	ADDQ    $0x00000300, CX
	SUBQ    $0x00000060, DX
	JMP     float64AddScalarBlockLoop

float64AddScalarTailLoop:
	CMPQ    DX, $0x00000008
	JL      float64AddScalarDone
	VADDPD  (AX), Z0, Z1
	VMOVUPD Z1, (CX)
	ADDQ    $0x00000040, AX
	ADDQ    $0x00000040, CX
	SUBQ    $0x00000008, DX
	JMP     float64AddScalarTailLoop

float64AddScalarDone:
	RET
