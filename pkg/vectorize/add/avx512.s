// Code generated by command: go run avx512.go -out plus/avx512.s -stubs plus/avx512_stubs.go. DO NOT EDIT.

#include "textflag.h"

// func int8PlusAvx512Asm(x []int8, y []int8, r []int8)
// Requires: AVX512BW, AVX512F
TEXT ·int8PlusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int8PlusBlockLoop:
	CMPQ      BX, $0x00000300
	JL        int8PlusTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPADDB    (CX), Z0, Z0
	VPADDB    64(CX), Z1, Z1
	VPADDB    128(CX), Z2, Z2
	VPADDB    192(CX), Z3, Z3
	VPADDB    256(CX), Z4, Z4
	VPADDB    320(CX), Z5, Z5
	VPADDB    384(CX), Z6, Z6
	VPADDB    448(CX), Z7, Z7
	VPADDB    512(CX), Z8, Z8
	VPADDB    576(CX), Z9, Z9
	VPADDB    640(CX), Z10, Z10
	VPADDB    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000300, BX
	JMP       int8PlusBlockLoop

int8PlusTailLoop:
	CMPQ      BX, $0x00000040
	JL        int8PlusDone
	VMOVDQU32 (AX), Z0
	VPADDB    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000040, BX
	JMP       int8PlusTailLoop

int8PlusDone:
	RET

// func int8PlusScalarAvx512Asm(x int8, y []int8, r []int8)
// Requires: AVX512BW, AVX512F, SSE2
TEXT ·int8PlusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVBLSX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTB X0, Z0

int8PlusScalarBlockLoop:
	CMPQ      BX, $0x00000300
	JL        int8PlusScalarTailLoop
	VMOVDQU32 (CX), Z1
	VMOVDQU32 64(CX), Z2
	VMOVDQU32 128(CX), Z3
	VMOVDQU32 192(CX), Z4
	VMOVDQU32 256(CX), Z5
	VMOVDQU32 320(CX), Z6
	VMOVDQU32 384(CX), Z7
	VMOVDQU32 448(CX), Z8
	VMOVDQU32 512(CX), Z9
	VMOVDQU32 576(CX), Z10
	VMOVDQU32 640(CX), Z11
	VMOVDQU32 704(CX), Z12
	VPADDB    Z0, Z1, Z1
	VPADDB    Z0, Z2, Z2
	VPADDB    Z0, Z3, Z3
	VPADDB    Z0, Z4, Z4
	VPADDB    Z0, Z5, Z5
	VPADDB    Z0, Z6, Z6
	VPADDB    Z0, Z7, Z7
	VPADDB    Z0, Z8, Z8
	VPADDB    Z0, Z9, Z9
	VPADDB    Z0, Z10, Z10
	VPADDB    Z0, Z11, Z11
	VPADDB    Z0, Z12, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000300, BX
	JMP       int8PlusScalarBlockLoop

int8PlusScalarTailLoop:
	CMPQ      BX, $0x00000040
	JL        int8PlusScalarDone
	VMOVDQU32 (CX), Z1
	VPADDB    Z0, Z1, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000040, BX
	JMP       int8PlusScalarTailLoop

int8PlusScalarDone:
	RET

// func int16PlusAvx512Asm(x []int16, y []int16, r []int16)
// Requires: AVX512BW, AVX512F
TEXT ·int16PlusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int16PlusBlockLoop:
	CMPQ      BX, $0x00000180
	JL        int16PlusTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPADDW    (CX), Z0, Z0
	VPADDW    64(CX), Z1, Z1
	VPADDW    128(CX), Z2, Z2
	VPADDW    192(CX), Z3, Z3
	VPADDW    256(CX), Z4, Z4
	VPADDW    320(CX), Z5, Z5
	VPADDW    384(CX), Z6, Z6
	VPADDW    448(CX), Z7, Z7
	VPADDW    512(CX), Z8, Z8
	VPADDW    576(CX), Z9, Z9
	VPADDW    640(CX), Z10, Z10
	VPADDW    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000180, BX
	JMP       int16PlusBlockLoop

int16PlusTailLoop:
	CMPQ      BX, $0x00000020
	JL        int16PlusDone
	VMOVDQU32 (AX), Z0
	VPADDW    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000020, BX
	JMP       int16PlusTailLoop

int16PlusDone:
	RET

// func int16PlusScalarAvx512Asm(x int16, y []int16, r []int16)
// Requires: AVX512BW, AVX512F, SSE2
TEXT ·int16PlusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVWLSX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTW X0, Z0

int16PlusScalarBlockLoop:
	CMPQ      BX, $0x00000180
	JL        int16PlusScalarTailLoop
	VMOVDQU32 (CX), Z1
	VMOVDQU32 64(CX), Z2
	VMOVDQU32 128(CX), Z3
	VMOVDQU32 192(CX), Z4
	VMOVDQU32 256(CX), Z5
	VMOVDQU32 320(CX), Z6
	VMOVDQU32 384(CX), Z7
	VMOVDQU32 448(CX), Z8
	VMOVDQU32 512(CX), Z9
	VMOVDQU32 576(CX), Z10
	VMOVDQU32 640(CX), Z11
	VMOVDQU32 704(CX), Z12
	VPADDW    Z0, Z1, Z1
	VPADDW    Z0, Z2, Z2
	VPADDW    Z0, Z3, Z3
	VPADDW    Z0, Z4, Z4
	VPADDW    Z0, Z5, Z5
	VPADDW    Z0, Z6, Z6
	VPADDW    Z0, Z7, Z7
	VPADDW    Z0, Z8, Z8
	VPADDW    Z0, Z9, Z9
	VPADDW    Z0, Z10, Z10
	VPADDW    Z0, Z11, Z11
	VPADDW    Z0, Z12, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000180, BX
	JMP       int16PlusScalarBlockLoop

int16PlusScalarTailLoop:
	CMPQ      BX, $0x00000020
	JL        int16PlusScalarDone
	VMOVDQU32 (CX), Z1
	VPADDW    Z0, Z1, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000020, BX
	JMP       int16PlusScalarTailLoop

int16PlusScalarDone:
	RET

// func int32PlusAvx512Asm(x []int32, y []int32, r []int32)
// Requires: AVX512F
TEXT ·int32PlusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int32PlusBlockLoop:
	CMPQ      BX, $0x000000c0
	JL        int32PlusTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPADDD    (CX), Z0, Z0
	VPADDD    64(CX), Z1, Z1
	VPADDD    128(CX), Z2, Z2
	VPADDD    192(CX), Z3, Z3
	VPADDD    256(CX), Z4, Z4
	VPADDD    320(CX), Z5, Z5
	VPADDD    384(CX), Z6, Z6
	VPADDD    448(CX), Z7, Z7
	VPADDD    512(CX), Z8, Z8
	VPADDD    576(CX), Z9, Z9
	VPADDD    640(CX), Z10, Z10
	VPADDD    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x000000c0, BX
	JMP       int32PlusBlockLoop

int32PlusTailLoop:
	CMPQ      BX, $0x00000010
	JL        int32PlusDone
	VMOVDQU32 (AX), Z0
	VPADDD    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000010, BX
	JMP       int32PlusTailLoop

int32PlusDone:
	RET

// func int32PlusScalarAvx512Asm(x int32, y []int32, r []int32)
// Requires: AVX512F, SSE2
TEXT ·int32PlusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVL         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTD X0, Z0

int32PlusScalarBlockLoop:
	CMPQ      BX, $0x000000c0
	JL        int32PlusScalarTailLoop
	VMOVDQU32 (CX), Z1
	VMOVDQU32 64(CX), Z2
	VMOVDQU32 128(CX), Z3
	VMOVDQU32 192(CX), Z4
	VMOVDQU32 256(CX), Z5
	VMOVDQU32 320(CX), Z6
	VMOVDQU32 384(CX), Z7
	VMOVDQU32 448(CX), Z8
	VMOVDQU32 512(CX), Z9
	VMOVDQU32 576(CX), Z10
	VMOVDQU32 640(CX), Z11
	VMOVDQU32 704(CX), Z12
	VPADDD    Z0, Z1, Z1
	VPADDD    Z0, Z2, Z2
	VPADDD    Z0, Z3, Z3
	VPADDD    Z0, Z4, Z4
	VPADDD    Z0, Z5, Z5
	VPADDD    Z0, Z6, Z6
	VPADDD    Z0, Z7, Z7
	VPADDD    Z0, Z8, Z8
	VPADDD    Z0, Z9, Z9
	VPADDD    Z0, Z10, Z10
	VPADDD    Z0, Z11, Z11
	VPADDD    Z0, Z12, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x000000c0, BX
	JMP       int32PlusScalarBlockLoop

int32PlusScalarTailLoop:
	CMPQ      BX, $0x00000010
	JL        int32PlusScalarDone
	VMOVDQU32 (CX), Z1
	VPADDD    Z0, Z1, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000010, BX
	JMP       int32PlusScalarTailLoop

int32PlusScalarDone:
	RET

// func int64PlusAvx512Asm(x []int64, y []int64, r []int64)
// Requires: AVX512F
TEXT ·int64PlusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int64PlusBlockLoop:
	CMPQ      BX, $0x00000060
	JL        int64PlusTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPADDQ    (CX), Z0, Z0
	VPADDQ    64(CX), Z1, Z1
	VPADDQ    128(CX), Z2, Z2
	VPADDQ    192(CX), Z3, Z3
	VPADDQ    256(CX), Z4, Z4
	VPADDQ    320(CX), Z5, Z5
	VPADDQ    384(CX), Z6, Z6
	VPADDQ    448(CX), Z7, Z7
	VPADDQ    512(CX), Z8, Z8
	VPADDQ    576(CX), Z9, Z9
	VPADDQ    640(CX), Z10, Z10
	VPADDQ    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000060, BX
	JMP       int64PlusBlockLoop

int64PlusTailLoop:
	CMPQ      BX, $0x00000008
	JL        int64PlusDone
	VMOVDQU32 (AX), Z0
	VPADDQ    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000008, BX
	JMP       int64PlusTailLoop

int64PlusDone:
	RET

// func int64PlusScalarAvx512Asm(x int64, y []int64, r []int64)
// Requires: AVX512F, SSE2
TEXT ·int64PlusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVQ         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVQ         AX, X0
	VPBROADCASTQ X0, Z0

int64PlusScalarBlockLoop:
	CMPQ      BX, $0x00000060
	JL        int64PlusScalarTailLoop
	VMOVDQU32 (CX), Z1
	VMOVDQU32 64(CX), Z2
	VMOVDQU32 128(CX), Z3
	VMOVDQU32 192(CX), Z4
	VMOVDQU32 256(CX), Z5
	VMOVDQU32 320(CX), Z6
	VMOVDQU32 384(CX), Z7
	VMOVDQU32 448(CX), Z8
	VMOVDQU32 512(CX), Z9
	VMOVDQU32 576(CX), Z10
	VMOVDQU32 640(CX), Z11
	VMOVDQU32 704(CX), Z12
	VPADDQ    Z0, Z1, Z1
	VPADDQ    Z0, Z2, Z2
	VPADDQ    Z0, Z3, Z3
	VPADDQ    Z0, Z4, Z4
	VPADDQ    Z0, Z5, Z5
	VPADDQ    Z0, Z6, Z6
	VPADDQ    Z0, Z7, Z7
	VPADDQ    Z0, Z8, Z8
	VPADDQ    Z0, Z9, Z9
	VPADDQ    Z0, Z10, Z10
	VPADDQ    Z0, Z11, Z11
	VPADDQ    Z0, Z12, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000060, BX
	JMP       int64PlusScalarBlockLoop

int64PlusScalarTailLoop:
	CMPQ      BX, $0x00000008
	JL        int64PlusScalarDone
	VMOVDQU32 (CX), Z1
	VPADDQ    Z0, Z1, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000008, BX
	JMP       int64PlusScalarTailLoop

int64PlusScalarDone:
	RET

// func uint8PlusAvx512Asm(x []uint8, y []uint8, r []uint8)
// Requires: AVX512BW, AVX512F
TEXT ·uint8PlusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint8PlusBlockLoop:
	CMPQ      BX, $0x00000300
	JL        uint8PlusTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPADDB    (CX), Z0, Z0
	VPADDB    64(CX), Z1, Z1
	VPADDB    128(CX), Z2, Z2
	VPADDB    192(CX), Z3, Z3
	VPADDB    256(CX), Z4, Z4
	VPADDB    320(CX), Z5, Z5
	VPADDB    384(CX), Z6, Z6
	VPADDB    448(CX), Z7, Z7
	VPADDB    512(CX), Z8, Z8
	VPADDB    576(CX), Z9, Z9
	VPADDB    640(CX), Z10, Z10
	VPADDB    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000300, BX
	JMP       uint8PlusBlockLoop

uint8PlusTailLoop:
	CMPQ      BX, $0x00000040
	JL        uint8PlusDone
	VMOVDQU32 (AX), Z0
	VPADDB    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000040, BX
	JMP       uint8PlusTailLoop

uint8PlusDone:
	RET

// func uint8PlusScalarAvx512Asm(x uint8, y []uint8, r []uint8)
// Requires: AVX512BW, AVX512F, SSE2
TEXT ·uint8PlusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVBLZX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTB X0, Z0

uint8PlusScalarBlockLoop:
	CMPQ      BX, $0x00000300
	JL        uint8PlusScalarTailLoop
	VMOVDQU32 (CX), Z1
	VMOVDQU32 64(CX), Z2
	VMOVDQU32 128(CX), Z3
	VMOVDQU32 192(CX), Z4
	VMOVDQU32 256(CX), Z5
	VMOVDQU32 320(CX), Z6
	VMOVDQU32 384(CX), Z7
	VMOVDQU32 448(CX), Z8
	VMOVDQU32 512(CX), Z9
	VMOVDQU32 576(CX), Z10
	VMOVDQU32 640(CX), Z11
	VMOVDQU32 704(CX), Z12
	VPADDB    Z0, Z1, Z1
	VPADDB    Z0, Z2, Z2
	VPADDB    Z0, Z3, Z3
	VPADDB    Z0, Z4, Z4
	VPADDB    Z0, Z5, Z5
	VPADDB    Z0, Z6, Z6
	VPADDB    Z0, Z7, Z7
	VPADDB    Z0, Z8, Z8
	VPADDB    Z0, Z9, Z9
	VPADDB    Z0, Z10, Z10
	VPADDB    Z0, Z11, Z11
	VPADDB    Z0, Z12, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000300, BX
	JMP       uint8PlusScalarBlockLoop

uint8PlusScalarTailLoop:
	CMPQ      BX, $0x00000040
	JL        uint8PlusScalarDone
	VMOVDQU32 (CX), Z1
	VPADDB    Z0, Z1, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000040, BX
	JMP       uint8PlusScalarTailLoop

uint8PlusScalarDone:
	RET

// func uint16PlusAvx512Asm(x []uint16, y []uint16, r []uint16)
// Requires: AVX512BW, AVX512F
TEXT ·uint16PlusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint16PlusBlockLoop:
	CMPQ      BX, $0x00000180
	JL        uint16PlusTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPADDW    (CX), Z0, Z0
	VPADDW    64(CX), Z1, Z1
	VPADDW    128(CX), Z2, Z2
	VPADDW    192(CX), Z3, Z3
	VPADDW    256(CX), Z4, Z4
	VPADDW    320(CX), Z5, Z5
	VPADDW    384(CX), Z6, Z6
	VPADDW    448(CX), Z7, Z7
	VPADDW    512(CX), Z8, Z8
	VPADDW    576(CX), Z9, Z9
	VPADDW    640(CX), Z10, Z10
	VPADDW    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000180, BX
	JMP       uint16PlusBlockLoop

uint16PlusTailLoop:
	CMPQ      BX, $0x00000020
	JL        uint16PlusDone
	VMOVDQU32 (AX), Z0
	VPADDW    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000020, BX
	JMP       uint16PlusTailLoop

uint16PlusDone:
	RET

// func uint16PlusScalarAvx512Asm(x uint16, y []uint16, r []uint16)
// Requires: AVX512BW, AVX512F, SSE2
TEXT ·uint16PlusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVWLZX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTW X0, Z0

uint16PlusScalarBlockLoop:
	CMPQ      BX, $0x00000180
	JL        uint16PlusScalarTailLoop
	VMOVDQU32 (CX), Z1
	VMOVDQU32 64(CX), Z2
	VMOVDQU32 128(CX), Z3
	VMOVDQU32 192(CX), Z4
	VMOVDQU32 256(CX), Z5
	VMOVDQU32 320(CX), Z6
	VMOVDQU32 384(CX), Z7
	VMOVDQU32 448(CX), Z8
	VMOVDQU32 512(CX), Z9
	VMOVDQU32 576(CX), Z10
	VMOVDQU32 640(CX), Z11
	VMOVDQU32 704(CX), Z12
	VPADDW    Z0, Z1, Z1
	VPADDW    Z0, Z2, Z2
	VPADDW    Z0, Z3, Z3
	VPADDW    Z0, Z4, Z4
	VPADDW    Z0, Z5, Z5
	VPADDW    Z0, Z6, Z6
	VPADDW    Z0, Z7, Z7
	VPADDW    Z0, Z8, Z8
	VPADDW    Z0, Z9, Z9
	VPADDW    Z0, Z10, Z10
	VPADDW    Z0, Z11, Z11
	VPADDW    Z0, Z12, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000180, BX
	JMP       uint16PlusScalarBlockLoop

uint16PlusScalarTailLoop:
	CMPQ      BX, $0x00000020
	JL        uint16PlusScalarDone
	VMOVDQU32 (CX), Z1
	VPADDW    Z0, Z1, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000020, BX
	JMP       uint16PlusScalarTailLoop

uint16PlusScalarDone:
	RET

// func uint32PlusAvx512Asm(x []uint32, y []uint32, r []uint32)
// Requires: AVX512F
TEXT ·uint32PlusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint32PlusBlockLoop:
	CMPQ      BX, $0x000000c0
	JL        uint32PlusTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPADDD    (CX), Z0, Z0
	VPADDD    64(CX), Z1, Z1
	VPADDD    128(CX), Z2, Z2
	VPADDD    192(CX), Z3, Z3
	VPADDD    256(CX), Z4, Z4
	VPADDD    320(CX), Z5, Z5
	VPADDD    384(CX), Z6, Z6
	VPADDD    448(CX), Z7, Z7
	VPADDD    512(CX), Z8, Z8
	VPADDD    576(CX), Z9, Z9
	VPADDD    640(CX), Z10, Z10
	VPADDD    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x000000c0, BX
	JMP       uint32PlusBlockLoop

uint32PlusTailLoop:
	CMPQ      BX, $0x00000010
	JL        uint32PlusDone
	VMOVDQU32 (AX), Z0
	VPADDD    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000010, BX
	JMP       uint32PlusTailLoop

uint32PlusDone:
	RET

// func uint32PlusScalarAvx512Asm(x uint32, y []uint32, r []uint32)
// Requires: AVX512F, SSE2
TEXT ·uint32PlusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVL         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTD X0, Z0

uint32PlusScalarBlockLoop:
	CMPQ      BX, $0x000000c0
	JL        uint32PlusScalarTailLoop
	VMOVDQU32 (CX), Z1
	VMOVDQU32 64(CX), Z2
	VMOVDQU32 128(CX), Z3
	VMOVDQU32 192(CX), Z4
	VMOVDQU32 256(CX), Z5
	VMOVDQU32 320(CX), Z6
	VMOVDQU32 384(CX), Z7
	VMOVDQU32 448(CX), Z8
	VMOVDQU32 512(CX), Z9
	VMOVDQU32 576(CX), Z10
	VMOVDQU32 640(CX), Z11
	VMOVDQU32 704(CX), Z12
	VPADDD    Z0, Z1, Z1
	VPADDD    Z0, Z2, Z2
	VPADDD    Z0, Z3, Z3
	VPADDD    Z0, Z4, Z4
	VPADDD    Z0, Z5, Z5
	VPADDD    Z0, Z6, Z6
	VPADDD    Z0, Z7, Z7
	VPADDD    Z0, Z8, Z8
	VPADDD    Z0, Z9, Z9
	VPADDD    Z0, Z10, Z10
	VPADDD    Z0, Z11, Z11
	VPADDD    Z0, Z12, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x000000c0, BX
	JMP       uint32PlusScalarBlockLoop

uint32PlusScalarTailLoop:
	CMPQ      BX, $0x00000010
	JL        uint32PlusScalarDone
	VMOVDQU32 (CX), Z1
	VPADDD    Z0, Z1, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000010, BX
	JMP       uint32PlusScalarTailLoop

uint32PlusScalarDone:
	RET

// func uint64PlusAvx512Asm(x []uint64, y []uint64, r []uint64)
// Requires: AVX512F
TEXT ·uint64PlusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint64PlusBlockLoop:
	CMPQ      BX, $0x00000060
	JL        uint64PlusTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPADDQ    (CX), Z0, Z0
	VPADDQ    64(CX), Z1, Z1
	VPADDQ    128(CX), Z2, Z2
	VPADDQ    192(CX), Z3, Z3
	VPADDQ    256(CX), Z4, Z4
	VPADDQ    320(CX), Z5, Z5
	VPADDQ    384(CX), Z6, Z6
	VPADDQ    448(CX), Z7, Z7
	VPADDQ    512(CX), Z8, Z8
	VPADDQ    576(CX), Z9, Z9
	VPADDQ    640(CX), Z10, Z10
	VPADDQ    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000060, BX
	JMP       uint64PlusBlockLoop

uint64PlusTailLoop:
	CMPQ      BX, $0x00000008
	JL        uint64PlusDone
	VMOVDQU32 (AX), Z0
	VPADDQ    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000008, BX
	JMP       uint64PlusTailLoop

uint64PlusDone:
	RET

// func uint64PlusScalarAvx512Asm(x uint64, y []uint64, r []uint64)
// Requires: AVX512F, SSE2
TEXT ·uint64PlusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVQ         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVQ         AX, X0
	VPBROADCASTQ X0, Z0

uint64PlusScalarBlockLoop:
	CMPQ      BX, $0x00000060
	JL        uint64PlusScalarTailLoop
	VMOVDQU32 (CX), Z1
	VMOVDQU32 64(CX), Z2
	VMOVDQU32 128(CX), Z3
	VMOVDQU32 192(CX), Z4
	VMOVDQU32 256(CX), Z5
	VMOVDQU32 320(CX), Z6
	VMOVDQU32 384(CX), Z7
	VMOVDQU32 448(CX), Z8
	VMOVDQU32 512(CX), Z9
	VMOVDQU32 576(CX), Z10
	VMOVDQU32 640(CX), Z11
	VMOVDQU32 704(CX), Z12
	VPADDQ    Z0, Z1, Z1
	VPADDQ    Z0, Z2, Z2
	VPADDQ    Z0, Z3, Z3
	VPADDQ    Z0, Z4, Z4
	VPADDQ    Z0, Z5, Z5
	VPADDQ    Z0, Z6, Z6
	VPADDQ    Z0, Z7, Z7
	VPADDQ    Z0, Z8, Z8
	VPADDQ    Z0, Z9, Z9
	VPADDQ    Z0, Z10, Z10
	VPADDQ    Z0, Z11, Z11
	VPADDQ    Z0, Z12, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000060, BX
	JMP       uint64PlusScalarBlockLoop

uint64PlusScalarTailLoop:
	CMPQ      BX, $0x00000008
	JL        uint64PlusScalarDone
	VMOVDQU32 (CX), Z1
	VPADDQ    Z0, Z1, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000008, BX
	JMP       uint64PlusScalarTailLoop

uint64PlusScalarDone:
	RET

// func float32PlusAvx512Asm(x []float32, y []float32, r []float32)
// Requires: AVX512F
TEXT ·float32PlusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

float32PlusBlockLoop:
	CMPQ    BX, $0x000000c0
	JL      float32PlusTailLoop
	VMOVUPS (AX), Z0
	VMOVUPS 64(AX), Z1
	VMOVUPS 128(AX), Z2
	VMOVUPS 192(AX), Z3
	VMOVUPS 256(AX), Z4
	VMOVUPS 320(AX), Z5
	VMOVUPS 384(AX), Z6
	VMOVUPS 448(AX), Z7
	VMOVUPS 512(AX), Z8
	VMOVUPS 576(AX), Z9
	VMOVUPS 640(AX), Z10
	VMOVUPS 704(AX), Z11
	VADDPS  (CX), Z0, Z0
	VADDPS  64(CX), Z1, Z1
	VADDPS  128(CX), Z2, Z2
	VADDPS  192(CX), Z3, Z3
	VADDPS  256(CX), Z4, Z4
	VADDPS  320(CX), Z5, Z5
	VADDPS  384(CX), Z6, Z6
	VADDPS  448(CX), Z7, Z7
	VADDPS  512(CX), Z8, Z8
	VADDPS  576(CX), Z9, Z9
	VADDPS  640(CX), Z10, Z10
	VADDPS  704(CX), Z11, Z11
	VMOVUPS Z0, (DX)
	VMOVUPS Z1, 64(DX)
	VMOVUPS Z2, 128(DX)
	VMOVUPS Z3, 192(DX)
	VMOVUPS Z4, 256(DX)
	VMOVUPS Z5, 320(DX)
	VMOVUPS Z6, 384(DX)
	VMOVUPS Z7, 448(DX)
	VMOVUPS Z8, 512(DX)
	VMOVUPS Z9, 576(DX)
	VMOVUPS Z10, 640(DX)
	VMOVUPS Z11, 704(DX)
	ADDQ    $0x00000300, AX
	ADDQ    $0x00000300, CX
	ADDQ    $0x00000300, DX
	SUBQ    $0x000000c0, BX
	JMP     float32PlusBlockLoop

float32PlusTailLoop:
	CMPQ    BX, $0x00000010
	JL      float32PlusDone
	VMOVUPS (AX), Z0
	VADDPS  (CX), Z0, Z0
	VMOVUPS Z0, (DX)
	ADDQ    $0x00000040, AX
	ADDQ    $0x00000040, CX
	ADDQ    $0x00000040, DX
	SUBQ    $0x00000010, BX
	JMP     float32PlusTailLoop

float32PlusDone:
	RET

// func float32PlusScalarAvx512Asm(x float32, y []float32, r []float32)
// Requires: AVX512F, SSE
TEXT ·float32PlusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVSS        x+0(FP), X0
	MOVQ         y_base+8(FP), AX
	MOVQ         r_base+32(FP), CX
	MOVQ         y_len+16(FP), DX
	VBROADCASTSS X0, Z0

float32PlusScalarBlockLoop:
	CMPQ    DX, $0x000000c0
	JL      float32PlusScalarTailLoop
	VMOVUPS (AX), Z1
	VMOVUPS 64(AX), Z2
	VMOVUPS 128(AX), Z3
	VMOVUPS 192(AX), Z4
	VMOVUPS 256(AX), Z5
	VMOVUPS 320(AX), Z6
	VMOVUPS 384(AX), Z7
	VMOVUPS 448(AX), Z8
	VMOVUPS 512(AX), Z9
	VMOVUPS 576(AX), Z10
	VMOVUPS 640(AX), Z11
	VMOVUPS 704(AX), Z12
	VADDPS  Z0, Z1, Z1
	VADDPS  Z0, Z2, Z2
	VADDPS  Z0, Z3, Z3
	VADDPS  Z0, Z4, Z4
	VADDPS  Z0, Z5, Z5
	VADDPS  Z0, Z6, Z6
	VADDPS  Z0, Z7, Z7
	VADDPS  Z0, Z8, Z8
	VADDPS  Z0, Z9, Z9
	VADDPS  Z0, Z10, Z10
	VADDPS  Z0, Z11, Z11
	VADDPS  Z0, Z12, Z12
	VMOVUPS Z1, (CX)
	VMOVUPS Z2, 64(CX)
	VMOVUPS Z3, 128(CX)
	VMOVUPS Z4, 192(CX)
	VMOVUPS Z5, 256(CX)
	VMOVUPS Z6, 320(CX)
	VMOVUPS Z7, 384(CX)
	VMOVUPS Z8, 448(CX)
	VMOVUPS Z9, 512(CX)
	VMOVUPS Z10, 576(CX)
	VMOVUPS Z11, 640(CX)
	VMOVUPS Z12, 704(CX)
	ADDQ    $0x00000300, AX
	ADDQ    $0x00000300, CX
	SUBQ    $0x000000c0, DX
	JMP     float32PlusScalarBlockLoop

float32PlusScalarTailLoop:
	CMPQ    DX, $0x00000010
	JL      float32PlusScalarDone
	VMOVUPS (AX), Z1
	VADDPS  Z0, Z1, Z1
	VMOVUPS Z1, (CX)
	ADDQ    $0x00000040, AX
	ADDQ    $0x00000040, CX
	SUBQ    $0x00000010, DX
	JMP     float32PlusScalarTailLoop

float32PlusScalarDone:
	RET

// func float64PlusAvx512Asm(x []float64, y []float64, r []float64)
// Requires: AVX512F
TEXT ·float64PlusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

float64PlusBlockLoop:
	CMPQ    BX, $0x00000060
	JL      float64PlusTailLoop
	VMOVUPD (AX), Z0
	VMOVUPD 64(AX), Z1
	VMOVUPD 128(AX), Z2
	VMOVUPD 192(AX), Z3
	VMOVUPD 256(AX), Z4
	VMOVUPD 320(AX), Z5
	VMOVUPD 384(AX), Z6
	VMOVUPD 448(AX), Z7
	VMOVUPD 512(AX), Z8
	VMOVUPD 576(AX), Z9
	VMOVUPD 640(AX), Z10
	VMOVUPD 704(AX), Z11
	VADDPD  (CX), Z0, Z0
	VADDPD  64(CX), Z1, Z1
	VADDPD  128(CX), Z2, Z2
	VADDPD  192(CX), Z3, Z3
	VADDPD  256(CX), Z4, Z4
	VADDPD  320(CX), Z5, Z5
	VADDPD  384(CX), Z6, Z6
	VADDPD  448(CX), Z7, Z7
	VADDPD  512(CX), Z8, Z8
	VADDPD  576(CX), Z9, Z9
	VADDPD  640(CX), Z10, Z10
	VADDPD  704(CX), Z11, Z11
	VMOVUPD Z0, (DX)
	VMOVUPD Z1, 64(DX)
	VMOVUPD Z2, 128(DX)
	VMOVUPD Z3, 192(DX)
	VMOVUPD Z4, 256(DX)
	VMOVUPD Z5, 320(DX)
	VMOVUPD Z6, 384(DX)
	VMOVUPD Z7, 448(DX)
	VMOVUPD Z8, 512(DX)
	VMOVUPD Z9, 576(DX)
	VMOVUPD Z10, 640(DX)
	VMOVUPD Z11, 704(DX)
	ADDQ    $0x00000300, AX
	ADDQ    $0x00000300, CX
	ADDQ    $0x00000300, DX
	SUBQ    $0x00000060, BX
	JMP     float64PlusBlockLoop

float64PlusTailLoop:
	CMPQ    BX, $0x00000008
	JL      float64PlusDone
	VMOVUPD (AX), Z0
	VADDPD  (CX), Z0, Z0
	VMOVUPD Z0, (DX)
	ADDQ    $0x00000040, AX
	ADDQ    $0x00000040, CX
	ADDQ    $0x00000040, DX
	SUBQ    $0x00000008, BX
	JMP     float64PlusTailLoop

float64PlusDone:
	RET

// func float64PlusScalarAvx512Asm(x float64, y []float64, r []float64)
// Requires: AVX512F, SSE2
TEXT ·float64PlusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVSD        x+0(FP), X0
	MOVQ         y_base+8(FP), AX
	MOVQ         r_base+32(FP), CX
	MOVQ         y_len+16(FP), DX
	VBROADCASTSD X0, Z0

float64PlusScalarBlockLoop:
	CMPQ    DX, $0x00000060
	JL      float64PlusScalarTailLoop
	VMOVUPD (AX), Z1
	VMOVUPD 64(AX), Z2
	VMOVUPD 128(AX), Z3
	VMOVUPD 192(AX), Z4
	VMOVUPD 256(AX), Z5
	VMOVUPD 320(AX), Z6
	VMOVUPD 384(AX), Z7
	VMOVUPD 448(AX), Z8
	VMOVUPD 512(AX), Z9
	VMOVUPD 576(AX), Z10
	VMOVUPD 640(AX), Z11
	VMOVUPD 704(AX), Z12
	VADDPD  Z0, Z1, Z1
	VADDPD  Z0, Z2, Z2
	VADDPD  Z0, Z3, Z3
	VADDPD  Z0, Z4, Z4
	VADDPD  Z0, Z5, Z5
	VADDPD  Z0, Z6, Z6
	VADDPD  Z0, Z7, Z7
	VADDPD  Z0, Z8, Z8
	VADDPD  Z0, Z9, Z9
	VADDPD  Z0, Z10, Z10
	VADDPD  Z0, Z11, Z11
	VADDPD  Z0, Z12, Z12
	VMOVUPD Z1, (CX)
	VMOVUPD Z2, 64(CX)
	VMOVUPD Z3, 128(CX)
	VMOVUPD Z4, 192(CX)
	VMOVUPD Z5, 256(CX)
	VMOVUPD Z6, 320(CX)
	VMOVUPD Z7, 384(CX)
	VMOVUPD Z8, 448(CX)
	VMOVUPD Z9, 512(CX)
	VMOVUPD Z10, 576(CX)
	VMOVUPD Z11, 640(CX)
	VMOVUPD Z12, 704(CX)
	ADDQ    $0x00000300, AX
	ADDQ    $0x00000300, CX
	SUBQ    $0x00000060, DX
	JMP     float64PlusScalarBlockLoop

float64PlusScalarTailLoop:
	CMPQ    DX, $0x00000008
	JL      float64PlusScalarDone
	VMOVUPD (AX), Z1
	VADDPD  Z0, Z1, Z1
	VMOVUPD Z1, (CX)
	ADDQ    $0x00000040, AX
	ADDQ    $0x00000040, CX
	SUBQ    $0x00000008, DX
	JMP     float64PlusScalarTailLoop

float64PlusScalarDone:
	RET
