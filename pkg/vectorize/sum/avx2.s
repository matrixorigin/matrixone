// Code generated by command: go run avx2_gen.go -out avx2.s -stubs avx2_stubs.go. DO NOT EDIT.
// +build amd64

#include "textflag.h"

// func int8SumAvx2Asm(x []int8) int64
// Requires: AVX, AVX2, SSE2
TEXT ·int8SumAvx2Asm(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), AX
	MOVQ  x_len+8(FP), CX
	XORL  DX, DX
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	CMPQ  CX, $0x00000070
	JL    int8SumTail
	MOVL  $0x00000007, BX
	VPXOR Y3, Y3, Y3
	VPXOR Y5, Y5, Y5
	VPXOR Y7, Y7, Y7
	VPXOR Y9, Y9, Y9
	VPXOR Y11, Y11, Y11
	VPXOR Y13, Y13, Y13
	VPXOR Y15, Y15, Y15

int8SumBlockLoop:
	VPMOVSXBW    (AX), Y2
	VPMOVSXBW    16(AX), Y4
	VPMOVSXBW    32(AX), Y6
	VPMOVSXBW    48(AX), Y8
	VPMOVSXBW    64(AX), Y10
	VPMOVSXBW    80(AX), Y12
	VPMOVSXBW    96(AX), Y14
	VPADDW       Y2, Y3, Y3
	VPADDW       Y4, Y5, Y5
	VPADDW       Y6, Y7, Y7
	VPADDW       Y8, Y9, Y9
	VPADDW       Y10, Y11, Y11
	VPADDW       Y12, Y13, Y13
	VPADDW       Y14, Y15, Y15
	ADDQ         $0x00000070, AX
	SUBQ         $0x00000070, CX
	INCL         DX
	CMPQ         CX, $0x00000070
	JL           int8SumTail
	CMPL         DX, $0x000000ff
	JLE          int8SumBlockLoop
	VEXTRACTI128 $0x01, Y3, X2
	VPMOVSXWD    X3, Y3
	VPMOVSXWD    X2, Y2
	VPADDD       Y2, Y3, Y3
	VEXTRACTI128 $0x01, Y5, X4
	VPMOVSXWD    X5, Y5
	VPMOVSXWD    X4, Y4
	VPADDD       Y4, Y5, Y5
	VEXTRACTI128 $0x01, Y7, X6
	VPMOVSXWD    X7, Y7
	VPMOVSXWD    X6, Y6
	VPADDD       Y6, Y7, Y7
	VEXTRACTI128 $0x01, Y9, X8
	VPMOVSXWD    X9, Y9
	VPMOVSXWD    X8, Y8
	VPADDD       Y8, Y9, Y9
	VEXTRACTI128 $0x01, Y11, X10
	VPMOVSXWD    X11, Y11
	VPMOVSXWD    X10, Y10
	VPADDD       Y10, Y11, Y11
	VEXTRACTI128 $0x01, Y13, X12
	VPMOVSXWD    X13, Y13
	VPMOVSXWD    X12, Y12
	VPADDD       Y12, Y13, Y13
	VEXTRACTI128 $0x01, Y15, X14
	VPMOVSXWD    X15, Y15
	VPMOVSXWD    X14, Y14
	VPADDD       Y14, Y15, Y15
	VPADDD       Y5, Y3, Y3
	VPXOR        Y5, Y5, Y5
	VPADDD       Y9, Y7, Y7
	VPXOR        Y9, Y9, Y9
	VPADDD       Y13, Y11, Y11
	VPXOR        Y13, Y13, Y13
	VPADDD       Y7, Y3, Y3
	VPXOR        Y7, Y7, Y7
	VPADDD       Y15, Y11, Y11
	VPXOR        Y15, Y15, Y15
	VPADDD       Y11, Y0, Y0
	VPXOR        Y11, Y11, Y11
	VPADDD       Y3, Y0, Y0
	VPXOR        Y3, Y3, Y3
	XORL         DX, DX
	ADDL         $0x07, BX
	CMPL         BX, $0x00010000
	JLE          int8SumBlockLoop
	XORL         BX, BX
	VEXTRACTI128 $0x01, Y0, X2
	VPMOVSXDQ    X0, Y0
	VPADDQ       Y0, Y1, Y1
	VPMOVSXDQ    X2, Y2
	VPADDQ       Y2, Y1, Y1
	VPXOR        Y0, Y0, Y0
	JMP          int8SumBlockLoop

int8SumTail:
	TESTL        DX, DX
	JZ           int8SumTailLoop
	VEXTRACTI128 $0x01, Y3, X2
	VPMOVSXWD    X3, Y3
	VPMOVSXWD    X2, Y2
	VPADDD       Y2, Y3, Y3
	VEXTRACTI128 $0x01, Y5, X4
	VPMOVSXWD    X5, Y5
	VPMOVSXWD    X4, Y4
	VPADDD       Y4, Y5, Y5
	VEXTRACTI128 $0x01, Y7, X6
	VPMOVSXWD    X7, Y7
	VPMOVSXWD    X6, Y6
	VPADDD       Y6, Y7, Y7
	VEXTRACTI128 $0x01, Y9, X8
	VPMOVSXWD    X9, Y9
	VPMOVSXWD    X8, Y8
	VPADDD       Y8, Y9, Y9
	VEXTRACTI128 $0x01, Y11, X10
	VPMOVSXWD    X11, Y11
	VPMOVSXWD    X10, Y10
	VPADDD       Y10, Y11, Y11
	VEXTRACTI128 $0x01, Y13, X12
	VPMOVSXWD    X13, Y13
	VPMOVSXWD    X12, Y12
	VPADDD       Y12, Y13, Y13
	VEXTRACTI128 $0x01, Y15, X14
	VPMOVSXWD    X15, Y15
	VPMOVSXWD    X14, Y14
	VPADDD       Y14, Y15, Y15
	VPADDD       Y5, Y3, Y3
	VPADDD       Y9, Y7, Y7
	VPADDD       Y13, Y11, Y11
	VPADDD       Y7, Y3, Y3
	VPADDD       Y15, Y11, Y11
	VPADDD       Y11, Y0, Y0
	VPADDD       Y3, Y0, Y0
	VEXTRACTI128 $0x01, Y0, X2
	VPMOVSXDQ    X0, Y0
	VPADDQ       Y0, Y1, Y1
	VPMOVSXDQ    X2, Y2
	VPADDQ       Y2, Y1, Y1

int8SumTailLoop:
	CMPQ      CX, $0x00000004
	JL        int8SumDone
	VPMOVSXBQ (AX), Y0
	VPADDQ    Y0, Y1, Y1
	ADDQ      $0x00000004, AX
	SUBQ      $0x00000004, CX
	JMP       int8SumTailLoop

int8SumDone:
	VEXTRACTI128 $0x01, Y1, X0
	VPADDQ       X0, X1, X1
	VPSHUFD      $0x4e, X1, X0
	VPADDQ       X0, X1, X1
	MOVQ         X1, ret+24(FP)
	RET

// func int16SumAvx2Asm(x []int16) int64
// Requires: AVX, AVX2, SSE2
TEXT ·int16SumAvx2Asm(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), AX
	MOVQ  x_len+8(FP), CX
	XORL  DX, DX
	VPXOR Y0, Y0, Y0
	CMPQ  CX, $0x00000038
	JL    int16SumTail
	VPXOR Y2, Y2, Y2
	VPXOR Y4, Y4, Y4
	VPXOR Y6, Y6, Y6
	VPXOR Y8, Y8, Y8
	VPXOR Y10, Y10, Y10
	VPXOR Y12, Y12, Y12
	VPXOR Y14, Y14, Y14

int16SumBlockLoop:
	VPMOVSXWD    (AX), Y1
	VPMOVSXWD    16(AX), Y3
	VPMOVSXWD    32(AX), Y5
	VPMOVSXWD    48(AX), Y7
	VPMOVSXWD    64(AX), Y9
	VPMOVSXWD    80(AX), Y11
	VPMOVSXWD    96(AX), Y13
	VPADDD       Y1, Y2, Y2
	VPADDD       Y3, Y4, Y4
	VPADDD       Y5, Y6, Y6
	VPADDD       Y7, Y8, Y8
	VPADDD       Y9, Y10, Y10
	VPADDD       Y11, Y12, Y12
	VPADDD       Y13, Y14, Y14
	ADDQ         $0x00000070, AX
	SUBQ         $0x00000038, CX
	INCL         DX
	CMPQ         CX, $0x00000038
	JL           int16SumTail
	CMPL         DX, $0x0000ffff
	JLE          int16SumBlockLoop
	VEXTRACTI128 $0x01, Y2, X1
	VPMOVSXDQ    X2, Y2
	VPMOVSXDQ    X1, Y1
	VPADDQ       Y1, Y2, Y2
	VEXTRACTI128 $0x01, Y4, X3
	VPMOVSXDQ    X4, Y4
	VPMOVSXDQ    X3, Y3
	VPADDQ       Y3, Y4, Y4
	VEXTRACTI128 $0x01, Y6, X5
	VPMOVSXDQ    X6, Y6
	VPMOVSXDQ    X5, Y5
	VPADDQ       Y5, Y6, Y6
	VEXTRACTI128 $0x01, Y8, X7
	VPMOVSXDQ    X8, Y8
	VPMOVSXDQ    X7, Y7
	VPADDQ       Y7, Y8, Y8
	VEXTRACTI128 $0x01, Y10, X9
	VPMOVSXDQ    X10, Y10
	VPMOVSXDQ    X9, Y9
	VPADDQ       Y9, Y10, Y10
	VEXTRACTI128 $0x01, Y12, X11
	VPMOVSXDQ    X12, Y12
	VPMOVSXDQ    X11, Y11
	VPADDQ       Y11, Y12, Y12
	VEXTRACTI128 $0x01, Y14, X13
	VPMOVSXDQ    X14, Y14
	VPMOVSXDQ    X13, Y13
	VPADDQ       Y13, Y14, Y14
	VPADDQ       Y4, Y2, Y2
	VPXOR        Y4, Y4, Y4
	VPADDQ       Y8, Y6, Y6
	VPXOR        Y8, Y8, Y8
	VPADDQ       Y12, Y10, Y10
	VPXOR        Y12, Y12, Y12
	VPADDQ       Y6, Y2, Y2
	VPXOR        Y6, Y6, Y6
	VPADDQ       Y14, Y10, Y10
	VPXOR        Y14, Y14, Y14
	VPADDQ       Y10, Y0, Y0
	VPXOR        Y10, Y10, Y10
	VPADDD       Y2, Y0, Y0
	VPXOR        Y2, Y2, Y2
	XORL         DX, DX
	JMP          int16SumBlockLoop

int16SumTail:
	TESTL        DX, DX
	JZ           int16SumTailLoop
	VEXTRACTI128 $0x01, Y2, X1
	VPMOVSXDQ    X2, Y2
	VPMOVSXDQ    X1, Y1
	VPADDQ       Y1, Y2, Y2
	VEXTRACTI128 $0x01, Y4, X3
	VPMOVSXDQ    X4, Y4
	VPMOVSXDQ    X3, Y3
	VPADDQ       Y3, Y4, Y4
	VEXTRACTI128 $0x01, Y6, X5
	VPMOVSXDQ    X6, Y6
	VPMOVSXDQ    X5, Y5
	VPADDQ       Y5, Y6, Y6
	VEXTRACTI128 $0x01, Y8, X7
	VPMOVSXDQ    X8, Y8
	VPMOVSXDQ    X7, Y7
	VPADDQ       Y7, Y8, Y8
	VEXTRACTI128 $0x01, Y10, X9
	VPMOVSXDQ    X10, Y10
	VPMOVSXDQ    X9, Y9
	VPADDQ       Y9, Y10, Y10
	VEXTRACTI128 $0x01, Y12, X11
	VPMOVSXDQ    X12, Y12
	VPMOVSXDQ    X11, Y11
	VPADDQ       Y11, Y12, Y12
	VEXTRACTI128 $0x01, Y14, X13
	VPMOVSXDQ    X14, Y14
	VPMOVSXDQ    X13, Y13
	VPADDQ       Y13, Y14, Y14
	VPADDQ       Y4, Y2, Y2
	VPADDQ       Y8, Y6, Y6
	VPADDQ       Y12, Y10, Y10
	VPADDQ       Y6, Y2, Y2
	VPADDQ       Y14, Y10, Y10
	VPADDQ       Y10, Y0, Y0
	VPADDQ       Y2, Y0, Y0

int16SumTailLoop:
	CMPQ      CX, $0x00000004
	JL        int16SumDone
	VPMOVSXWQ (AX), Y2
	VPADDQ    Y2, Y0, Y0
	ADDQ      $0x00000008, AX
	SUBQ      $0x00000004, CX
	JMP       int16SumTailLoop

int16SumDone:
	VEXTRACTI128 $0x01, Y0, X1
	VPADDQ       X1, X0, X0
	VPSHUFD      $0x4e, X0, X1
	VPADDQ       X1, X0, X0
	MOVQ         X0, ret+24(FP)
	RET

// func int32SumAvx2Asm(x []int32) int64
// Requires: AVX, AVX2, SSE2
TEXT ·int32SumAvx2Asm(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), AX
	MOVQ  x_len+8(FP), CX
	MOVQ  CX, DX
	VPXOR Y1, Y1, Y1
	CMPQ  CX, $0x00000020
	JL    int32SumTail
	VPXOR Y3, Y3, Y3
	VPXOR Y5, Y5, Y5
	VPXOR Y7, Y7, Y7
	VPXOR Y9, Y9, Y9
	VPXOR Y11, Y11, Y11
	VPXOR Y13, Y13, Y13
	VPXOR Y15, Y15, Y15

int32SumBlockLoop:
	VPMOVSXDQ (AX), Y0
	VPMOVSXDQ 16(AX), Y2
	VPMOVSXDQ 32(AX), Y4
	VPMOVSXDQ 48(AX), Y6
	VPMOVSXDQ 64(AX), Y8
	VPMOVSXDQ 80(AX), Y10
	VPMOVSXDQ 96(AX), Y12
	VPMOVSXDQ 112(AX), Y14
	VPADDQ    Y0, Y1, Y1
	VPADDQ    Y2, Y3, Y3
	VPADDQ    Y4, Y5, Y5
	VPADDQ    Y6, Y7, Y7
	VPADDQ    Y8, Y9, Y9
	VPADDQ    Y10, Y11, Y11
	VPADDQ    Y12, Y13, Y13
	VPADDQ    Y14, Y15, Y15
	ADDQ      $0x00000080, AX
	SUBQ      $0x00000020, CX
	CMPQ      CX, $0x00000020
	JL        int32SumTail
	JMP       int32SumBlockLoop

int32SumTail:
	CMPQ   CX, DX
	JZ     int32SumTailLoop
	VPADDQ Y3, Y1, Y1
	VPADDQ Y7, Y5, Y5
	VPADDQ Y11, Y9, Y9
	VPADDQ Y15, Y13, Y13
	VPADDQ Y5, Y1, Y1
	VPADDQ Y13, Y9, Y9
	VPADDQ Y9, Y1, Y1

int32SumTailLoop:
	CMPQ      CX, $0x00000004
	JL        int32SumDone
	VPMOVSXDQ (AX), Y3
	VPADDQ    Y3, Y1, Y1
	ADDQ      $0x00000010, AX
	SUBQ      $0x00000004, CX
	JMP       int32SumTailLoop

int32SumDone:
	VEXTRACTI128 $0x01, Y1, X0
	VPADDQ       X0, X1, X1
	VPSHUFD      $0x4e, X1, X0
	VPADDQ       X0, X1, X1
	MOVQ         X1, ret+24(FP)
	RET

// func int64SumAvx2Asm(x []int64) int64
// Requires: AVX, AVX2, SSE2
TEXT ·int64SumAvx2Asm(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), AX
	MOVQ  x_len+8(FP), CX
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8
	VPXOR Y9, Y9, Y9
	VPXOR Y10, Y10, Y10
	VPXOR Y11, Y11, Y11
	VPXOR Y12, Y12, Y12
	VPXOR Y13, Y13, Y13
	VPXOR Y14, Y14, Y14
	VPXOR Y15, Y15, Y15

int64SumBlockLoop:
	CMPQ   CX, $0x00000040
	JL     int64SumTailLoop
	VPADDQ (AX), Y0, Y0
	VPADDQ 32(AX), Y1, Y1
	VPADDQ 64(AX), Y2, Y2
	VPADDQ 96(AX), Y3, Y3
	VPADDQ 128(AX), Y4, Y4
	VPADDQ 160(AX), Y5, Y5
	VPADDQ 192(AX), Y6, Y6
	VPADDQ 224(AX), Y7, Y7
	VPADDQ 256(AX), Y8, Y8
	VPADDQ 288(AX), Y9, Y9
	VPADDQ 320(AX), Y10, Y10
	VPADDQ 352(AX), Y11, Y11
	VPADDQ 384(AX), Y12, Y12
	VPADDQ 416(AX), Y13, Y13
	VPADDQ 448(AX), Y14, Y14
	VPADDQ 480(AX), Y15, Y15
	ADDQ   $0x00000200, AX
	SUBQ   $0x00000040, CX
	JMP    int64SumBlockLoop

int64SumTailLoop:
	CMPQ   CX, $0x00000004
	JL     int64SumDone
	VPADDQ (AX), Y0, Y0
	ADDQ   $0x00000020, AX
	SUBQ   $0x00000004, CX
	JMP    int64SumTailLoop

int64SumDone:
	VPADDQ       Y1, Y0, Y0
	VPADDQ       Y3, Y2, Y2
	VPADDQ       Y5, Y4, Y4
	VPADDQ       Y7, Y6, Y6
	VPADDQ       Y9, Y8, Y8
	VPADDQ       Y11, Y10, Y10
	VPADDQ       Y13, Y12, Y12
	VPADDQ       Y15, Y14, Y14
	VPADDQ       Y2, Y0, Y0
	VPADDQ       Y6, Y4, Y4
	VPADDQ       Y10, Y8, Y8
	VPADDQ       Y14, Y12, Y12
	VPADDQ       Y4, Y0, Y0
	VPADDQ       Y12, Y8, Y8
	VPADDQ       Y8, Y0, Y0
	VEXTRACTI128 $0x01, Y0, X1
	VPADDQ       X1, X0, X0
	VPSHUFD      $0x4e, X0, X1
	VPADDQ       X1, X0, X0
	MOVQ         X0, ret+24(FP)
	RET

// func uint8SumAvx2Asm(x []uint8) uint64
// Requires: AVX, AVX2, SSE2
TEXT ·uint8SumAvx2Asm(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), AX
	MOVQ  x_len+8(FP), CX
	XORL  DX, DX
	MOVL  $0x00000007, BX
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	CMPQ  CX, $0x00000070
	JL    uint8SumTail
	VPXOR Y3, Y3, Y3
	VPXOR Y5, Y5, Y5
	VPXOR Y7, Y7, Y7
	VPXOR Y9, Y9, Y9
	VPXOR Y11, Y11, Y11
	VPXOR Y13, Y13, Y13
	VPXOR Y15, Y15, Y15

uint8SumBlockLoop:
	VPMOVZXBW    (AX), Y2
	VPMOVZXBW    16(AX), Y4
	VPMOVZXBW    32(AX), Y6
	VPMOVZXBW    48(AX), Y8
	VPMOVZXBW    64(AX), Y10
	VPMOVZXBW    80(AX), Y12
	VPMOVZXBW    96(AX), Y14
	VPADDW       Y2, Y3, Y3
	VPADDW       Y4, Y5, Y5
	VPADDW       Y6, Y7, Y7
	VPADDW       Y8, Y9, Y9
	VPADDW       Y10, Y11, Y11
	VPADDW       Y12, Y13, Y13
	VPADDW       Y14, Y15, Y15
	ADDQ         $0x00000070, AX
	SUBQ         $0x00000070, CX
	INCL         DX
	CMPQ         CX, $0x00000070
	JL           uint8SumTail
	CMPL         DX, $0x000000ff
	JNA          uint8SumBlockLoop
	VEXTRACTI128 $0x01, Y3, X2
	VPMOVZXWD    X3, Y3
	VPMOVZXWD    X2, Y2
	VPADDD       Y2, Y3, Y3
	VEXTRACTI128 $0x01, Y5, X4
	VPMOVZXWD    X5, Y5
	VPMOVZXWD    X4, Y4
	VPADDD       Y4, Y5, Y5
	VEXTRACTI128 $0x01, Y7, X6
	VPMOVZXWD    X7, Y7
	VPMOVZXWD    X6, Y6
	VPADDD       Y6, Y7, Y7
	VEXTRACTI128 $0x01, Y9, X8
	VPMOVZXWD    X9, Y9
	VPMOVZXWD    X8, Y8
	VPADDD       Y8, Y9, Y9
	VEXTRACTI128 $0x01, Y11, X10
	VPMOVZXWD    X11, Y11
	VPMOVZXWD    X10, Y10
	VPADDD       Y10, Y11, Y11
	VEXTRACTI128 $0x01, Y13, X12
	VPMOVZXWD    X13, Y13
	VPMOVZXWD    X12, Y12
	VPADDD       Y12, Y13, Y13
	VEXTRACTI128 $0x01, Y15, X14
	VPMOVZXWD    X15, Y15
	VPMOVZXWD    X14, Y14
	VPADDD       Y14, Y15, Y15
	VPADDD       Y5, Y3, Y3
	VPXOR        Y5, Y5, Y5
	VPADDD       Y9, Y7, Y7
	VPXOR        Y9, Y9, Y9
	VPADDD       Y13, Y11, Y11
	VPXOR        Y13, Y13, Y13
	VPADDD       Y7, Y3, Y3
	VPXOR        Y7, Y7, Y7
	VPADDD       Y15, Y11, Y11
	VPXOR        Y15, Y15, Y15
	VPADDD       Y11, Y0, Y0
	VPXOR        Y11, Y11, Y11
	VPADDD       Y3, Y0, Y0
	VPXOR        Y3, Y3, Y3
	XORL         DX, DX
	ADDL         $0x07, BX
	CMPL         BX, $0x00010000
	JNA          uint8SumBlockLoop
	XORL         BX, BX
	VEXTRACTI128 $0x01, Y0, X2
	VPMOVZXDQ    X0, Y0
	VPADDQ       Y0, Y1, Y1
	VPMOVZXDQ    X2, Y2
	VPADDQ       Y2, Y1, Y1
	VPXOR        Y0, Y0, Y0
	JMP          uint8SumBlockLoop

uint8SumTail:
	TESTL        DX, DX
	JZ           uint8SumTailLoop
	VEXTRACTI128 $0x01, Y3, X2
	VPMOVZXWD    X3, Y3
	VPMOVZXWD    X2, Y2
	VPADDD       Y2, Y3, Y3
	VEXTRACTI128 $0x01, Y5, X4
	VPMOVZXWD    X5, Y5
	VPMOVZXWD    X4, Y4
	VPADDD       Y4, Y5, Y5
	VEXTRACTI128 $0x01, Y7, X6
	VPMOVZXWD    X7, Y7
	VPMOVZXWD    X6, Y6
	VPADDD       Y6, Y7, Y7
	VEXTRACTI128 $0x01, Y9, X8
	VPMOVZXWD    X9, Y9
	VPMOVZXWD    X8, Y8
	VPADDD       Y8, Y9, Y9
	VEXTRACTI128 $0x01, Y11, X10
	VPMOVZXWD    X11, Y11
	VPMOVZXWD    X10, Y10
	VPADDD       Y10, Y11, Y11
	VEXTRACTI128 $0x01, Y13, X12
	VPMOVZXWD    X13, Y13
	VPMOVZXWD    X12, Y12
	VPADDD       Y12, Y13, Y13
	VEXTRACTI128 $0x01, Y15, X14
	VPMOVZXWD    X15, Y15
	VPMOVZXWD    X14, Y14
	VPADDD       Y14, Y15, Y15
	VPADDD       Y5, Y3, Y3
	VPADDD       Y9, Y7, Y7
	VPADDD       Y13, Y11, Y11
	VPADDD       Y7, Y3, Y3
	VPADDD       Y15, Y11, Y11
	VPADDD       Y11, Y0, Y0
	VPADDD       Y3, Y0, Y0
	VEXTRACTI128 $0x01, Y0, X2
	VPMOVZXDQ    X0, Y0
	VPADDQ       Y0, Y1, Y1
	VPMOVZXDQ    X2, Y2
	VPADDQ       Y2, Y1, Y1

uint8SumTailLoop:
	CMPQ      CX, $0x00000004
	JL        uint8SumDone
	VPMOVZXBQ (AX), Y0
	VPADDQ    Y0, Y1, Y1
	ADDQ      $0x00000004, AX
	SUBQ      $0x00000004, CX
	JMP       uint8SumTailLoop

uint8SumDone:
	VEXTRACTI128 $0x01, Y1, X0
	VPADDQ       X0, X1, X1
	VPSHUFD      $0x4e, X1, X0
	VPADDQ       X0, X1, X1
	MOVQ         X1, ret+24(FP)
	RET

// func uint16SumAvx2Asm(x []uint16) uint64
// Requires: AVX, AVX2, SSE2
TEXT ·uint16SumAvx2Asm(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), AX
	MOVQ  x_len+8(FP), CX
	XORL  DX, DX
	VPXOR Y0, Y0, Y0
	CMPQ  CX, $0x00000038
	JL    uint16SumTail
	VPXOR Y2, Y2, Y2
	VPXOR Y4, Y4, Y4
	VPXOR Y6, Y6, Y6
	VPXOR Y8, Y8, Y8
	VPXOR Y10, Y10, Y10
	VPXOR Y12, Y12, Y12
	VPXOR Y14, Y14, Y14

uint16SumBlockLoop:
	VPMOVZXWD    (AX), Y1
	VPMOVZXWD    16(AX), Y3
	VPMOVZXWD    32(AX), Y5
	VPMOVZXWD    48(AX), Y7
	VPMOVZXWD    64(AX), Y9
	VPMOVZXWD    80(AX), Y11
	VPMOVZXWD    96(AX), Y13
	VPADDD       Y1, Y2, Y2
	VPADDD       Y3, Y4, Y4
	VPADDD       Y5, Y6, Y6
	VPADDD       Y7, Y8, Y8
	VPADDD       Y9, Y10, Y10
	VPADDD       Y11, Y12, Y12
	VPADDD       Y13, Y14, Y14
	ADDQ         $0x00000070, AX
	SUBQ         $0x00000038, CX
	INCL         DX
	CMPQ         CX, $0x00000038
	JL           uint16SumTail
	CMPL         DX, $0x0000ffff
	JNA          uint16SumBlockLoop
	VEXTRACTI128 $0x01, Y2, X1
	VPMOVZXDQ    X2, Y2
	VPMOVZXDQ    X1, Y1
	VPADDQ       Y1, Y2, Y2
	VEXTRACTI128 $0x01, Y4, X3
	VPMOVZXDQ    X4, Y4
	VPMOVZXDQ    X3, Y3
	VPADDQ       Y3, Y4, Y4
	VEXTRACTI128 $0x01, Y6, X5
	VPMOVZXDQ    X6, Y6
	VPMOVZXDQ    X5, Y5
	VPADDQ       Y5, Y6, Y6
	VEXTRACTI128 $0x01, Y8, X7
	VPMOVZXDQ    X8, Y8
	VPMOVZXDQ    X7, Y7
	VPADDQ       Y7, Y8, Y8
	VEXTRACTI128 $0x01, Y10, X9
	VPMOVZXDQ    X10, Y10
	VPMOVZXDQ    X9, Y9
	VPADDQ       Y9, Y10, Y10
	VEXTRACTI128 $0x01, Y12, X11
	VPMOVZXDQ    X12, Y12
	VPMOVZXDQ    X11, Y11
	VPADDQ       Y11, Y12, Y12
	VEXTRACTI128 $0x01, Y14, X13
	VPMOVZXDQ    X14, Y14
	VPMOVZXDQ    X13, Y13
	VPADDQ       Y13, Y14, Y14
	VPADDQ       Y4, Y2, Y2
	VPXOR        Y4, Y4, Y4
	VPADDQ       Y8, Y6, Y6
	VPXOR        Y8, Y8, Y8
	VPADDQ       Y12, Y10, Y10
	VPXOR        Y12, Y12, Y12
	VPADDQ       Y6, Y2, Y2
	VPXOR        Y6, Y6, Y6
	VPADDQ       Y14, Y10, Y10
	VPXOR        Y14, Y14, Y14
	VPADDQ       Y10, Y0, Y0
	VPXOR        Y10, Y10, Y10
	VPADDD       Y2, Y0, Y0
	VPXOR        Y2, Y2, Y2
	XORL         DX, DX
	JMP          uint16SumBlockLoop

uint16SumTail:
	TESTL        DX, DX
	JZ           uint16SumTailLoop
	VEXTRACTI128 $0x01, Y2, X1
	VPMOVZXDQ    X2, Y2
	VPMOVZXDQ    X1, Y1
	VPADDQ       Y1, Y2, Y2
	VEXTRACTI128 $0x01, Y4, X3
	VPMOVZXDQ    X4, Y4
	VPMOVZXDQ    X3, Y3
	VPADDQ       Y3, Y4, Y4
	VEXTRACTI128 $0x01, Y6, X5
	VPMOVZXDQ    X6, Y6
	VPMOVZXDQ    X5, Y5
	VPADDQ       Y5, Y6, Y6
	VEXTRACTI128 $0x01, Y8, X7
	VPMOVZXDQ    X8, Y8
	VPMOVZXDQ    X7, Y7
	VPADDQ       Y7, Y8, Y8
	VEXTRACTI128 $0x01, Y10, X9
	VPMOVZXDQ    X10, Y10
	VPMOVZXDQ    X9, Y9
	VPADDQ       Y9, Y10, Y10
	VEXTRACTI128 $0x01, Y12, X11
	VPMOVZXDQ    X12, Y12
	VPMOVZXDQ    X11, Y11
	VPADDQ       Y11, Y12, Y12
	VEXTRACTI128 $0x01, Y14, X13
	VPMOVZXDQ    X14, Y14
	VPMOVZXDQ    X13, Y13
	VPADDQ       Y13, Y14, Y14
	VPADDQ       Y4, Y2, Y2
	VPADDQ       Y8, Y6, Y6
	VPADDQ       Y12, Y10, Y10
	VPADDQ       Y6, Y2, Y2
	VPADDQ       Y14, Y10, Y10
	VPADDQ       Y10, Y0, Y0
	VPADDQ       Y2, Y0, Y0

uint16SumTailLoop:
	CMPQ      CX, $0x00000004
	JL        uint16SumDone
	VPMOVZXWQ (AX), Y2
	VPADDQ    Y2, Y0, Y0
	ADDQ      $0x00000008, AX
	SUBQ      $0x00000004, CX
	JMP       uint16SumTailLoop

uint16SumDone:
	VEXTRACTI128 $0x01, Y0, X1
	VPADDQ       X1, X0, X0
	VPSHUFD      $0x4e, X0, X1
	VPADDQ       X1, X0, X0
	MOVQ         X0, ret+24(FP)
	RET

// func uint32SumAvx2Asm(x []uint32) uint64
// Requires: AVX, AVX2, SSE2
TEXT ·uint32SumAvx2Asm(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), AX
	MOVQ  x_len+8(FP), CX
	MOVQ  CX, DX
	VPXOR Y1, Y1, Y1
	CMPQ  CX, $0x00000020
	JL    uint32SumTail
	VPXOR Y3, Y3, Y3
	VPXOR Y5, Y5, Y5
	VPXOR Y7, Y7, Y7
	VPXOR Y9, Y9, Y9
	VPXOR Y11, Y11, Y11
	VPXOR Y13, Y13, Y13
	VPXOR Y15, Y15, Y15

uint32SumBlockLoop:
	VPMOVZXDQ (AX), Y0
	VPMOVZXDQ 16(AX), Y2
	VPMOVZXDQ 32(AX), Y4
	VPMOVZXDQ 48(AX), Y6
	VPMOVZXDQ 64(AX), Y8
	VPMOVZXDQ 80(AX), Y10
	VPMOVZXDQ 96(AX), Y12
	VPMOVZXDQ 112(AX), Y14
	VPADDQ    Y0, Y1, Y1
	VPADDQ    Y2, Y3, Y3
	VPADDQ    Y4, Y5, Y5
	VPADDQ    Y6, Y7, Y7
	VPADDQ    Y8, Y9, Y9
	VPADDQ    Y10, Y11, Y11
	VPADDQ    Y12, Y13, Y13
	VPADDQ    Y14, Y15, Y15
	ADDQ      $0x00000080, AX
	SUBQ      $0x00000020, CX
	CMPQ      CX, $0x00000020
	JL        uint32SumTail
	JMP       uint32SumBlockLoop

uint32SumTail:
	CMPQ   CX, DX
	JZ     uint32SumTailLoop
	VPADDQ Y3, Y1, Y1
	VPADDQ Y7, Y5, Y5
	VPADDQ Y11, Y9, Y9
	VPADDQ Y15, Y13, Y13
	VPADDQ Y5, Y1, Y1
	VPADDQ Y13, Y9, Y9
	VPADDQ Y9, Y1, Y1

uint32SumTailLoop:
	CMPQ      CX, $0x00000004
	JL        uint32SumDone
	VPMOVZXDQ (AX), Y3
	VPADDQ    Y3, Y1, Y1
	ADDQ      $0x00000010, AX
	SUBQ      $0x00000004, CX
	JMP       uint32SumTailLoop

uint32SumDone:
	VEXTRACTI128 $0x01, Y1, X0
	VPADDQ       X0, X1, X1
	VPSHUFD      $0x4e, X1, X0
	VPADDQ       X0, X1, X1
	MOVQ         X1, ret+24(FP)
	RET

// func uint64SumAvx2Asm(x []uint64) uint64
// Requires: AVX, AVX2, SSE2
TEXT ·uint64SumAvx2Asm(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), AX
	MOVQ  x_len+8(FP), CX
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8
	VPXOR Y9, Y9, Y9
	VPXOR Y10, Y10, Y10
	VPXOR Y11, Y11, Y11
	VPXOR Y12, Y12, Y12
	VPXOR Y13, Y13, Y13
	VPXOR Y14, Y14, Y14
	VPXOR Y15, Y15, Y15

uint64SumBlockLoop:
	CMPQ   CX, $0x00000040
	JL     uint64SumTailLoop
	VPADDQ (AX), Y0, Y0
	VPADDQ 32(AX), Y1, Y1
	VPADDQ 64(AX), Y2, Y2
	VPADDQ 96(AX), Y3, Y3
	VPADDQ 128(AX), Y4, Y4
	VPADDQ 160(AX), Y5, Y5
	VPADDQ 192(AX), Y6, Y6
	VPADDQ 224(AX), Y7, Y7
	VPADDQ 256(AX), Y8, Y8
	VPADDQ 288(AX), Y9, Y9
	VPADDQ 320(AX), Y10, Y10
	VPADDQ 352(AX), Y11, Y11
	VPADDQ 384(AX), Y12, Y12
	VPADDQ 416(AX), Y13, Y13
	VPADDQ 448(AX), Y14, Y14
	VPADDQ 480(AX), Y15, Y15
	ADDQ   $0x00000200, AX
	SUBQ   $0x00000040, CX
	JMP    uint64SumBlockLoop

uint64SumTailLoop:
	CMPQ   CX, $0x00000004
	JL     uint64SumDone
	VPADDQ (AX), Y0, Y0
	ADDQ   $0x00000020, AX
	SUBQ   $0x00000004, CX
	JMP    uint64SumTailLoop

uint64SumDone:
	VPADDQ       Y1, Y0, Y0
	VPADDQ       Y3, Y2, Y2
	VPADDQ       Y5, Y4, Y4
	VPADDQ       Y7, Y6, Y6
	VPADDQ       Y9, Y8, Y8
	VPADDQ       Y11, Y10, Y10
	VPADDQ       Y13, Y12, Y12
	VPADDQ       Y15, Y14, Y14
	VPADDQ       Y2, Y0, Y0
	VPADDQ       Y6, Y4, Y4
	VPADDQ       Y10, Y8, Y8
	VPADDQ       Y14, Y12, Y12
	VPADDQ       Y4, Y0, Y0
	VPADDQ       Y12, Y8, Y8
	VPADDQ       Y8, Y0, Y0
	VEXTRACTI128 $0x01, Y0, X1
	VPADDQ       X1, X0, X0
	VPSHUFD      $0x4e, X0, X1
	VPADDQ       X1, X0, X0
	MOVQ         X0, ret+24(FP)
	RET

// func float32SumAvx2Asm(x []float32) float32
// Requires: AVX, AVX2, SSE
TEXT ·float32SumAvx2Asm(SB), NOSPLIT, $0-28
	MOVQ  x_base+0(FP), AX
	MOVQ  x_len+8(FP), CX
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8
	VPXOR Y9, Y9, Y9
	VPXOR Y10, Y10, Y10
	VPXOR Y11, Y11, Y11
	VPXOR Y12, Y12, Y12
	VPXOR Y13, Y13, Y13
	VPXOR Y14, Y14, Y14
	VPXOR Y15, Y15, Y15

float32SumBlockLoop:
	CMPQ   CX, $0x00000080
	JL     float32SumTailLoop
	VADDPS (AX), Y0, Y0
	VADDPS 32(AX), Y1, Y1
	VADDPS 64(AX), Y2, Y2
	VADDPS 96(AX), Y3, Y3
	VADDPS 128(AX), Y4, Y4
	VADDPS 160(AX), Y5, Y5
	VADDPS 192(AX), Y6, Y6
	VADDPS 224(AX), Y7, Y7
	VADDPS 256(AX), Y8, Y8
	VADDPS 288(AX), Y9, Y9
	VADDPS 320(AX), Y10, Y10
	VADDPS 352(AX), Y11, Y11
	VADDPS 384(AX), Y12, Y12
	VADDPS 416(AX), Y13, Y13
	VADDPS 448(AX), Y14, Y14
	VADDPS 480(AX), Y15, Y15
	ADDQ   $0x00000200, AX
	SUBQ   $0x00000080, CX
	JMP    float32SumBlockLoop

float32SumTailLoop:
	CMPQ   CX, $0x00000008
	JL     float32SumDone
	VADDPS (AX), Y0, Y0
	ADDQ   $0x00000020, AX
	SUBQ   $0x00000008, CX
	JMP    float32SumTailLoop

float32SumDone:
	VADDPS       Y0, Y1, Y0
	VADDPS       Y2, Y3, Y2
	VADDPS       Y4, Y5, Y4
	VADDPS       Y6, Y7, Y6
	VADDPS       Y8, Y9, Y8
	VADDPS       Y10, Y11, Y10
	VADDPS       Y12, Y13, Y12
	VADDPS       Y14, Y15, Y14
	VADDPS       Y0, Y2, Y0
	VADDPS       Y4, Y6, Y4
	VADDPS       Y8, Y10, Y8
	VADDPS       Y12, Y14, Y12
	VADDPS       Y0, Y4, Y0
	VADDPS       Y8, Y12, Y8
	VADDPS       Y0, Y8, Y0
	VEXTRACTI128 $0x01, Y0, X1
	VADDPS       X1, X0, X0
	CMPQ         CX, $0x04
	JL           float32SumDone1
	VADDPS       (AX), X0, X0

float32SumDone1:
	VHADDPS X0, X0, X0
	VHADDPS X0, X0, X0
	MOVSS   X0, ret+24(FP)
	RET

// func float64SumAvx2Asm(x []float64) float64
// Requires: AVX, AVX2, SSE2
TEXT ·float64SumAvx2Asm(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), AX
	MOVQ  x_len+8(FP), CX
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8
	VPXOR Y9, Y9, Y9
	VPXOR Y10, Y10, Y10
	VPXOR Y11, Y11, Y11
	VPXOR Y12, Y12, Y12
	VPXOR Y13, Y13, Y13
	VPXOR Y14, Y14, Y14
	VPXOR Y15, Y15, Y15

float64SumBlockLoop:
	CMPQ   CX, $0x00000040
	JL     float64SumTailLoop
	VADDPD (AX), Y0, Y0
	VADDPD 32(AX), Y1, Y1
	VADDPD 64(AX), Y2, Y2
	VADDPD 96(AX), Y3, Y3
	VADDPD 128(AX), Y4, Y4
	VADDPD 160(AX), Y5, Y5
	VADDPD 192(AX), Y6, Y6
	VADDPD 224(AX), Y7, Y7
	VADDPD 256(AX), Y8, Y8
	VADDPD 288(AX), Y9, Y9
	VADDPD 320(AX), Y10, Y10
	VADDPD 352(AX), Y11, Y11
	VADDPD 384(AX), Y12, Y12
	VADDPD 416(AX), Y13, Y13
	VADDPD 448(AX), Y14, Y14
	VADDPD 480(AX), Y15, Y15
	ADDQ   $0x00000200, AX
	SUBQ   $0x00000040, CX
	JMP    float64SumBlockLoop

float64SumTailLoop:
	CMPQ   CX, $0x00000004
	JL     float64SumDone
	VADDPD (AX), Y0, Y0
	ADDQ   $0x00000020, AX
	SUBQ   $0x00000004, CX
	JMP    float64SumTailLoop

float64SumDone:
	VADDPD       Y0, Y1, Y0
	VADDPD       Y2, Y3, Y2
	VADDPD       Y4, Y5, Y4
	VADDPD       Y6, Y7, Y6
	VADDPD       Y8, Y9, Y8
	VADDPD       Y10, Y11, Y10
	VADDPD       Y12, Y13, Y12
	VADDPD       Y14, Y15, Y14
	VADDPD       Y0, Y2, Y0
	VADDPD       Y4, Y6, Y4
	VADDPD       Y8, Y10, Y8
	VADDPD       Y12, Y14, Y12
	VADDPD       Y0, Y4, Y0
	VADDPD       Y8, Y12, Y8
	VADDPD       Y0, Y8, Y0
	VEXTRACTI128 $0x01, Y0, X1
	VADDPD       X1, X0, X0
	VHADDPD      X0, X0, X0
	MOVSD        X0, ret+24(FP)
	RET
