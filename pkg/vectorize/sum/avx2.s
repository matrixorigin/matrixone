// Code generated by command: go run avx2.go -out sum/avx2.s -stubs sum/avx2_stubs.go. DO NOT EDIT.

#include "textflag.h"

// func int8SumAvx2Asm(x []int8, r []int8)
// Requires: AVX, AVX2, SSE2
TEXT ·int8SumAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), AX
	MOVQ  r_base+24(FP), CX
	MOVQ  x_len+8(FP), DX
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5

int8SumBlockLoop:
	CMPQ   DX, $0x000000c0
	JL     int8SumTailLoop
	VPADDB (AX), Y0, Y0
	VPADDB 32(AX), Y1, Y1
	VPADDB 64(AX), Y2, Y2
	VPADDB 96(AX), Y3, Y3
	VPADDB 128(AX), Y4, Y4
	VPADDB 160(AX), Y5, Y5
	ADDQ   $0x000000c0, AX
	SUBQ   $0x000000c0, DX
	JMP    int8SumBlockLoop

int8SumTailLoop:
	CMPQ   DX, $0x00000004
	JL     int8SumDone
	VPADDB (AX), Y0, Y0
	ADDQ   $0x00000020, AX
	SUBQ   $0x00000020, DX
	JMP    int8SumTailLoop

int8SumDone:
	VPADDB       Y0, Y1, Y0
	VPADDB       Y0, Y2, Y0
	VPADDB       Y0, Y3, Y0
	VPADDB       Y0, Y4, Y0
	VPADDB       Y0, Y5, Y0
	VEXTRACTI128 $0x01, Y0, X1
	PADDB        X1, X0
	MOVOU        X0, (CX)
	RET

// func int16SumAvx2Asm(x []int16, r []int16)
// Requires: AVX, AVX2, SSE2
TEXT ·int16SumAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), AX
	MOVQ  r_base+24(FP), CX
	MOVQ  x_len+8(FP), DX
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5

int16SumBlockLoop:
	CMPQ   DX, $0x00000060
	JL     int16SumTailLoop
	VPADDW (AX), Y0, Y0
	VPADDW 32(AX), Y1, Y1
	VPADDW 64(AX), Y2, Y2
	VPADDW 96(AX), Y3, Y3
	VPADDW 128(AX), Y4, Y4
	VPADDW 160(AX), Y5, Y5
	ADDQ   $0x000000c0, AX
	SUBQ   $0x00000060, DX
	JMP    int16SumBlockLoop

int16SumTailLoop:
	CMPQ   DX, $0x00000004
	JL     int16SumDone
	VPADDW (AX), Y0, Y0
	ADDQ   $0x00000020, AX
	SUBQ   $0x00000010, DX
	JMP    int16SumTailLoop

int16SumDone:
	VPADDW       Y0, Y1, Y0
	VPADDW       Y0, Y2, Y0
	VPADDW       Y0, Y3, Y0
	VPADDW       Y0, Y4, Y0
	VPADDW       Y0, Y5, Y0
	VEXTRACTI128 $0x01, Y0, X1
	PADDW        X1, X0
	MOVOU        X0, (CX)
	RET

// func int32SumAvx2Asm(x []int32, r []int32)
// Requires: AVX, AVX2, SSE2
TEXT ·int32SumAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), AX
	MOVQ  r_base+24(FP), CX
	MOVQ  x_len+8(FP), DX
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5

int32SumBlockLoop:
	CMPQ   DX, $0x00000030
	JL     int32SumTailLoop
	VPADDD (AX), Y0, Y0
	VPADDD 32(AX), Y1, Y1
	VPADDD 64(AX), Y2, Y2
	VPADDD 96(AX), Y3, Y3
	VPADDD 128(AX), Y4, Y4
	VPADDD 160(AX), Y5, Y5
	ADDQ   $0x000000c0, AX
	SUBQ   $0x00000030, DX
	JMP    int32SumBlockLoop

int32SumTailLoop:
	CMPQ   DX, $0x00000004
	JL     int32SumDone
	VPADDD (AX), Y0, Y0
	ADDQ   $0x00000020, AX
	SUBQ   $0x00000008, DX
	JMP    int32SumTailLoop

int32SumDone:
	VPADDD       Y0, Y1, Y0
	VPADDD       Y0, Y2, Y0
	VPADDD       Y0, Y3, Y0
	VPADDD       Y0, Y4, Y0
	VPADDD       Y0, Y5, Y0
	VEXTRACTI128 $0x01, Y0, X1
	PADDD        X1, X0
	MOVOU        X0, (CX)
	RET

// func int64SumAvx2Asm(x []int64, r []int64)
// Requires: AVX, AVX2, SSE2
TEXT ·int64SumAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), AX
	MOVQ  r_base+24(FP), CX
	MOVQ  x_len+8(FP), DX
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5

int64SumBlockLoop:
	CMPQ   DX, $0x00000018
	JL     int64SumTailLoop
	VPADDQ (AX), Y0, Y0
	VPADDQ 32(AX), Y1, Y1
	VPADDQ 64(AX), Y2, Y2
	VPADDQ 96(AX), Y3, Y3
	VPADDQ 128(AX), Y4, Y4
	VPADDQ 160(AX), Y5, Y5
	ADDQ   $0x000000c0, AX
	SUBQ   $0x00000018, DX
	JMP    int64SumBlockLoop

int64SumTailLoop:
	CMPQ   DX, $0x00000004
	JL     int64SumDone
	VPADDQ (AX), Y0, Y0
	ADDQ   $0x00000020, AX
	SUBQ   $0x00000004, DX
	JMP    int64SumTailLoop

int64SumDone:
	VPADDQ       Y0, Y1, Y0
	VPADDQ       Y0, Y2, Y0
	VPADDQ       Y0, Y3, Y0
	VPADDQ       Y0, Y4, Y0
	VPADDQ       Y0, Y5, Y0
	VEXTRACTI128 $0x01, Y0, X1
	PADDQ        X1, X0
	MOVOU        X0, (CX)
	RET

// func float32SumAvx2Asm(x []float32, r []float32)
// Requires: AVX, AVX2, SSE, SSE2
TEXT ·float32SumAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), AX
	MOVQ  r_base+24(FP), CX
	MOVQ  x_len+8(FP), DX
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5

float32SumBlockLoop:
	CMPQ   DX, $0x00000030
	JL     float32SumTailLoop
	VADDPS (AX), Y0, Y0
	VADDPS 32(AX), Y1, Y1
	VADDPS 64(AX), Y2, Y2
	VADDPS 96(AX), Y3, Y3
	VADDPS 128(AX), Y4, Y4
	VADDPS 160(AX), Y5, Y5
	ADDQ   $0x000000c0, AX
	SUBQ   $0x00000030, DX
	JMP    float32SumBlockLoop

float32SumTailLoop:
	CMPQ   DX, $0x00000004
	JL     float32SumDone
	VADDPS (AX), Y0, Y0
	ADDQ   $0x00000020, AX
	SUBQ   $0x00000008, DX
	JMP    float32SumTailLoop

float32SumDone:
	VADDPS       Y0, Y1, Y0
	VADDPS       Y0, Y2, Y0
	VADDPS       Y0, Y3, Y0
	VADDPS       Y0, Y4, Y0
	VADDPS       Y0, Y5, Y0
	VEXTRACTF128 $0x01, Y0, X1
	ADDPS        X1, X0
	MOVOU        X0, (CX)
	RET

// func float64SumAvx2Asm(x []float64, r []float64)
// Requires: AVX, AVX2, SSE2
TEXT ·float64SumAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), AX
	MOVQ  r_base+24(FP), CX
	MOVQ  x_len+8(FP), DX
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5

float64SumBlockLoop:
	CMPQ   DX, $0x00000018
	JL     float64SumTailLoop
	VADDPD (AX), Y0, Y0
	VADDPD 32(AX), Y1, Y1
	VADDPD 64(AX), Y2, Y2
	VADDPD 96(AX), Y3, Y3
	VADDPD 128(AX), Y4, Y4
	VADDPD 160(AX), Y5, Y5
	ADDQ   $0x000000c0, AX
	SUBQ   $0x00000018, DX
	JMP    float64SumBlockLoop

float64SumTailLoop:
	CMPQ   DX, $0x00000004
	JL     float64SumDone
	VADDPD (AX), Y0, Y0
	ADDQ   $0x00000020, AX
	SUBQ   $0x00000004, DX
	JMP    float64SumTailLoop

float64SumDone:
	VADDPD       Y0, Y1, Y0
	VADDPD       Y0, Y2, Y0
	VADDPD       Y0, Y3, Y0
	VADDPD       Y0, Y4, Y0
	VADDPD       Y0, Y5, Y0
	VEXTRACTF128 $0x01, Y0, X1
	ADDPD        X1, X0
	MOVOU        X0, (CX)
	RET
