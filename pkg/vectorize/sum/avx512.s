// Code generated by command: go run avx512.go -out sum/avx512.s -stubs sum/avx512_stubs.go. DO NOT EDIT.

#include "textflag.h"

// func int8SumAvx512Asm(x []int8, r []int8)
// Requires: AVX2, AVX512BW, AVX512F, SSE2
TEXT ·int8SumAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ   x_base+0(FP), AX
	MOVQ   r_base+24(FP), CX
	MOVQ   x_len+8(FP), DX
	VPXORD Z0, Z0, Z0
	VPXORD Z1, Z1, Z1
	VPXORD Z2, Z2, Z2
	VPXORD Z3, Z3, Z3
	VPXORD Z4, Z4, Z4
	VPXORD Z5, Z5, Z5
	VPXORD Z6, Z6, Z6
	VPXORD Z7, Z7, Z7
	VPXORD Z8, Z8, Z8
	VPXORD Z9, Z9, Z9
	VPXORD Z10, Z10, Z10
	VPXORD Z11, Z11, Z11

int8SumBlockLoop:
	CMPQ   DX, $0x00000300
	JL     int8SumTailLoop
	VPADDB (AX), Z0, Z0
	VPADDB 64(AX), Z1, Z1
	VPADDB 128(AX), Z2, Z2
	VPADDB 192(AX), Z3, Z3
	VPADDB 256(AX), Z4, Z4
	VPADDB 320(AX), Z5, Z5
	VPADDB 384(AX), Z6, Z6
	VPADDB 448(AX), Z7, Z7
	VPADDB 512(AX), Z8, Z8
	VPADDB 576(AX), Z9, Z9
	VPADDB 640(AX), Z10, Z10
	VPADDB 704(AX), Z11, Z11
	ADDQ   $0x00000300, AX
	SUBQ   $0x00000300, DX
	JMP    int8SumBlockLoop

int8SumTailLoop:
	CMPQ   DX, $0x00000004
	JL     int8SumDone
	VPADDB (AX), Z0, Z0
	ADDQ   $0x00000040, AX
	SUBQ   $0x00000040, DX
	JMP    int8SumTailLoop

int8SumDone:
	VPADDB        Z0, Z1, Z0
	VPADDB        Z0, Z2, Z0
	VPADDB        Z0, Z3, Z0
	VPADDB        Z0, Z4, Z0
	VPADDB        Z0, Z5, Z0
	VPADDB        Z0, Z6, Z0
	VPADDB        Z0, Z7, Z0
	VPADDB        Z0, Z8, Z0
	VPADDB        Z0, Z9, Z0
	VPADDB        Z0, Z10, Z0
	VPADDB        Z0, Z11, Z0
	VEXTRACTI64X4 $0x01, Z0, Y1
	VPADDB        Y1, Y0, Y0
	VEXTRACTI128  $0x01, Y0, X1
	PADDB         X1, X0
	MOVOU         X0, (CX)
	RET

// func int16SumAvx512Asm(x []int16, r []int16)
// Requires: AVX2, AVX512BW, AVX512F, SSE2
TEXT ·int16SumAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ   x_base+0(FP), AX
	MOVQ   r_base+24(FP), CX
	MOVQ   x_len+8(FP), DX
	VPXORD Z0, Z0, Z0
	VPXORD Z1, Z1, Z1
	VPXORD Z2, Z2, Z2
	VPXORD Z3, Z3, Z3
	VPXORD Z4, Z4, Z4
	VPXORD Z5, Z5, Z5
	VPXORD Z6, Z6, Z6
	VPXORD Z7, Z7, Z7
	VPXORD Z8, Z8, Z8
	VPXORD Z9, Z9, Z9
	VPXORD Z10, Z10, Z10
	VPXORD Z11, Z11, Z11

int16SumBlockLoop:
	CMPQ   DX, $0x00000180
	JL     int16SumTailLoop
	VPADDW (AX), Z0, Z0
	VPADDW 64(AX), Z1, Z1
	VPADDW 128(AX), Z2, Z2
	VPADDW 192(AX), Z3, Z3
	VPADDW 256(AX), Z4, Z4
	VPADDW 320(AX), Z5, Z5
	VPADDW 384(AX), Z6, Z6
	VPADDW 448(AX), Z7, Z7
	VPADDW 512(AX), Z8, Z8
	VPADDW 576(AX), Z9, Z9
	VPADDW 640(AX), Z10, Z10
	VPADDW 704(AX), Z11, Z11
	ADDQ   $0x00000300, AX
	SUBQ   $0x00000180, DX
	JMP    int16SumBlockLoop

int16SumTailLoop:
	CMPQ   DX, $0x00000004
	JL     int16SumDone
	VPADDW (AX), Z0, Z0
	ADDQ   $0x00000040, AX
	SUBQ   $0x00000020, DX
	JMP    int16SumTailLoop

int16SumDone:
	VPADDW        Z0, Z1, Z0
	VPADDW        Z0, Z2, Z0
	VPADDW        Z0, Z3, Z0
	VPADDW        Z0, Z4, Z0
	VPADDW        Z0, Z5, Z0
	VPADDW        Z0, Z6, Z0
	VPADDW        Z0, Z7, Z0
	VPADDW        Z0, Z8, Z0
	VPADDW        Z0, Z9, Z0
	VPADDW        Z0, Z10, Z0
	VPADDW        Z0, Z11, Z0
	VEXTRACTI64X4 $0x01, Z0, Y1
	VPADDW        Y1, Y0, Y0
	VEXTRACTI128  $0x01, Y0, X1
	PADDW         X1, X0
	MOVOU         X0, (CX)
	RET

// func int32SumAvx512Asm(x []int32, r []int32)
// Requires: AVX2, AVX512F, SSE2
TEXT ·int32SumAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ   x_base+0(FP), AX
	MOVQ   r_base+24(FP), CX
	MOVQ   x_len+8(FP), DX
	VPXORD Z0, Z0, Z0
	VPXORD Z1, Z1, Z1
	VPXORD Z2, Z2, Z2
	VPXORD Z3, Z3, Z3
	VPXORD Z4, Z4, Z4
	VPXORD Z5, Z5, Z5
	VPXORD Z6, Z6, Z6
	VPXORD Z7, Z7, Z7
	VPXORD Z8, Z8, Z8
	VPXORD Z9, Z9, Z9
	VPXORD Z10, Z10, Z10
	VPXORD Z11, Z11, Z11

int32SumBlockLoop:
	CMPQ   DX, $0x000000c0
	JL     int32SumTailLoop
	VPADDD (AX), Z0, Z0
	VPADDD 64(AX), Z1, Z1
	VPADDD 128(AX), Z2, Z2
	VPADDD 192(AX), Z3, Z3
	VPADDD 256(AX), Z4, Z4
	VPADDD 320(AX), Z5, Z5
	VPADDD 384(AX), Z6, Z6
	VPADDD 448(AX), Z7, Z7
	VPADDD 512(AX), Z8, Z8
	VPADDD 576(AX), Z9, Z9
	VPADDD 640(AX), Z10, Z10
	VPADDD 704(AX), Z11, Z11
	ADDQ   $0x00000300, AX
	SUBQ   $0x000000c0, DX
	JMP    int32SumBlockLoop

int32SumTailLoop:
	CMPQ   DX, $0x00000004
	JL     int32SumDone
	VPADDD (AX), Z0, Z0
	ADDQ   $0x00000040, AX
	SUBQ   $0x00000010, DX
	JMP    int32SumTailLoop

int32SumDone:
	VPADDD        Z0, Z1, Z0
	VPADDD        Z0, Z2, Z0
	VPADDD        Z0, Z3, Z0
	VPADDD        Z0, Z4, Z0
	VPADDD        Z0, Z5, Z0
	VPADDD        Z0, Z6, Z0
	VPADDD        Z0, Z7, Z0
	VPADDD        Z0, Z8, Z0
	VPADDD        Z0, Z9, Z0
	VPADDD        Z0, Z10, Z0
	VPADDD        Z0, Z11, Z0
	VEXTRACTI64X4 $0x01, Z0, Y1
	VPADDD        Y1, Y0, Y0
	VEXTRACTI128  $0x01, Y0, X1
	PADDD         X1, X0
	MOVOU         X0, (CX)
	RET

// func int64SumAvx512Asm(x []int64, r []int64)
// Requires: AVX2, AVX512F, SSE2
TEXT ·int64SumAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ   x_base+0(FP), AX
	MOVQ   r_base+24(FP), CX
	MOVQ   x_len+8(FP), DX
	VPXORD Z0, Z0, Z0
	VPXORD Z1, Z1, Z1
	VPXORD Z2, Z2, Z2
	VPXORD Z3, Z3, Z3
	VPXORD Z4, Z4, Z4
	VPXORD Z5, Z5, Z5
	VPXORD Z6, Z6, Z6
	VPXORD Z7, Z7, Z7
	VPXORD Z8, Z8, Z8
	VPXORD Z9, Z9, Z9
	VPXORD Z10, Z10, Z10
	VPXORD Z11, Z11, Z11

int64SumBlockLoop:
	CMPQ   DX, $0x00000060
	JL     int64SumTailLoop
	VPADDQ (AX), Z0, Z0
	VPADDQ 64(AX), Z1, Z1
	VPADDQ 128(AX), Z2, Z2
	VPADDQ 192(AX), Z3, Z3
	VPADDQ 256(AX), Z4, Z4
	VPADDQ 320(AX), Z5, Z5
	VPADDQ 384(AX), Z6, Z6
	VPADDQ 448(AX), Z7, Z7
	VPADDQ 512(AX), Z8, Z8
	VPADDQ 576(AX), Z9, Z9
	VPADDQ 640(AX), Z10, Z10
	VPADDQ 704(AX), Z11, Z11
	ADDQ   $0x00000300, AX
	SUBQ   $0x00000060, DX
	JMP    int64SumBlockLoop

int64SumTailLoop:
	CMPQ   DX, $0x00000004
	JL     int64SumDone
	VPADDQ (AX), Z0, Z0
	ADDQ   $0x00000040, AX
	SUBQ   $0x00000008, DX
	JMP    int64SumTailLoop

int64SumDone:
	VPADDQ        Z0, Z1, Z0
	VPADDQ        Z0, Z2, Z0
	VPADDQ        Z0, Z3, Z0
	VPADDQ        Z0, Z4, Z0
	VPADDQ        Z0, Z5, Z0
	VPADDQ        Z0, Z6, Z0
	VPADDQ        Z0, Z7, Z0
	VPADDQ        Z0, Z8, Z0
	VPADDQ        Z0, Z9, Z0
	VPADDQ        Z0, Z10, Z0
	VPADDQ        Z0, Z11, Z0
	VEXTRACTI64X4 $0x01, Z0, Y1
	VPADDQ        Y1, Y0, Y0
	VEXTRACTI128  $0x01, Y0, X1
	PADDQ         X1, X0
	MOVOU         X0, (CX)
	RET

// func float32SumAvx512Asm(x []float32, r []float32)
// Requires: AVX, AVX2, AVX512F, SSE, SSE2
TEXT ·float32SumAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ   x_base+0(FP), AX
	MOVQ   r_base+24(FP), CX
	MOVQ   x_len+8(FP), DX
	VPXORD Z0, Z0, Z0
	VPXORD Z1, Z1, Z1
	VPXORD Z2, Z2, Z2
	VPXORD Z3, Z3, Z3
	VPXORD Z4, Z4, Z4
	VPXORD Z5, Z5, Z5
	VPXORD Z6, Z6, Z6
	VPXORD Z7, Z7, Z7
	VPXORD Z8, Z8, Z8
	VPXORD Z9, Z9, Z9
	VPXORD Z10, Z10, Z10
	VPXORD Z11, Z11, Z11

float32SumBlockLoop:
	CMPQ   DX, $0x000000c0
	JL     float32SumTailLoop
	VADDPS (AX), Z0, Z0
	VADDPS 64(AX), Z1, Z1
	VADDPS 128(AX), Z2, Z2
	VADDPS 192(AX), Z3, Z3
	VADDPS 256(AX), Z4, Z4
	VADDPS 320(AX), Z5, Z5
	VADDPS 384(AX), Z6, Z6
	VADDPS 448(AX), Z7, Z7
	VADDPS 512(AX), Z8, Z8
	VADDPS 576(AX), Z9, Z9
	VADDPS 640(AX), Z10, Z10
	VADDPS 704(AX), Z11, Z11
	ADDQ   $0x00000300, AX
	SUBQ   $0x000000c0, DX
	JMP    float32SumBlockLoop

float32SumTailLoop:
	CMPQ   DX, $0x00000004
	JL     float32SumDone
	VADDPS (AX), Z0, Z0
	ADDQ   $0x00000040, AX
	SUBQ   $0x00000010, DX
	JMP    float32SumTailLoop

float32SumDone:
	VADDPS        Z0, Z1, Z0
	VADDPS        Z0, Z2, Z0
	VADDPS        Z0, Z3, Z0
	VADDPS        Z0, Z4, Z0
	VADDPS        Z0, Z5, Z0
	VADDPS        Z0, Z6, Z0
	VADDPS        Z0, Z7, Z0
	VADDPS        Z0, Z8, Z0
	VADDPS        Z0, Z9, Z0
	VADDPS        Z0, Z10, Z0
	VADDPS        Z0, Z11, Z0
	VEXTRACTF32X8 $0x01, Z0, Y1
	VADDPS        Y1, Y0, Y0
	VEXTRACTF128  $0x01, Y0, X1
	ADDPS         X1, X0
	MOVOU         X0, (CX)
	RET

// func float64SumAvx512Asm(x []float64, r []float64)
// Requires: AVX, AVX2, AVX512F, SSE2
TEXT ·float64SumAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ   x_base+0(FP), AX
	MOVQ   r_base+24(FP), CX
	MOVQ   x_len+8(FP), DX
	VPXORD Z0, Z0, Z0
	VPXORD Z1, Z1, Z1
	VPXORD Z2, Z2, Z2
	VPXORD Z3, Z3, Z3
	VPXORD Z4, Z4, Z4
	VPXORD Z5, Z5, Z5
	VPXORD Z6, Z6, Z6
	VPXORD Z7, Z7, Z7
	VPXORD Z8, Z8, Z8
	VPXORD Z9, Z9, Z9
	VPXORD Z10, Z10, Z10
	VPXORD Z11, Z11, Z11

float64SumBlockLoop:
	CMPQ   DX, $0x00000060
	JL     float64SumTailLoop
	VADDPD (AX), Z0, Z0
	VADDPD 64(AX), Z1, Z1
	VADDPD 128(AX), Z2, Z2
	VADDPD 192(AX), Z3, Z3
	VADDPD 256(AX), Z4, Z4
	VADDPD 320(AX), Z5, Z5
	VADDPD 384(AX), Z6, Z6
	VADDPD 448(AX), Z7, Z7
	VADDPD 512(AX), Z8, Z8
	VADDPD 576(AX), Z9, Z9
	VADDPD 640(AX), Z10, Z10
	VADDPD 704(AX), Z11, Z11
	ADDQ   $0x00000300, AX
	SUBQ   $0x00000060, DX
	JMP    float64SumBlockLoop

float64SumTailLoop:
	CMPQ   DX, $0x00000004
	JL     float64SumDone
	VADDPD (AX), Z0, Z0
	ADDQ   $0x00000040, AX
	SUBQ   $0x00000008, DX
	JMP    float64SumTailLoop

float64SumDone:
	VADDPD        Z0, Z1, Z0
	VADDPD        Z0, Z2, Z0
	VADDPD        Z0, Z3, Z0
	VADDPD        Z0, Z4, Z0
	VADDPD        Z0, Z5, Z0
	VADDPD        Z0, Z6, Z0
	VADDPD        Z0, Z7, Z0
	VADDPD        Z0, Z8, Z0
	VADDPD        Z0, Z9, Z0
	VADDPD        Z0, Z10, Z0
	VADDPD        Z0, Z11, Z0
	VEXTRACTF64X4 $0x01, Z0, Y1
	VADDPD        Y1, Y0, Y0
	VEXTRACTF128  $0x01, Y0, X1
	ADDPD         X1, X0
	MOVOU         X0, (CX)
	RET
