// Code generated by command: go run avx512_gen.go -out avx512.s -stubs avx512_stubs.go. DO NOT EDIT.
// +build amd64

#include "textflag.h"

// func int8SumAvx512Asm(x []int8) int64
// Requires: AVX, AVX2, AVX512BW, AVX512F, SSE2
TEXT ·int8SumAvx512Asm(SB), NOSPLIT, $0-32
	MOVQ   x_base+0(FP), AX
	MOVQ   x_len+8(FP), CX
	XORL   DX, DX
	VPXORQ Z0, Z0, Z0
	VPXORQ Z1, Z1, Z1
	CMPQ   CX, $0x000001e0
	JL     int8SumTail
	MOVL   $0x0000000f, BX
	VPXORQ Z3, Z3, Z3
	VPXORQ Z5, Z5, Z5
	VPXORQ Z7, Z7, Z7
	VPXORQ Z9, Z9, Z9
	VPXORQ Z11, Z11, Z11
	VPXORQ Z13, Z13, Z13
	VPXORQ Z15, Z15, Z15
	VPXORQ Z17, Z17, Z17
	VPXORQ Z19, Z19, Z19
	VPXORQ Z21, Z21, Z21
	VPXORQ Z23, Z23, Z23
	VPXORQ Z25, Z25, Z25
	VPXORQ Z27, Z27, Z27
	VPXORQ Z29, Z29, Z29
	VPXORQ Z31, Z31, Z31

int8SumBlockLoop:
	VPMOVSXBW     (AX), Z2
	VPMOVSXBW     32(AX), Z4
	VPMOVSXBW     64(AX), Z6
	VPMOVSXBW     96(AX), Z8
	VPMOVSXBW     128(AX), Z10
	VPMOVSXBW     160(AX), Z12
	VPMOVSXBW     192(AX), Z14
	VPMOVSXBW     224(AX), Z16
	VPMOVSXBW     256(AX), Z18
	VPMOVSXBW     288(AX), Z20
	VPMOVSXBW     320(AX), Z22
	VPMOVSXBW     352(AX), Z24
	VPMOVSXBW     384(AX), Z26
	VPMOVSXBW     416(AX), Z28
	VPMOVSXBW     448(AX), Z30
	VPADDW        Z2, Z3, Z3
	VPADDW        Z4, Z5, Z5
	VPADDW        Z6, Z7, Z7
	VPADDW        Z8, Z9, Z9
	VPADDW        Z10, Z11, Z11
	VPADDW        Z12, Z13, Z13
	VPADDW        Z14, Z15, Z15
	VPADDW        Z16, Z17, Z17
	VPADDW        Z18, Z19, Z19
	VPADDW        Z20, Z21, Z21
	VPADDW        Z22, Z23, Z23
	VPADDW        Z24, Z25, Z25
	VPADDW        Z26, Z27, Z27
	VPADDW        Z28, Z29, Z29
	VPADDW        Z30, Z31, Z31
	ADDQ          $0x000001e0, AX
	SUBQ          $0x000001e0, CX
	INCL          DX
	CMPQ          CX, $0x000001e0
	JL            int8SumTail
	CMPL          DX, $0x000000ff
	JLE           int8SumBlockLoop
	VEXTRACTI64X4 $0x01, Z3, Y2
	VPMOVSXWD     Y3, Z3
	VPMOVSXWD     Y2, Z2
	VPADDD        Z2, Z3, Z3
	VEXTRACTI64X4 $0x01, Z5, Y4
	VPMOVSXWD     Y5, Z5
	VPMOVSXWD     Y4, Z4
	VPADDD        Z4, Z5, Z5
	VEXTRACTI64X4 $0x01, Z7, Y6
	VPMOVSXWD     Y7, Z7
	VPMOVSXWD     Y6, Z6
	VPADDD        Z6, Z7, Z7
	VEXTRACTI64X4 $0x01, Z9, Y8
	VPMOVSXWD     Y9, Z9
	VPMOVSXWD     Y8, Z8
	VPADDD        Z8, Z9, Z9
	VEXTRACTI64X4 $0x01, Z11, Y10
	VPMOVSXWD     Y11, Z11
	VPMOVSXWD     Y10, Z10
	VPADDD        Z10, Z11, Z11
	VEXTRACTI64X4 $0x01, Z13, Y12
	VPMOVSXWD     Y13, Z13
	VPMOVSXWD     Y12, Z12
	VPADDD        Z12, Z13, Z13
	VEXTRACTI64X4 $0x01, Z15, Y14
	VPMOVSXWD     Y15, Z15
	VPMOVSXWD     Y14, Z14
	VPADDD        Z14, Z15, Z15
	VEXTRACTI64X4 $0x01, Z17, Y16
	VPMOVSXWD     Y17, Z17
	VPMOVSXWD     Y16, Z16
	VPADDD        Z16, Z17, Z17
	VEXTRACTI64X4 $0x01, Z19, Y18
	VPMOVSXWD     Y19, Z19
	VPMOVSXWD     Y18, Z18
	VPADDD        Z18, Z19, Z19
	VEXTRACTI64X4 $0x01, Z21, Y20
	VPMOVSXWD     Y21, Z21
	VPMOVSXWD     Y20, Z20
	VPADDD        Z20, Z21, Z21
	VEXTRACTI64X4 $0x01, Z23, Y22
	VPMOVSXWD     Y23, Z23
	VPMOVSXWD     Y22, Z22
	VPADDD        Z22, Z23, Z23
	VEXTRACTI64X4 $0x01, Z25, Y24
	VPMOVSXWD     Y25, Z25
	VPMOVSXWD     Y24, Z24
	VPADDD        Z24, Z25, Z25
	VEXTRACTI64X4 $0x01, Z27, Y26
	VPMOVSXWD     Y27, Z27
	VPMOVSXWD     Y26, Z26
	VPADDD        Z26, Z27, Z27
	VEXTRACTI64X4 $0x01, Z29, Y28
	VPMOVSXWD     Y29, Z29
	VPMOVSXWD     Y28, Z28
	VPADDD        Z28, Z29, Z29
	VEXTRACTI64X4 $0x01, Z31, Y30
	VPMOVSXWD     Y31, Z31
	VPMOVSXWD     Y30, Z30
	VPADDD        Z30, Z31, Z31
	VPADDD        Z5, Z3, Z3
	VPXORQ        Z5, Z5, Z5
	VPADDD        Z9, Z7, Z7
	VPXORQ        Z9, Z9, Z9
	VPADDD        Z13, Z11, Z11
	VPXORQ        Z13, Z13, Z13
	VPADDD        Z17, Z15, Z15
	VPXORQ        Z17, Z17, Z17
	VPADDD        Z21, Z19, Z19
	VPXORQ        Z21, Z21, Z21
	VPADDD        Z25, Z23, Z23
	VPXORQ        Z25, Z25, Z25
	VPADDD        Z29, Z27, Z27
	VPXORQ        Z29, Z29, Z29
	VPADDD        Z7, Z3, Z3
	VPXORQ        Z7, Z7, Z7
	VPADDD        Z15, Z11, Z11
	VPXORQ        Z15, Z15, Z15
	VPADDD        Z23, Z19, Z19
	VPXORQ        Z23, Z23, Z23
	VPADDD        Z31, Z27, Z27
	VPXORQ        Z31, Z31, Z31
	VPADDD        Z11, Z3, Z3
	VPXORQ        Z11, Z11, Z11
	VPADDD        Z27, Z19, Z19
	VPXORQ        Z27, Z27, Z27
	VPADDD        Z19, Z0, Z0
	VPXORQ        Z19, Z19, Z19
	VPADDD        Z3, Z0, Z0
	VPXORQ        Z3, Z3, Z3
	XORL          DX, DX
	ADDL          $0x0f, BX
	CMPL          BX, $0x00010000
	JLE           int8SumBlockLoop
	XORL          BX, BX
	VEXTRACTI64X4 $0x01, Z0, Y2
	VPMOVSXDQ     Y0, Z0
	VPADDQ        Z0, Z1, Z1
	VPMOVSXDQ     Y2, Z2
	VPADDQ        Z2, Z1, Z1
	VPXORQ        Z0, Z0, Z0
	JMP           int8SumBlockLoop

int8SumTail:
	TESTL         DX, DX
	JZ            int8SumTailLoop
	VEXTRACTI64X4 $0x01, Z3, Y2
	VPMOVSXWD     Y3, Z3
	VPMOVSXWD     Y2, Z2
	VPADDD        Z2, Z3, Z3
	VEXTRACTI64X4 $0x01, Z5, Y4
	VPMOVSXWD     Y5, Z5
	VPMOVSXWD     Y4, Z4
	VPADDD        Z4, Z5, Z5
	VEXTRACTI64X4 $0x01, Z7, Y6
	VPMOVSXWD     Y7, Z7
	VPMOVSXWD     Y6, Z6
	VPADDD        Z6, Z7, Z7
	VEXTRACTI64X4 $0x01, Z9, Y8
	VPMOVSXWD     Y9, Z9
	VPMOVSXWD     Y8, Z8
	VPADDD        Z8, Z9, Z9
	VEXTRACTI64X4 $0x01, Z11, Y10
	VPMOVSXWD     Y11, Z11
	VPMOVSXWD     Y10, Z10
	VPADDD        Z10, Z11, Z11
	VEXTRACTI64X4 $0x01, Z13, Y12
	VPMOVSXWD     Y13, Z13
	VPMOVSXWD     Y12, Z12
	VPADDD        Z12, Z13, Z13
	VEXTRACTI64X4 $0x01, Z15, Y14
	VPMOVSXWD     Y15, Z15
	VPMOVSXWD     Y14, Z14
	VPADDD        Z14, Z15, Z15
	VEXTRACTI64X4 $0x01, Z17, Y16
	VPMOVSXWD     Y17, Z17
	VPMOVSXWD     Y16, Z16
	VPADDD        Z16, Z17, Z17
	VEXTRACTI64X4 $0x01, Z19, Y18
	VPMOVSXWD     Y19, Z19
	VPMOVSXWD     Y18, Z18
	VPADDD        Z18, Z19, Z19
	VEXTRACTI64X4 $0x01, Z21, Y20
	VPMOVSXWD     Y21, Z21
	VPMOVSXWD     Y20, Z20
	VPADDD        Z20, Z21, Z21
	VEXTRACTI64X4 $0x01, Z23, Y22
	VPMOVSXWD     Y23, Z23
	VPMOVSXWD     Y22, Z22
	VPADDD        Z22, Z23, Z23
	VEXTRACTI64X4 $0x01, Z25, Y24
	VPMOVSXWD     Y25, Z25
	VPMOVSXWD     Y24, Z24
	VPADDD        Z24, Z25, Z25
	VEXTRACTI64X4 $0x01, Z27, Y26
	VPMOVSXWD     Y27, Z27
	VPMOVSXWD     Y26, Z26
	VPADDD        Z26, Z27, Z27
	VEXTRACTI64X4 $0x01, Z29, Y28
	VPMOVSXWD     Y29, Z29
	VPMOVSXWD     Y28, Z28
	VPADDD        Z28, Z29, Z29
	VEXTRACTI64X4 $0x01, Z31, Y30
	VPMOVSXWD     Y31, Z31
	VPMOVSXWD     Y30, Z30
	VPADDD        Z30, Z31, Z31
	VPADDD        Z5, Z3, Z3
	VPADDD        Z9, Z7, Z7
	VPADDD        Z13, Z11, Z11
	VPADDD        Z17, Z15, Z15
	VPADDD        Z21, Z19, Z19
	VPADDD        Z25, Z23, Z23
	VPADDD        Z29, Z27, Z27
	VPADDD        Z7, Z3, Z3
	VPADDD        Z15, Z11, Z11
	VPADDD        Z23, Z19, Z19
	VPADDD        Z31, Z27, Z27
	VPADDD        Z11, Z3, Z3
	VPADDD        Z27, Z19, Z19
	VPADDD        Z19, Z0, Z0
	VPADDD        Z3, Z0, Z0
	VEXTRACTI64X4 $0x01, Z0, Y2
	VPMOVSXDQ     Y0, Z0
	VPADDQ        Z0, Z1, Z1
	VPMOVSXDQ     Y2, Z2
	VPADDQ        Z2, Z1, Z1

int8SumTailLoop:
	CMPQ      CX, $0x00000008
	JL        int8SumDone
	VPMOVSXBQ (AX), Z0
	VPADDQ    Z0, Z1, Z1
	ADDQ      $0x00000008, AX
	SUBQ      $0x00000008, CX
	JMP       int8SumTailLoop

int8SumDone:
	VEXTRACTI64X4 $0x01, Z1, Y0
	VPADDQ        Y0, Y1, Y1
	CMPQ          CX, $0x04
	JL            int8SumDone1
	VPMOVSXBQ     (AX), Y2
	VPADDQ        Y2, Y1, Y1

int8SumDone1:
	VEXTRACTI128 $0x01, Y1, X0
	VPADDQ       X0, X1, X1
	VPSHUFD      $0x4e, X1, X0
	VPADDQ       X0, X1, X1
	MOVQ         X1, ret+24(FP)
	RET

// func int16SumAvx512Asm(x []int16) int64
// Requires: AVX, AVX2, AVX512F, SSE2
TEXT ·int16SumAvx512Asm(SB), NOSPLIT, $0-32
	MOVQ   x_base+0(FP), AX
	MOVQ   x_len+8(FP), CX
	XORL   DX, DX
	VPXORQ Z0, Z0, Z0
	CMPQ   CX, $0x000000f0
	JL     int16SumTail
	VPXORQ Z2, Z2, Z2
	VPXORQ Z4, Z4, Z4
	VPXORQ Z6, Z6, Z6
	VPXORQ Z8, Z8, Z8
	VPXORQ Z10, Z10, Z10
	VPXORQ Z12, Z12, Z12
	VPXORQ Z14, Z14, Z14
	VPXORQ Z16, Z16, Z16
	VPXORQ Z18, Z18, Z18
	VPXORQ Z20, Z20, Z20
	VPXORQ Z22, Z22, Z22
	VPXORQ Z24, Z24, Z24
	VPXORQ Z26, Z26, Z26
	VPXORQ Z28, Z28, Z28
	VPXORQ Z30, Z30, Z30

int16SumBlockLoop:
	VPMOVSXWD     (AX), Z1
	VPMOVSXWD     32(AX), Z3
	VPMOVSXWD     64(AX), Z5
	VPMOVSXWD     96(AX), Z7
	VPMOVSXWD     128(AX), Z9
	VPMOVSXWD     160(AX), Z11
	VPMOVSXWD     192(AX), Z13
	VPMOVSXWD     224(AX), Z15
	VPMOVSXWD     256(AX), Z17
	VPMOVSXWD     288(AX), Z19
	VPMOVSXWD     320(AX), Z21
	VPMOVSXWD     352(AX), Z23
	VPMOVSXWD     384(AX), Z25
	VPMOVSXWD     416(AX), Z27
	VPMOVSXWD     448(AX), Z29
	VPADDD        Z1, Z2, Z2
	VPADDD        Z3, Z4, Z4
	VPADDD        Z5, Z6, Z6
	VPADDD        Z7, Z8, Z8
	VPADDD        Z9, Z10, Z10
	VPADDD        Z11, Z12, Z12
	VPADDD        Z13, Z14, Z14
	VPADDD        Z15, Z16, Z16
	VPADDD        Z17, Z18, Z18
	VPADDD        Z19, Z20, Z20
	VPADDD        Z21, Z22, Z22
	VPADDD        Z23, Z24, Z24
	VPADDD        Z25, Z26, Z26
	VPADDD        Z27, Z28, Z28
	VPADDD        Z29, Z30, Z30
	ADDQ          $0x000001e0, AX
	SUBQ          $0x000000f0, CX
	INCL          DX
	CMPQ          CX, $0x000000f0
	JL            int16SumTail
	CMPL          DX, $0x0000ffff
	JLE           int16SumBlockLoop
	VEXTRACTI64X4 $0x01, Z2, Y1
	VPMOVSXDQ     Y2, Z2
	VPMOVSXDQ     Y1, Z1
	VPADDQ        Z1, Z2, Z2
	VEXTRACTI64X4 $0x01, Z4, Y3
	VPMOVSXDQ     Y4, Z4
	VPMOVSXDQ     Y3, Z3
	VPADDQ        Z3, Z4, Z4
	VEXTRACTI64X4 $0x01, Z6, Y5
	VPMOVSXDQ     Y6, Z6
	VPMOVSXDQ     Y5, Z5
	VPADDQ        Z5, Z6, Z6
	VEXTRACTI64X4 $0x01, Z8, Y7
	VPMOVSXDQ     Y8, Z8
	VPMOVSXDQ     Y7, Z7
	VPADDQ        Z7, Z8, Z8
	VEXTRACTI64X4 $0x01, Z10, Y9
	VPMOVSXDQ     Y10, Z10
	VPMOVSXDQ     Y9, Z9
	VPADDQ        Z9, Z10, Z10
	VEXTRACTI64X4 $0x01, Z12, Y11
	VPMOVSXDQ     Y12, Z12
	VPMOVSXDQ     Y11, Z11
	VPADDQ        Z11, Z12, Z12
	VEXTRACTI64X4 $0x01, Z14, Y13
	VPMOVSXDQ     Y14, Z14
	VPMOVSXDQ     Y13, Z13
	VPADDQ        Z13, Z14, Z14
	VEXTRACTI64X4 $0x01, Z16, Y15
	VPMOVSXDQ     Y16, Z16
	VPMOVSXDQ     Y15, Z15
	VPADDQ        Z15, Z16, Z16
	VEXTRACTI64X4 $0x01, Z18, Y17
	VPMOVSXDQ     Y18, Z18
	VPMOVSXDQ     Y17, Z17
	VPADDQ        Z17, Z18, Z18
	VEXTRACTI64X4 $0x01, Z20, Y19
	VPMOVSXDQ     Y20, Z20
	VPMOVSXDQ     Y19, Z19
	VPADDQ        Z19, Z20, Z20
	VEXTRACTI64X4 $0x01, Z22, Y21
	VPMOVSXDQ     Y22, Z22
	VPMOVSXDQ     Y21, Z21
	VPADDQ        Z21, Z22, Z22
	VEXTRACTI64X4 $0x01, Z24, Y23
	VPMOVSXDQ     Y24, Z24
	VPMOVSXDQ     Y23, Z23
	VPADDQ        Z23, Z24, Z24
	VEXTRACTI64X4 $0x01, Z26, Y25
	VPMOVSXDQ     Y26, Z26
	VPMOVSXDQ     Y25, Z25
	VPADDQ        Z25, Z26, Z26
	VEXTRACTI64X4 $0x01, Z28, Y27
	VPMOVSXDQ     Y28, Z28
	VPMOVSXDQ     Y27, Z27
	VPADDQ        Z27, Z28, Z28
	VEXTRACTI64X4 $0x01, Z30, Y29
	VPMOVSXDQ     Y30, Z30
	VPMOVSXDQ     Y29, Z29
	VPADDQ        Z29, Z30, Z30
	VPADDQ        Z4, Z2, Z2
	VPXORQ        Z4, Z4, Z4
	VPADDQ        Z8, Z6, Z6
	VPXORQ        Z8, Z8, Z8
	VPADDQ        Z12, Z10, Z10
	VPXORQ        Z12, Z12, Z12
	VPADDQ        Z16, Z14, Z14
	VPXORQ        Z16, Z16, Z16
	VPADDQ        Z20, Z18, Z18
	VPXORQ        Z20, Z20, Z20
	VPADDQ        Z24, Z22, Z22
	VPXORQ        Z24, Z24, Z24
	VPADDQ        Z28, Z26, Z26
	VPXORQ        Z28, Z28, Z28
	VPADDQ        Z6, Z2, Z2
	VPXORQ        Z6, Z6, Z6
	VPADDQ        Z14, Z10, Z10
	VPXORQ        Z14, Z14, Z14
	VPADDQ        Z22, Z18, Z18
	VPXORQ        Z22, Z22, Z22
	VPADDQ        Z30, Z26, Z26
	VPXORQ        Z30, Z30, Z30
	VPADDQ        Z10, Z2, Z2
	VPXORQ        Z10, Z10, Z10
	VPADDQ        Z26, Z18, Z18
	VPXORQ        Z26, Z26, Z26
	VPADDQ        Z18, Z0, Z0
	VPXORQ        Z18, Z18, Z18
	VPADDD        Z2, Z0, Z0
	VPXORQ        Z2, Z2, Z2
	XORL          DX, DX
	JMP           int16SumBlockLoop

int16SumTail:
	TESTL         DX, DX
	JZ            int16SumTailLoop
	VEXTRACTI64X4 $0x01, Z2, Y1
	VPMOVSXDQ     Y2, Z2
	VPMOVSXDQ     Y1, Z1
	VPADDQ        Z1, Z2, Z2
	VEXTRACTI64X4 $0x01, Z4, Y3
	VPMOVSXDQ     Y4, Z4
	VPMOVSXDQ     Y3, Z3
	VPADDQ        Z3, Z4, Z4
	VEXTRACTI64X4 $0x01, Z6, Y5
	VPMOVSXDQ     Y6, Z6
	VPMOVSXDQ     Y5, Z5
	VPADDQ        Z5, Z6, Z6
	VEXTRACTI64X4 $0x01, Z8, Y7
	VPMOVSXDQ     Y8, Z8
	VPMOVSXDQ     Y7, Z7
	VPADDQ        Z7, Z8, Z8
	VEXTRACTI64X4 $0x01, Z10, Y9
	VPMOVSXDQ     Y10, Z10
	VPMOVSXDQ     Y9, Z9
	VPADDQ        Z9, Z10, Z10
	VEXTRACTI64X4 $0x01, Z12, Y11
	VPMOVSXDQ     Y12, Z12
	VPMOVSXDQ     Y11, Z11
	VPADDQ        Z11, Z12, Z12
	VEXTRACTI64X4 $0x01, Z14, Y13
	VPMOVSXDQ     Y14, Z14
	VPMOVSXDQ     Y13, Z13
	VPADDQ        Z13, Z14, Z14
	VEXTRACTI64X4 $0x01, Z16, Y15
	VPMOVSXDQ     Y16, Z16
	VPMOVSXDQ     Y15, Z15
	VPADDQ        Z15, Z16, Z16
	VEXTRACTI64X4 $0x01, Z18, Y17
	VPMOVSXDQ     Y18, Z18
	VPMOVSXDQ     Y17, Z17
	VPADDQ        Z17, Z18, Z18
	VEXTRACTI64X4 $0x01, Z20, Y19
	VPMOVSXDQ     Y20, Z20
	VPMOVSXDQ     Y19, Z19
	VPADDQ        Z19, Z20, Z20
	VEXTRACTI64X4 $0x01, Z22, Y21
	VPMOVSXDQ     Y22, Z22
	VPMOVSXDQ     Y21, Z21
	VPADDQ        Z21, Z22, Z22
	VEXTRACTI64X4 $0x01, Z24, Y23
	VPMOVSXDQ     Y24, Z24
	VPMOVSXDQ     Y23, Z23
	VPADDQ        Z23, Z24, Z24
	VEXTRACTI64X4 $0x01, Z26, Y25
	VPMOVSXDQ     Y26, Z26
	VPMOVSXDQ     Y25, Z25
	VPADDQ        Z25, Z26, Z26
	VEXTRACTI64X4 $0x01, Z28, Y27
	VPMOVSXDQ     Y28, Z28
	VPMOVSXDQ     Y27, Z27
	VPADDQ        Z27, Z28, Z28
	VEXTRACTI64X4 $0x01, Z30, Y29
	VPMOVSXDQ     Y30, Z30
	VPMOVSXDQ     Y29, Z29
	VPADDQ        Z29, Z30, Z30
	VPADDQ        Z4, Z2, Z2
	VPADDQ        Z8, Z6, Z6
	VPADDQ        Z12, Z10, Z10
	VPADDQ        Z16, Z14, Z14
	VPADDQ        Z20, Z18, Z18
	VPADDQ        Z24, Z22, Z22
	VPADDQ        Z28, Z26, Z26
	VPADDQ        Z6, Z2, Z2
	VPADDQ        Z14, Z10, Z10
	VPADDQ        Z22, Z18, Z18
	VPADDQ        Z30, Z26, Z26
	VPADDQ        Z10, Z2, Z2
	VPADDQ        Z26, Z18, Z18
	VPADDQ        Z18, Z0, Z0
	VPADDQ        Z2, Z0, Z0

int16SumTailLoop:
	CMPQ      CX, $0x00000008
	JL        int16SumDone
	VPMOVSXWQ (AX), Z2
	VPADDQ    Z2, Z0, Z0
	ADDQ      $0x00000010, AX
	SUBQ      $0x00000008, CX
	JMP       int16SumTailLoop

int16SumDone:
	VEXTRACTI64X4 $0x01, Z0, Y1
	VPADDQ        Y1, Y0, Y0
	CMPQ          CX, $0x04
	JL            int16SumDone1
	VPMOVSXWQ     (AX), Y1
	VPADDQ        Y1, Y0, Y0

int16SumDone1:
	VEXTRACTI128 $0x01, Y0, X1
	VPSHUFD      $0x4e, X0, X1
	VPADDQ       X1, X0, X0
	MOVQ         X0, ret+24(FP)
	RET

// func int32SumAvx512Asm(x []int32) int64
// Requires: AVX, AVX2, AVX512F, SSE2
TEXT ·int32SumAvx512Asm(SB), NOSPLIT, $0-32
	MOVQ   x_base+0(FP), AX
	MOVQ   x_len+8(FP), CX
	MOVQ   CX, DX
	VPXORQ Z1, Z1, Z1
	CMPQ   CX, $0x00000080
	JL     int32SumTail
	VPXORQ Z3, Z3, Z3
	VPXORQ Z5, Z5, Z5
	VPXORQ Z7, Z7, Z7
	VPXORQ Z9, Z9, Z9
	VPXORQ Z11, Z11, Z11
	VPXORQ Z13, Z13, Z13
	VPXORQ Z15, Z15, Z15
	VPXORQ Z17, Z17, Z17
	VPXORQ Z19, Z19, Z19
	VPXORQ Z21, Z21, Z21
	VPXORQ Z23, Z23, Z23
	VPXORQ Z25, Z25, Z25
	VPXORQ Z27, Z27, Z27
	VPXORQ Z29, Z29, Z29
	VPXORQ Z31, Z31, Z31

int32SumBlockLoop:
	VPMOVSXDQ (AX), Z0
	VPMOVSXDQ 32(AX), Z2
	VPMOVSXDQ 64(AX), Z4
	VPMOVSXDQ 96(AX), Z6
	VPMOVSXDQ 128(AX), Z8
	VPMOVSXDQ 160(AX), Z10
	VPMOVSXDQ 192(AX), Z12
	VPMOVSXDQ 224(AX), Z14
	VPMOVSXDQ 256(AX), Z16
	VPMOVSXDQ 288(AX), Z18
	VPMOVSXDQ 320(AX), Z20
	VPMOVSXDQ 352(AX), Z22
	VPMOVSXDQ 384(AX), Z24
	VPMOVSXDQ 416(AX), Z26
	VPMOVSXDQ 448(AX), Z28
	VPMOVSXDQ 480(AX), Z30
	VPADDQ    Z0, Z1, Z1
	VPADDQ    Z2, Z3, Z3
	VPADDQ    Z4, Z5, Z5
	VPADDQ    Z6, Z7, Z7
	VPADDQ    Z8, Z9, Z9
	VPADDQ    Z10, Z11, Z11
	VPADDQ    Z12, Z13, Z13
	VPADDQ    Z14, Z15, Z15
	VPADDQ    Z16, Z17, Z17
	VPADDQ    Z18, Z19, Z19
	VPADDQ    Z20, Z21, Z21
	VPADDQ    Z22, Z23, Z23
	VPADDQ    Z24, Z25, Z25
	VPADDQ    Z26, Z27, Z27
	VPADDQ    Z28, Z29, Z29
	VPADDQ    Z30, Z31, Z31
	ADDQ      $0x00000200, AX
	SUBQ      $0x00000080, CX
	CMPQ      CX, $0x00000080
	JL        int32SumTail
	JMP       int32SumBlockLoop

int32SumTail:
	CMPQ   CX, DX
	JZ     int32SumTailLoop
	VPADDQ Z3, Z1, Z1
	VPADDQ Z7, Z5, Z5
	VPADDQ Z11, Z9, Z9
	VPADDQ Z15, Z13, Z13
	VPADDQ Z19, Z17, Z17
	VPADDQ Z23, Z21, Z21
	VPADDQ Z27, Z25, Z25
	VPADDQ Z31, Z29, Z29
	VPADDQ Z5, Z1, Z1
	VPADDQ Z13, Z9, Z9
	VPADDQ Z21, Z17, Z17
	VPADDQ Z29, Z25, Z25
	VPADDQ Z9, Z1, Z1
	VPADDQ Z25, Z17, Z17
	VPADDQ Z17, Z1, Z1

int32SumTailLoop:
	CMPQ      CX, $0x00000004
	JL        int32SumDone
	VPMOVSXDQ (AX), Z3
	VPADDQ    Z3, Z1, Z1
	ADDQ      $0x00000010, AX
	SUBQ      $0x00000004, CX
	JMP       int32SumTailLoop

int32SumDone:
	VEXTRACTI64X4 $0x01, Z1, Y0
	VPADDQ        Y0, Y1, Y1
	CMPQ          CX, $0x04
	JL            int32SumDone1
	VPMOVSXDQ     (AX), Y0
	VPADDQ        Y0, Y1, Y1

int32SumDone1:
	VEXTRACTI128 $0x01, Y1, X0
	VPSHUFD      $0x4e, X1, X0
	VPADDQ       X0, X1, X1
	MOVQ         X1, ret+24(FP)
	RET

// func int64SumAvx512Asm(x []int64) int64
// Requires: AVX, AVX2, AVX512F, SSE2
TEXT ·int64SumAvx512Asm(SB), NOSPLIT, $0-32
	MOVQ   x_base+0(FP), AX
	MOVQ   x_len+8(FP), CX
	VPXORQ Z0, Z0, Z0
	VPXORQ Z1, Z1, Z1
	VPXORQ Z2, Z2, Z2
	VPXORQ Z3, Z3, Z3
	VPXORQ Z4, Z4, Z4
	VPXORQ Z5, Z5, Z5
	VPXORQ Z6, Z6, Z6
	VPXORQ Z7, Z7, Z7
	VPXORQ Z8, Z8, Z8
	VPXORQ Z9, Z9, Z9
	VPXORQ Z10, Z10, Z10
	VPXORQ Z11, Z11, Z11
	VPXORQ Z12, Z12, Z12
	VPXORQ Z13, Z13, Z13
	VPXORQ Z14, Z14, Z14
	VPXORQ Z15, Z15, Z15
	VPXORQ Z16, Z16, Z16
	VPXORQ Z17, Z17, Z17
	VPXORQ Z18, Z18, Z18
	VPXORQ Z19, Z19, Z19
	VPXORQ Z20, Z20, Z20
	VPXORQ Z21, Z21, Z21
	VPXORQ Z22, Z22, Z22
	VPXORQ Z23, Z23, Z23
	VPXORQ Z24, Z24, Z24
	VPXORQ Z25, Z25, Z25
	VPXORQ Z26, Z26, Z26
	VPXORQ Z27, Z27, Z27
	VPXORQ Z28, Z28, Z28
	VPXORQ Z29, Z29, Z29
	VPXORQ Z30, Z30, Z30
	VPXORQ Z31, Z31, Z31

int64SumBlockLoop:
	CMPQ   CX, $0x00000100
	JL     int64SumTailLoop
	VPADDQ (AX), Z0, Z0
	VPADDQ 64(AX), Z1, Z1
	VPADDQ 128(AX), Z2, Z2
	VPADDQ 192(AX), Z3, Z3
	VPADDQ 256(AX), Z4, Z4
	VPADDQ 320(AX), Z5, Z5
	VPADDQ 384(AX), Z6, Z6
	VPADDQ 448(AX), Z7, Z7
	VPADDQ 512(AX), Z8, Z8
	VPADDQ 576(AX), Z9, Z9
	VPADDQ 640(AX), Z10, Z10
	VPADDQ 704(AX), Z11, Z11
	VPADDQ 768(AX), Z12, Z12
	VPADDQ 832(AX), Z13, Z13
	VPADDQ 896(AX), Z14, Z14
	VPADDQ 960(AX), Z15, Z15
	VPADDQ 1024(AX), Z16, Z16
	VPADDQ 1088(AX), Z17, Z17
	VPADDQ 1152(AX), Z18, Z18
	VPADDQ 1216(AX), Z19, Z19
	VPADDQ 1280(AX), Z20, Z20
	VPADDQ 1344(AX), Z21, Z21
	VPADDQ 1408(AX), Z22, Z22
	VPADDQ 1472(AX), Z23, Z23
	VPADDQ 1536(AX), Z24, Z24
	VPADDQ 1600(AX), Z25, Z25
	VPADDQ 1664(AX), Z26, Z26
	VPADDQ 1728(AX), Z27, Z27
	VPADDQ 1792(AX), Z28, Z28
	VPADDQ 1856(AX), Z29, Z29
	VPADDQ 1920(AX), Z30, Z30
	VPADDQ 1984(AX), Z31, Z31
	ADDQ   $0x00000800, AX
	SUBQ   $0x00000100, CX
	JMP    int64SumBlockLoop

int64SumTailLoop:
	CMPQ   CX, $0x00000008
	JL     int64SumDone
	VPADDQ (AX), Z0, Z0
	ADDQ   $0x00000040, AX
	SUBQ   $0x00000008, CX
	JMP    int64SumTailLoop

int64SumDone:
	VPADDQ        Z1, Z0, Z0
	VPADDQ        Z3, Z2, Z2
	VPADDQ        Z5, Z4, Z4
	VPADDQ        Z7, Z6, Z6
	VPADDQ        Z9, Z8, Z8
	VPADDQ        Z11, Z10, Z10
	VPADDQ        Z13, Z12, Z12
	VPADDQ        Z15, Z14, Z14
	VPADDQ        Z17, Z16, Z16
	VPADDQ        Z19, Z18, Z18
	VPADDQ        Z21, Z20, Z20
	VPADDQ        Z23, Z22, Z22
	VPADDQ        Z25, Z24, Z24
	VPADDQ        Z27, Z26, Z26
	VPADDQ        Z29, Z28, Z28
	VPADDQ        Z31, Z30, Z30
	VPADDQ        Z2, Z0, Z0
	VPADDQ        Z6, Z4, Z4
	VPADDQ        Z10, Z8, Z8
	VPADDQ        Z14, Z12, Z12
	VPADDQ        Z18, Z16, Z16
	VPADDQ        Z22, Z20, Z20
	VPADDQ        Z26, Z24, Z24
	VPADDQ        Z30, Z28, Z28
	VPADDQ        Z4, Z0, Z0
	VPADDQ        Z12, Z8, Z8
	VPADDQ        Z20, Z16, Z16
	VPADDQ        Z28, Z24, Z24
	VPADDQ        Z8, Z0, Z0
	VPADDQ        Z24, Z16, Z16
	VPADDQ        Z16, Z0, Z0
	VEXTRACTI64X4 $0x01, Z0, Y1
	VPADDQ        Y1, Y0, Y0
	CMPQ          CX, $0x04
	JL            int64SumDone1
	VPADDQ        (AX), Y0, Y0

int64SumDone1:
	VEXTRACTI128 $0x01, Y0, X1
	VPSHUFD      $0x4e, X0, X1
	VPADDQ       X1, X0, X0
	MOVQ         X0, ret+24(FP)
	RET

// func uint8SumAvx512Asm(x []uint8) uint64
// Requires: AVX, AVX2, AVX512BW, AVX512F, SSE2
TEXT ·uint8SumAvx512Asm(SB), NOSPLIT, $0-32
	MOVQ   x_base+0(FP), AX
	MOVQ   x_len+8(FP), CX
	XORL   DX, DX
	VPXORQ Z0, Z0, Z0
	VPXORQ Z1, Z1, Z1
	CMPQ   CX, $0x000001e0
	JL     uint8SumTail
	MOVL   $0x0000000f, BX
	VPXORQ Z3, Z3, Z3
	VPXORQ Z5, Z5, Z5
	VPXORQ Z7, Z7, Z7
	VPXORQ Z9, Z9, Z9
	VPXORQ Z11, Z11, Z11
	VPXORQ Z13, Z13, Z13
	VPXORQ Z15, Z15, Z15
	VPXORQ Z17, Z17, Z17
	VPXORQ Z19, Z19, Z19
	VPXORQ Z21, Z21, Z21
	VPXORQ Z23, Z23, Z23
	VPXORQ Z25, Z25, Z25
	VPXORQ Z27, Z27, Z27
	VPXORQ Z29, Z29, Z29
	VPXORQ Z31, Z31, Z31

uint8SumBlockLoop:
	VPMOVZXBW     (AX), Z2
	VPMOVZXBW     32(AX), Z4
	VPMOVZXBW     64(AX), Z6
	VPMOVZXBW     96(AX), Z8
	VPMOVZXBW     128(AX), Z10
	VPMOVZXBW     160(AX), Z12
	VPMOVZXBW     192(AX), Z14
	VPMOVZXBW     224(AX), Z16
	VPMOVZXBW     256(AX), Z18
	VPMOVZXBW     288(AX), Z20
	VPMOVZXBW     320(AX), Z22
	VPMOVZXBW     352(AX), Z24
	VPMOVZXBW     384(AX), Z26
	VPMOVZXBW     416(AX), Z28
	VPMOVZXBW     448(AX), Z30
	VPADDW        Z2, Z3, Z3
	VPADDW        Z4, Z5, Z5
	VPADDW        Z6, Z7, Z7
	VPADDW        Z8, Z9, Z9
	VPADDW        Z10, Z11, Z11
	VPADDW        Z12, Z13, Z13
	VPADDW        Z14, Z15, Z15
	VPADDW        Z16, Z17, Z17
	VPADDW        Z18, Z19, Z19
	VPADDW        Z20, Z21, Z21
	VPADDW        Z22, Z23, Z23
	VPADDW        Z24, Z25, Z25
	VPADDW        Z26, Z27, Z27
	VPADDW        Z28, Z29, Z29
	VPADDW        Z30, Z31, Z31
	ADDQ          $0x000001e0, AX
	SUBQ          $0x000001e0, CX
	INCL          DX
	CMPQ          CX, $0x000001e0
	JL            uint8SumTail
	CMPL          DX, $0x000000ff
	JLE           uint8SumBlockLoop
	VEXTRACTI64X4 $0x01, Z3, Y2
	VPMOVZXWD     Y3, Z3
	VPMOVZXWD     Y2, Z2
	VPADDD        Z2, Z3, Z3
	VEXTRACTI64X4 $0x01, Z5, Y4
	VPMOVZXWD     Y5, Z5
	VPMOVZXWD     Y4, Z4
	VPADDD        Z4, Z5, Z5
	VEXTRACTI64X4 $0x01, Z7, Y6
	VPMOVZXWD     Y7, Z7
	VPMOVZXWD     Y6, Z6
	VPADDD        Z6, Z7, Z7
	VEXTRACTI64X4 $0x01, Z9, Y8
	VPMOVZXWD     Y9, Z9
	VPMOVZXWD     Y8, Z8
	VPADDD        Z8, Z9, Z9
	VEXTRACTI64X4 $0x01, Z11, Y10
	VPMOVZXWD     Y11, Z11
	VPMOVZXWD     Y10, Z10
	VPADDD        Z10, Z11, Z11
	VEXTRACTI64X4 $0x01, Z13, Y12
	VPMOVZXWD     Y13, Z13
	VPMOVZXWD     Y12, Z12
	VPADDD        Z12, Z13, Z13
	VEXTRACTI64X4 $0x01, Z15, Y14
	VPMOVZXWD     Y15, Z15
	VPMOVZXWD     Y14, Z14
	VPADDD        Z14, Z15, Z15
	VEXTRACTI64X4 $0x01, Z17, Y16
	VPMOVZXWD     Y17, Z17
	VPMOVZXWD     Y16, Z16
	VPADDD        Z16, Z17, Z17
	VEXTRACTI64X4 $0x01, Z19, Y18
	VPMOVZXWD     Y19, Z19
	VPMOVZXWD     Y18, Z18
	VPADDD        Z18, Z19, Z19
	VEXTRACTI64X4 $0x01, Z21, Y20
	VPMOVZXWD     Y21, Z21
	VPMOVZXWD     Y20, Z20
	VPADDD        Z20, Z21, Z21
	VEXTRACTI64X4 $0x01, Z23, Y22
	VPMOVZXWD     Y23, Z23
	VPMOVZXWD     Y22, Z22
	VPADDD        Z22, Z23, Z23
	VEXTRACTI64X4 $0x01, Z25, Y24
	VPMOVZXWD     Y25, Z25
	VPMOVZXWD     Y24, Z24
	VPADDD        Z24, Z25, Z25
	VEXTRACTI64X4 $0x01, Z27, Y26
	VPMOVZXWD     Y27, Z27
	VPMOVZXWD     Y26, Z26
	VPADDD        Z26, Z27, Z27
	VEXTRACTI64X4 $0x01, Z29, Y28
	VPMOVZXWD     Y29, Z29
	VPMOVZXWD     Y28, Z28
	VPADDD        Z28, Z29, Z29
	VEXTRACTI64X4 $0x01, Z31, Y30
	VPMOVZXWD     Y31, Z31
	VPMOVZXWD     Y30, Z30
	VPADDD        Z30, Z31, Z31
	VPADDD        Z5, Z3, Z3
	VPXORQ        Z5, Z5, Z5
	VPADDD        Z9, Z7, Z7
	VPXORQ        Z9, Z9, Z9
	VPADDD        Z13, Z11, Z11
	VPXORQ        Z13, Z13, Z13
	VPADDD        Z17, Z15, Z15
	VPXORQ        Z17, Z17, Z17
	VPADDD        Z21, Z19, Z19
	VPXORQ        Z21, Z21, Z21
	VPADDD        Z25, Z23, Z23
	VPXORQ        Z25, Z25, Z25
	VPADDD        Z29, Z27, Z27
	VPXORQ        Z29, Z29, Z29
	VPADDD        Z7, Z3, Z3
	VPXORQ        Z7, Z7, Z7
	VPADDD        Z15, Z11, Z11
	VPXORQ        Z15, Z15, Z15
	VPADDD        Z23, Z19, Z19
	VPXORQ        Z23, Z23, Z23
	VPADDD        Z31, Z27, Z27
	VPXORQ        Z31, Z31, Z31
	VPADDD        Z11, Z3, Z3
	VPXORQ        Z11, Z11, Z11
	VPADDD        Z27, Z19, Z19
	VPXORQ        Z27, Z27, Z27
	VPADDD        Z19, Z0, Z0
	VPXORQ        Z19, Z19, Z19
	VPADDD        Z3, Z0, Z0
	VPXORQ        Z3, Z3, Z3
	XORL          DX, DX
	ADDL          $0x0f, BX
	CMPL          BX, $0x00010000
	JLE           uint8SumBlockLoop
	XORL          BX, BX
	VEXTRACTI64X4 $0x01, Z0, Y2
	VPMOVZXDQ     Y0, Z0
	VPADDQ        Z0, Z1, Z1
	VPMOVZXDQ     Y2, Z2
	VPADDQ        Z2, Z1, Z1
	VPXORQ        Z0, Z0, Z0
	JMP           uint8SumBlockLoop

uint8SumTail:
	TESTL         DX, DX
	JZ            uint8SumTailLoop
	VEXTRACTI64X4 $0x01, Z3, Y2
	VPMOVZXWD     Y3, Z3
	VPMOVZXWD     Y2, Z2
	VPADDD        Z2, Z3, Z3
	VEXTRACTI64X4 $0x01, Z5, Y4
	VPMOVZXWD     Y5, Z5
	VPMOVZXWD     Y4, Z4
	VPADDD        Z4, Z5, Z5
	VEXTRACTI64X4 $0x01, Z7, Y6
	VPMOVZXWD     Y7, Z7
	VPMOVZXWD     Y6, Z6
	VPADDD        Z6, Z7, Z7
	VEXTRACTI64X4 $0x01, Z9, Y8
	VPMOVZXWD     Y9, Z9
	VPMOVZXWD     Y8, Z8
	VPADDD        Z8, Z9, Z9
	VEXTRACTI64X4 $0x01, Z11, Y10
	VPMOVZXWD     Y11, Z11
	VPMOVZXWD     Y10, Z10
	VPADDD        Z10, Z11, Z11
	VEXTRACTI64X4 $0x01, Z13, Y12
	VPMOVZXWD     Y13, Z13
	VPMOVZXWD     Y12, Z12
	VPADDD        Z12, Z13, Z13
	VEXTRACTI64X4 $0x01, Z15, Y14
	VPMOVZXWD     Y15, Z15
	VPMOVZXWD     Y14, Z14
	VPADDD        Z14, Z15, Z15
	VEXTRACTI64X4 $0x01, Z17, Y16
	VPMOVZXWD     Y17, Z17
	VPMOVZXWD     Y16, Z16
	VPADDD        Z16, Z17, Z17
	VEXTRACTI64X4 $0x01, Z19, Y18
	VPMOVZXWD     Y19, Z19
	VPMOVZXWD     Y18, Z18
	VPADDD        Z18, Z19, Z19
	VEXTRACTI64X4 $0x01, Z21, Y20
	VPMOVZXWD     Y21, Z21
	VPMOVZXWD     Y20, Z20
	VPADDD        Z20, Z21, Z21
	VEXTRACTI64X4 $0x01, Z23, Y22
	VPMOVZXWD     Y23, Z23
	VPMOVZXWD     Y22, Z22
	VPADDD        Z22, Z23, Z23
	VEXTRACTI64X4 $0x01, Z25, Y24
	VPMOVZXWD     Y25, Z25
	VPMOVZXWD     Y24, Z24
	VPADDD        Z24, Z25, Z25
	VEXTRACTI64X4 $0x01, Z27, Y26
	VPMOVZXWD     Y27, Z27
	VPMOVZXWD     Y26, Z26
	VPADDD        Z26, Z27, Z27
	VEXTRACTI64X4 $0x01, Z29, Y28
	VPMOVZXWD     Y29, Z29
	VPMOVZXWD     Y28, Z28
	VPADDD        Z28, Z29, Z29
	VEXTRACTI64X4 $0x01, Z31, Y30
	VPMOVZXWD     Y31, Z31
	VPMOVZXWD     Y30, Z30
	VPADDD        Z30, Z31, Z31
	VPADDD        Z5, Z3, Z3
	VPADDD        Z9, Z7, Z7
	VPADDD        Z13, Z11, Z11
	VPADDD        Z17, Z15, Z15
	VPADDD        Z21, Z19, Z19
	VPADDD        Z25, Z23, Z23
	VPADDD        Z29, Z27, Z27
	VPADDD        Z7, Z3, Z3
	VPADDD        Z15, Z11, Z11
	VPADDD        Z23, Z19, Z19
	VPADDD        Z31, Z27, Z27
	VPADDD        Z11, Z3, Z3
	VPADDD        Z27, Z19, Z19
	VPADDD        Z19, Z0, Z0
	VPADDD        Z3, Z0, Z0
	VEXTRACTI64X4 $0x01, Z0, Y2
	VPMOVZXDQ     Y0, Z0
	VPADDQ        Z0, Z1, Z1
	VPMOVZXDQ     Y2, Z2
	VPADDQ        Z2, Z1, Z1

uint8SumTailLoop:
	CMPQ      CX, $0x00000008
	JL        uint8SumDone
	VPMOVZXBQ (AX), Z0
	VPADDQ    Z0, Z1, Z1
	ADDQ      $0x00000008, AX
	SUBQ      $0x00000008, CX
	JMP       uint8SumTailLoop

uint8SumDone:
	VEXTRACTI64X4 $0x01, Z1, Y0
	VPADDQ        Y0, Y1, Y1
	CMPQ          CX, $0x04
	JL            uint8SumDone1
	VPMOVZXBQ     (AX), Y2
	VPADDQ        Y2, Y1, Y1

uint8SumDone1:
	VEXTRACTI128 $0x01, Y1, X0
	VPADDQ       X0, X1, X1
	VPSHUFD      $0x4e, X1, X0
	VPADDQ       X0, X1, X1
	MOVQ         X1, ret+24(FP)
	RET

// func uint16SumAvx512Asm(x []uint16) uint64
// Requires: AVX, AVX2, AVX512F, SSE2
TEXT ·uint16SumAvx512Asm(SB), NOSPLIT, $0-32
	MOVQ   x_base+0(FP), AX
	MOVQ   x_len+8(FP), CX
	XORL   DX, DX
	VPXORQ Z0, Z0, Z0
	CMPQ   CX, $0x000000f0
	JL     uint16SumTail
	VPXORQ Z2, Z2, Z2
	VPXORQ Z4, Z4, Z4
	VPXORQ Z6, Z6, Z6
	VPXORQ Z8, Z8, Z8
	VPXORQ Z10, Z10, Z10
	VPXORQ Z12, Z12, Z12
	VPXORQ Z14, Z14, Z14
	VPXORQ Z16, Z16, Z16
	VPXORQ Z18, Z18, Z18
	VPXORQ Z20, Z20, Z20
	VPXORQ Z22, Z22, Z22
	VPXORQ Z24, Z24, Z24
	VPXORQ Z26, Z26, Z26
	VPXORQ Z28, Z28, Z28
	VPXORQ Z30, Z30, Z30

uint16SumBlockLoop:
	VPMOVZXWD     (AX), Z1
	VPMOVZXWD     32(AX), Z3
	VPMOVZXWD     64(AX), Z5
	VPMOVZXWD     96(AX), Z7
	VPMOVZXWD     128(AX), Z9
	VPMOVZXWD     160(AX), Z11
	VPMOVZXWD     192(AX), Z13
	VPMOVZXWD     224(AX), Z15
	VPMOVZXWD     256(AX), Z17
	VPMOVZXWD     288(AX), Z19
	VPMOVZXWD     320(AX), Z21
	VPMOVZXWD     352(AX), Z23
	VPMOVZXWD     384(AX), Z25
	VPMOVZXWD     416(AX), Z27
	VPMOVZXWD     448(AX), Z29
	VPADDD        Z1, Z2, Z2
	VPADDD        Z3, Z4, Z4
	VPADDD        Z5, Z6, Z6
	VPADDD        Z7, Z8, Z8
	VPADDD        Z9, Z10, Z10
	VPADDD        Z11, Z12, Z12
	VPADDD        Z13, Z14, Z14
	VPADDD        Z15, Z16, Z16
	VPADDD        Z17, Z18, Z18
	VPADDD        Z19, Z20, Z20
	VPADDD        Z21, Z22, Z22
	VPADDD        Z23, Z24, Z24
	VPADDD        Z25, Z26, Z26
	VPADDD        Z27, Z28, Z28
	VPADDD        Z29, Z30, Z30
	ADDQ          $0x000001e0, AX
	SUBQ          $0x000000f0, CX
	INCL          DX
	CMPQ          CX, $0x000000f0
	JL            uint16SumTail
	CMPL          DX, $0x0000ffff
	JLE           uint16SumBlockLoop
	VEXTRACTI64X4 $0x01, Z2, Y1
	VPMOVZXDQ     Y2, Z2
	VPMOVZXDQ     Y1, Z1
	VPADDQ        Z1, Z2, Z2
	VEXTRACTI64X4 $0x01, Z4, Y3
	VPMOVZXDQ     Y4, Z4
	VPMOVZXDQ     Y3, Z3
	VPADDQ        Z3, Z4, Z4
	VEXTRACTI64X4 $0x01, Z6, Y5
	VPMOVZXDQ     Y6, Z6
	VPMOVZXDQ     Y5, Z5
	VPADDQ        Z5, Z6, Z6
	VEXTRACTI64X4 $0x01, Z8, Y7
	VPMOVZXDQ     Y8, Z8
	VPMOVZXDQ     Y7, Z7
	VPADDQ        Z7, Z8, Z8
	VEXTRACTI64X4 $0x01, Z10, Y9
	VPMOVZXDQ     Y10, Z10
	VPMOVZXDQ     Y9, Z9
	VPADDQ        Z9, Z10, Z10
	VEXTRACTI64X4 $0x01, Z12, Y11
	VPMOVZXDQ     Y12, Z12
	VPMOVZXDQ     Y11, Z11
	VPADDQ        Z11, Z12, Z12
	VEXTRACTI64X4 $0x01, Z14, Y13
	VPMOVZXDQ     Y14, Z14
	VPMOVZXDQ     Y13, Z13
	VPADDQ        Z13, Z14, Z14
	VEXTRACTI64X4 $0x01, Z16, Y15
	VPMOVZXDQ     Y16, Z16
	VPMOVZXDQ     Y15, Z15
	VPADDQ        Z15, Z16, Z16
	VEXTRACTI64X4 $0x01, Z18, Y17
	VPMOVZXDQ     Y18, Z18
	VPMOVZXDQ     Y17, Z17
	VPADDQ        Z17, Z18, Z18
	VEXTRACTI64X4 $0x01, Z20, Y19
	VPMOVZXDQ     Y20, Z20
	VPMOVZXDQ     Y19, Z19
	VPADDQ        Z19, Z20, Z20
	VEXTRACTI64X4 $0x01, Z22, Y21
	VPMOVZXDQ     Y22, Z22
	VPMOVZXDQ     Y21, Z21
	VPADDQ        Z21, Z22, Z22
	VEXTRACTI64X4 $0x01, Z24, Y23
	VPMOVZXDQ     Y24, Z24
	VPMOVZXDQ     Y23, Z23
	VPADDQ        Z23, Z24, Z24
	VEXTRACTI64X4 $0x01, Z26, Y25
	VPMOVZXDQ     Y26, Z26
	VPMOVZXDQ     Y25, Z25
	VPADDQ        Z25, Z26, Z26
	VEXTRACTI64X4 $0x01, Z28, Y27
	VPMOVZXDQ     Y28, Z28
	VPMOVZXDQ     Y27, Z27
	VPADDQ        Z27, Z28, Z28
	VEXTRACTI64X4 $0x01, Z30, Y29
	VPMOVZXDQ     Y30, Z30
	VPMOVZXDQ     Y29, Z29
	VPADDQ        Z29, Z30, Z30
	VPADDQ        Z4, Z2, Z2
	VPXORQ        Z4, Z4, Z4
	VPADDQ        Z8, Z6, Z6
	VPXORQ        Z8, Z8, Z8
	VPADDQ        Z12, Z10, Z10
	VPXORQ        Z12, Z12, Z12
	VPADDQ        Z16, Z14, Z14
	VPXORQ        Z16, Z16, Z16
	VPADDQ        Z20, Z18, Z18
	VPXORQ        Z20, Z20, Z20
	VPADDQ        Z24, Z22, Z22
	VPXORQ        Z24, Z24, Z24
	VPADDQ        Z28, Z26, Z26
	VPXORQ        Z28, Z28, Z28
	VPADDQ        Z6, Z2, Z2
	VPXORQ        Z6, Z6, Z6
	VPADDQ        Z14, Z10, Z10
	VPXORQ        Z14, Z14, Z14
	VPADDQ        Z22, Z18, Z18
	VPXORQ        Z22, Z22, Z22
	VPADDQ        Z30, Z26, Z26
	VPXORQ        Z30, Z30, Z30
	VPADDQ        Z10, Z2, Z2
	VPXORQ        Z10, Z10, Z10
	VPADDQ        Z26, Z18, Z18
	VPXORQ        Z26, Z26, Z26
	VPADDQ        Z18, Z0, Z0
	VPXORQ        Z18, Z18, Z18
	VPADDD        Z2, Z0, Z0
	VPXORQ        Z2, Z2, Z2
	XORL          DX, DX
	JMP           uint16SumBlockLoop

uint16SumTail:
	TESTL         DX, DX
	JZ            uint16SumTailLoop
	VEXTRACTI64X4 $0x01, Z2, Y1
	VPMOVZXDQ     Y2, Z2
	VPMOVZXDQ     Y1, Z1
	VPADDQ        Z1, Z2, Z2
	VEXTRACTI64X4 $0x01, Z4, Y3
	VPMOVZXDQ     Y4, Z4
	VPMOVZXDQ     Y3, Z3
	VPADDQ        Z3, Z4, Z4
	VEXTRACTI64X4 $0x01, Z6, Y5
	VPMOVZXDQ     Y6, Z6
	VPMOVZXDQ     Y5, Z5
	VPADDQ        Z5, Z6, Z6
	VEXTRACTI64X4 $0x01, Z8, Y7
	VPMOVZXDQ     Y8, Z8
	VPMOVZXDQ     Y7, Z7
	VPADDQ        Z7, Z8, Z8
	VEXTRACTI64X4 $0x01, Z10, Y9
	VPMOVZXDQ     Y10, Z10
	VPMOVZXDQ     Y9, Z9
	VPADDQ        Z9, Z10, Z10
	VEXTRACTI64X4 $0x01, Z12, Y11
	VPMOVZXDQ     Y12, Z12
	VPMOVZXDQ     Y11, Z11
	VPADDQ        Z11, Z12, Z12
	VEXTRACTI64X4 $0x01, Z14, Y13
	VPMOVZXDQ     Y14, Z14
	VPMOVZXDQ     Y13, Z13
	VPADDQ        Z13, Z14, Z14
	VEXTRACTI64X4 $0x01, Z16, Y15
	VPMOVZXDQ     Y16, Z16
	VPMOVZXDQ     Y15, Z15
	VPADDQ        Z15, Z16, Z16
	VEXTRACTI64X4 $0x01, Z18, Y17
	VPMOVZXDQ     Y18, Z18
	VPMOVZXDQ     Y17, Z17
	VPADDQ        Z17, Z18, Z18
	VEXTRACTI64X4 $0x01, Z20, Y19
	VPMOVZXDQ     Y20, Z20
	VPMOVZXDQ     Y19, Z19
	VPADDQ        Z19, Z20, Z20
	VEXTRACTI64X4 $0x01, Z22, Y21
	VPMOVZXDQ     Y22, Z22
	VPMOVZXDQ     Y21, Z21
	VPADDQ        Z21, Z22, Z22
	VEXTRACTI64X4 $0x01, Z24, Y23
	VPMOVZXDQ     Y24, Z24
	VPMOVZXDQ     Y23, Z23
	VPADDQ        Z23, Z24, Z24
	VEXTRACTI64X4 $0x01, Z26, Y25
	VPMOVZXDQ     Y26, Z26
	VPMOVZXDQ     Y25, Z25
	VPADDQ        Z25, Z26, Z26
	VEXTRACTI64X4 $0x01, Z28, Y27
	VPMOVZXDQ     Y28, Z28
	VPMOVZXDQ     Y27, Z27
	VPADDQ        Z27, Z28, Z28
	VEXTRACTI64X4 $0x01, Z30, Y29
	VPMOVZXDQ     Y30, Z30
	VPMOVZXDQ     Y29, Z29
	VPADDQ        Z29, Z30, Z30
	VPADDQ        Z4, Z2, Z2
	VPADDQ        Z8, Z6, Z6
	VPADDQ        Z12, Z10, Z10
	VPADDQ        Z16, Z14, Z14
	VPADDQ        Z20, Z18, Z18
	VPADDQ        Z24, Z22, Z22
	VPADDQ        Z28, Z26, Z26
	VPADDQ        Z6, Z2, Z2
	VPADDQ        Z14, Z10, Z10
	VPADDQ        Z22, Z18, Z18
	VPADDQ        Z30, Z26, Z26
	VPADDQ        Z10, Z2, Z2
	VPADDQ        Z26, Z18, Z18
	VPADDQ        Z18, Z0, Z0
	VPADDQ        Z2, Z0, Z0

uint16SumTailLoop:
	CMPQ      CX, $0x00000008
	JL        uint16SumDone
	VPMOVZXWQ (AX), Z2
	VPADDQ    Z2, Z0, Z0
	ADDQ      $0x00000010, AX
	SUBQ      $0x00000008, CX
	JMP       uint16SumTailLoop

uint16SumDone:
	VEXTRACTI64X4 $0x01, Z0, Y1
	VPADDQ        Y1, Y0, Y0
	CMPQ          CX, $0x04
	JL            uint16SumDone1
	VPMOVZXWQ     (AX), Y1
	VPADDQ        Y1, Y0, Y0

uint16SumDone1:
	VEXTRACTI128 $0x01, Y0, X1
	VPSHUFD      $0x4e, X0, X1
	VPADDQ       X1, X0, X0
	MOVQ         X0, ret+24(FP)
	RET

// func uint32SumAvx512Asm(x []uint32) uint64
// Requires: AVX, AVX2, AVX512F, SSE2
TEXT ·uint32SumAvx512Asm(SB), NOSPLIT, $0-32
	MOVQ   x_base+0(FP), AX
	MOVQ   x_len+8(FP), CX
	MOVQ   CX, DX
	VPXORQ Z1, Z1, Z1
	CMPQ   CX, $0x00000080
	JL     uint32SumTail
	VPXORQ Z3, Z3, Z3
	VPXORQ Z5, Z5, Z5
	VPXORQ Z7, Z7, Z7
	VPXORQ Z9, Z9, Z9
	VPXORQ Z11, Z11, Z11
	VPXORQ Z13, Z13, Z13
	VPXORQ Z15, Z15, Z15
	VPXORQ Z17, Z17, Z17
	VPXORQ Z19, Z19, Z19
	VPXORQ Z21, Z21, Z21
	VPXORQ Z23, Z23, Z23
	VPXORQ Z25, Z25, Z25
	VPXORQ Z27, Z27, Z27
	VPXORQ Z29, Z29, Z29
	VPXORQ Z31, Z31, Z31

uint32SumBlockLoop:
	VPMOVZXDQ (AX), Z0
	VPMOVZXDQ 32(AX), Z2
	VPMOVZXDQ 64(AX), Z4
	VPMOVZXDQ 96(AX), Z6
	VPMOVZXDQ 128(AX), Z8
	VPMOVZXDQ 160(AX), Z10
	VPMOVZXDQ 192(AX), Z12
	VPMOVZXDQ 224(AX), Z14
	VPMOVZXDQ 256(AX), Z16
	VPMOVZXDQ 288(AX), Z18
	VPMOVZXDQ 320(AX), Z20
	VPMOVZXDQ 352(AX), Z22
	VPMOVZXDQ 384(AX), Z24
	VPMOVZXDQ 416(AX), Z26
	VPMOVZXDQ 448(AX), Z28
	VPMOVZXDQ 480(AX), Z30
	VPADDQ    Z0, Z1, Z1
	VPADDQ    Z2, Z3, Z3
	VPADDQ    Z4, Z5, Z5
	VPADDQ    Z6, Z7, Z7
	VPADDQ    Z8, Z9, Z9
	VPADDQ    Z10, Z11, Z11
	VPADDQ    Z12, Z13, Z13
	VPADDQ    Z14, Z15, Z15
	VPADDQ    Z16, Z17, Z17
	VPADDQ    Z18, Z19, Z19
	VPADDQ    Z20, Z21, Z21
	VPADDQ    Z22, Z23, Z23
	VPADDQ    Z24, Z25, Z25
	VPADDQ    Z26, Z27, Z27
	VPADDQ    Z28, Z29, Z29
	VPADDQ    Z30, Z31, Z31
	ADDQ      $0x00000200, AX
	SUBQ      $0x00000080, CX
	CMPQ      CX, $0x00000080
	JL        uint32SumTail
	JMP       uint32SumBlockLoop

uint32SumTail:
	CMPQ   CX, DX
	JZ     uint32SumTailLoop
	VPADDQ Z3, Z1, Z1
	VPADDQ Z7, Z5, Z5
	VPADDQ Z11, Z9, Z9
	VPADDQ Z15, Z13, Z13
	VPADDQ Z19, Z17, Z17
	VPADDQ Z23, Z21, Z21
	VPADDQ Z27, Z25, Z25
	VPADDQ Z31, Z29, Z29
	VPADDQ Z5, Z1, Z1
	VPADDQ Z13, Z9, Z9
	VPADDQ Z21, Z17, Z17
	VPADDQ Z29, Z25, Z25
	VPADDQ Z9, Z1, Z1
	VPADDQ Z25, Z17, Z17
	VPADDQ Z17, Z1, Z1

uint32SumTailLoop:
	CMPQ      CX, $0x00000004
	JL        uint32SumDone
	VPMOVZXDQ (AX), Z3
	VPADDQ    Z3, Z1, Z1
	ADDQ      $0x00000010, AX
	SUBQ      $0x00000004, CX
	JMP       uint32SumTailLoop

uint32SumDone:
	VEXTRACTI64X4 $0x01, Z1, Y0
	VPADDQ        Y0, Y1, Y1
	CMPQ          CX, $0x04
	JL            uint32SumDone1
	VPMOVZXDQ     (AX), Y0
	VPADDQ        Y0, Y1, Y1

uint32SumDone1:
	VEXTRACTI128 $0x01, Y1, X0
	VPSHUFD      $0x4e, X1, X0
	VPADDQ       X0, X1, X1
	MOVQ         X1, ret+24(FP)
	RET

// func uint64SumAvx512Asm(x []uint64) uint64
// Requires: AVX, AVX2, AVX512F, SSE2
TEXT ·uint64SumAvx512Asm(SB), NOSPLIT, $0-32
	MOVQ   x_base+0(FP), AX
	MOVQ   x_len+8(FP), CX
	VPXORQ Z0, Z0, Z0
	VPXORQ Z1, Z1, Z1
	VPXORQ Z2, Z2, Z2
	VPXORQ Z3, Z3, Z3
	VPXORQ Z4, Z4, Z4
	VPXORQ Z5, Z5, Z5
	VPXORQ Z6, Z6, Z6
	VPXORQ Z7, Z7, Z7
	VPXORQ Z8, Z8, Z8
	VPXORQ Z9, Z9, Z9
	VPXORQ Z10, Z10, Z10
	VPXORQ Z11, Z11, Z11
	VPXORQ Z12, Z12, Z12
	VPXORQ Z13, Z13, Z13
	VPXORQ Z14, Z14, Z14
	VPXORQ Z15, Z15, Z15
	VPXORQ Z16, Z16, Z16
	VPXORQ Z17, Z17, Z17
	VPXORQ Z18, Z18, Z18
	VPXORQ Z19, Z19, Z19
	VPXORQ Z20, Z20, Z20
	VPXORQ Z21, Z21, Z21
	VPXORQ Z22, Z22, Z22
	VPXORQ Z23, Z23, Z23
	VPXORQ Z24, Z24, Z24
	VPXORQ Z25, Z25, Z25
	VPXORQ Z26, Z26, Z26
	VPXORQ Z27, Z27, Z27
	VPXORQ Z28, Z28, Z28
	VPXORQ Z29, Z29, Z29
	VPXORQ Z30, Z30, Z30
	VPXORQ Z31, Z31, Z31

uint64SumBlockLoop:
	CMPQ   CX, $0x00000100
	JL     uint64SumTailLoop
	VPADDQ (AX), Z0, Z0
	VPADDQ 64(AX), Z1, Z1
	VPADDQ 128(AX), Z2, Z2
	VPADDQ 192(AX), Z3, Z3
	VPADDQ 256(AX), Z4, Z4
	VPADDQ 320(AX), Z5, Z5
	VPADDQ 384(AX), Z6, Z6
	VPADDQ 448(AX), Z7, Z7
	VPADDQ 512(AX), Z8, Z8
	VPADDQ 576(AX), Z9, Z9
	VPADDQ 640(AX), Z10, Z10
	VPADDQ 704(AX), Z11, Z11
	VPADDQ 768(AX), Z12, Z12
	VPADDQ 832(AX), Z13, Z13
	VPADDQ 896(AX), Z14, Z14
	VPADDQ 960(AX), Z15, Z15
	VPADDQ 1024(AX), Z16, Z16
	VPADDQ 1088(AX), Z17, Z17
	VPADDQ 1152(AX), Z18, Z18
	VPADDQ 1216(AX), Z19, Z19
	VPADDQ 1280(AX), Z20, Z20
	VPADDQ 1344(AX), Z21, Z21
	VPADDQ 1408(AX), Z22, Z22
	VPADDQ 1472(AX), Z23, Z23
	VPADDQ 1536(AX), Z24, Z24
	VPADDQ 1600(AX), Z25, Z25
	VPADDQ 1664(AX), Z26, Z26
	VPADDQ 1728(AX), Z27, Z27
	VPADDQ 1792(AX), Z28, Z28
	VPADDQ 1856(AX), Z29, Z29
	VPADDQ 1920(AX), Z30, Z30
	VPADDQ 1984(AX), Z31, Z31
	ADDQ   $0x00000800, AX
	SUBQ   $0x00000100, CX
	JMP    uint64SumBlockLoop

uint64SumTailLoop:
	CMPQ   CX, $0x00000008
	JL     uint64SumDone
	VPADDQ (AX), Z0, Z0
	ADDQ   $0x00000040, AX
	SUBQ   $0x00000008, CX
	JMP    uint64SumTailLoop

uint64SumDone:
	VPADDQ        Z1, Z0, Z0
	VPADDQ        Z3, Z2, Z2
	VPADDQ        Z5, Z4, Z4
	VPADDQ        Z7, Z6, Z6
	VPADDQ        Z9, Z8, Z8
	VPADDQ        Z11, Z10, Z10
	VPADDQ        Z13, Z12, Z12
	VPADDQ        Z15, Z14, Z14
	VPADDQ        Z17, Z16, Z16
	VPADDQ        Z19, Z18, Z18
	VPADDQ        Z21, Z20, Z20
	VPADDQ        Z23, Z22, Z22
	VPADDQ        Z25, Z24, Z24
	VPADDQ        Z27, Z26, Z26
	VPADDQ        Z29, Z28, Z28
	VPADDQ        Z31, Z30, Z30
	VPADDQ        Z2, Z0, Z0
	VPADDQ        Z6, Z4, Z4
	VPADDQ        Z10, Z8, Z8
	VPADDQ        Z14, Z12, Z12
	VPADDQ        Z18, Z16, Z16
	VPADDQ        Z22, Z20, Z20
	VPADDQ        Z26, Z24, Z24
	VPADDQ        Z30, Z28, Z28
	VPADDQ        Z4, Z0, Z0
	VPADDQ        Z12, Z8, Z8
	VPADDQ        Z20, Z16, Z16
	VPADDQ        Z28, Z24, Z24
	VPADDQ        Z8, Z0, Z0
	VPADDQ        Z24, Z16, Z16
	VPADDQ        Z16, Z0, Z0
	VEXTRACTI64X4 $0x01, Z0, Y1
	VPADDQ        Y1, Y0, Y0
	CMPQ          CX, $0x04
	JL            uint64SumDone1
	VPADDQ        (AX), Y0, Y0

uint64SumDone1:
	VEXTRACTI128 $0x01, Y0, X1
	VPSHUFD      $0x4e, X0, X1
	VPADDQ       X1, X0, X0
	MOVQ         X0, ret+24(FP)
	RET

// func float32SumAvx512Asm(x []float32) float32
// Requires: AVX, AVX512DQ, AVX512F, SSE
TEXT ·float32SumAvx512Asm(SB), NOSPLIT, $0-28
	MOVQ   x_base+0(FP), AX
	MOVQ   x_len+8(FP), CX
	VPXORQ Z0, Z0, Z0
	VPXORQ Z1, Z1, Z1
	VPXORQ Z2, Z2, Z2
	VPXORQ Z3, Z3, Z3
	VPXORQ Z4, Z4, Z4
	VPXORQ Z5, Z5, Z5
	VPXORQ Z6, Z6, Z6
	VPXORQ Z7, Z7, Z7
	VPXORQ Z8, Z8, Z8
	VPXORQ Z9, Z9, Z9
	VPXORQ Z10, Z10, Z10
	VPXORQ Z11, Z11, Z11
	VPXORQ Z12, Z12, Z12
	VPXORQ Z13, Z13, Z13
	VPXORQ Z14, Z14, Z14
	VPXORQ Z15, Z15, Z15
	VPXORQ Z16, Z16, Z16
	VPXORQ Z17, Z17, Z17
	VPXORQ Z18, Z18, Z18
	VPXORQ Z19, Z19, Z19
	VPXORQ Z20, Z20, Z20
	VPXORQ Z21, Z21, Z21
	VPXORQ Z22, Z22, Z22
	VPXORQ Z23, Z23, Z23
	VPXORQ Z24, Z24, Z24
	VPXORQ Z25, Z25, Z25
	VPXORQ Z26, Z26, Z26
	VPXORQ Z27, Z27, Z27
	VPXORQ Z28, Z28, Z28
	VPXORQ Z29, Z29, Z29
	VPXORQ Z30, Z30, Z30
	VPXORQ Z31, Z31, Z31

float32SumBlockLoop:
	CMPQ   CX, $0x00000200
	JL     float32SumTailLoop
	VADDPS (AX), Z0, Z0
	VADDPS 64(AX), Z1, Z1
	VADDPS 128(AX), Z2, Z2
	VADDPS 192(AX), Z3, Z3
	VADDPS 256(AX), Z4, Z4
	VADDPS 320(AX), Z5, Z5
	VADDPS 384(AX), Z6, Z6
	VADDPS 448(AX), Z7, Z7
	VADDPS 512(AX), Z8, Z8
	VADDPS 576(AX), Z9, Z9
	VADDPS 640(AX), Z10, Z10
	VADDPS 704(AX), Z11, Z11
	VADDPS 768(AX), Z12, Z12
	VADDPS 832(AX), Z13, Z13
	VADDPS 896(AX), Z14, Z14
	VADDPS 960(AX), Z15, Z15
	VADDPS 1024(AX), Z16, Z16
	VADDPS 1088(AX), Z17, Z17
	VADDPS 1152(AX), Z18, Z18
	VADDPS 1216(AX), Z19, Z19
	VADDPS 1280(AX), Z20, Z20
	VADDPS 1344(AX), Z21, Z21
	VADDPS 1408(AX), Z22, Z22
	VADDPS 1472(AX), Z23, Z23
	VADDPS 1536(AX), Z24, Z24
	VADDPS 1600(AX), Z25, Z25
	VADDPS 1664(AX), Z26, Z26
	VADDPS 1728(AX), Z27, Z27
	VADDPS 1792(AX), Z28, Z28
	VADDPS 1856(AX), Z29, Z29
	VADDPS 1920(AX), Z30, Z30
	VADDPS 1984(AX), Z31, Z31
	ADDQ   $0x00000800, AX
	SUBQ   $0x00000200, CX
	JMP    float32SumBlockLoop

float32SumTailLoop:
	CMPQ   CX, $0x00000010
	JL     float32SumDone
	VADDPS (AX), Z0, Z0
	ADDQ   $0x00000040, AX
	SUBQ   $0x00000010, CX
	JMP    float32SumTailLoop

float32SumDone:
	VADDPS        Z0, Z1, Z0
	VADDPS        Z2, Z3, Z2
	VADDPS        Z4, Z5, Z4
	VADDPS        Z6, Z7, Z6
	VADDPS        Z8, Z9, Z8
	VADDPS        Z10, Z11, Z10
	VADDPS        Z12, Z13, Z12
	VADDPS        Z14, Z15, Z14
	VADDPS        Z16, Z17, Z16
	VADDPS        Z18, Z19, Z18
	VADDPS        Z20, Z21, Z20
	VADDPS        Z22, Z23, Z22
	VADDPS        Z24, Z25, Z24
	VADDPS        Z26, Z27, Z26
	VADDPS        Z28, Z29, Z28
	VADDPS        Z30, Z31, Z30
	VADDPS        Z0, Z2, Z0
	VADDPS        Z4, Z6, Z4
	VADDPS        Z8, Z10, Z8
	VADDPS        Z12, Z14, Z12
	VADDPS        Z16, Z18, Z16
	VADDPS        Z20, Z22, Z20
	VADDPS        Z24, Z26, Z24
	VADDPS        Z28, Z30, Z28
	VADDPS        Z0, Z4, Z0
	VADDPS        Z8, Z12, Z8
	VADDPS        Z16, Z20, Z16
	VADDPS        Z24, Z28, Z24
	VADDPS        Z0, Z8, Z0
	VEXTRACTF32X8 $0x01, Z0, Y1
	VADDPS        Y1, Y0, Y0
	CMPQ          CX, $0x08
	JL            float32SumDone1
	VADDPS        (AX), Y0, Y0
	ADDQ          $0x20, AX
	SUBQ          $0x08, CX

float32SumDone1:
	VEXTRACTF128 $0x01, Y0, X1
	VADDPS       X1, X0, X0
	CMPQ         CX, $0x04
	JL           float32SumDone2
	VADDPS       (AX), X0, X0

float32SumDone2:
	VHADDPS X0, X0, X0
	VHADDPS X0, X0, X0
	MOVSS   X0, ret+24(FP)
	RET

// func float64SumAvx512Asm(x []float64) float64
// Requires: AVX, AVX512F, SSE2
TEXT ·float64SumAvx512Asm(SB), NOSPLIT, $0-32
	MOVQ   x_base+0(FP), AX
	MOVQ   x_len+8(FP), CX
	VPXORQ Z0, Z0, Z0
	VPXORQ Z1, Z1, Z1
	VPXORQ Z2, Z2, Z2
	VPXORQ Z3, Z3, Z3
	VPXORQ Z4, Z4, Z4
	VPXORQ Z5, Z5, Z5
	VPXORQ Z6, Z6, Z6
	VPXORQ Z7, Z7, Z7
	VPXORQ Z8, Z8, Z8
	VPXORQ Z9, Z9, Z9
	VPXORQ Z10, Z10, Z10
	VPXORQ Z11, Z11, Z11
	VPXORQ Z12, Z12, Z12
	VPXORQ Z13, Z13, Z13
	VPXORQ Z14, Z14, Z14
	VPXORQ Z15, Z15, Z15
	VPXORQ Z16, Z16, Z16
	VPXORQ Z17, Z17, Z17
	VPXORQ Z18, Z18, Z18
	VPXORQ Z19, Z19, Z19
	VPXORQ Z20, Z20, Z20
	VPXORQ Z21, Z21, Z21
	VPXORQ Z22, Z22, Z22
	VPXORQ Z23, Z23, Z23
	VPXORQ Z24, Z24, Z24
	VPXORQ Z25, Z25, Z25
	VPXORQ Z26, Z26, Z26
	VPXORQ Z27, Z27, Z27
	VPXORQ Z28, Z28, Z28
	VPXORQ Z29, Z29, Z29
	VPXORQ Z30, Z30, Z30
	VPXORQ Z31, Z31, Z31

float64SumBlockLoop:
	CMPQ   CX, $0x00000100
	JL     float64SumTailLoop
	VADDPD (AX), Z0, Z0
	VADDPD 64(AX), Z1, Z1
	VADDPD 128(AX), Z2, Z2
	VADDPD 192(AX), Z3, Z3
	VADDPD 256(AX), Z4, Z4
	VADDPD 320(AX), Z5, Z5
	VADDPD 384(AX), Z6, Z6
	VADDPD 448(AX), Z7, Z7
	VADDPD 512(AX), Z8, Z8
	VADDPD 576(AX), Z9, Z9
	VADDPD 640(AX), Z10, Z10
	VADDPD 704(AX), Z11, Z11
	VADDPD 768(AX), Z12, Z12
	VADDPD 832(AX), Z13, Z13
	VADDPD 896(AX), Z14, Z14
	VADDPD 960(AX), Z15, Z15
	VADDPD 1024(AX), Z16, Z16
	VADDPD 1088(AX), Z17, Z17
	VADDPD 1152(AX), Z18, Z18
	VADDPD 1216(AX), Z19, Z19
	VADDPD 1280(AX), Z20, Z20
	VADDPD 1344(AX), Z21, Z21
	VADDPD 1408(AX), Z22, Z22
	VADDPD 1472(AX), Z23, Z23
	VADDPD 1536(AX), Z24, Z24
	VADDPD 1600(AX), Z25, Z25
	VADDPD 1664(AX), Z26, Z26
	VADDPD 1728(AX), Z27, Z27
	VADDPD 1792(AX), Z28, Z28
	VADDPD 1856(AX), Z29, Z29
	VADDPD 1920(AX), Z30, Z30
	VADDPD 1984(AX), Z31, Z31
	ADDQ   $0x00000800, AX
	SUBQ   $0x00000100, CX
	JMP    float64SumBlockLoop

float64SumTailLoop:
	CMPQ   CX, $0x00000008
	JL     float64SumDone
	VADDPD (AX), Z0, Z0
	ADDQ   $0x00000040, AX
	SUBQ   $0x00000008, CX
	JMP    float64SumTailLoop

float64SumDone:
	VADDPD        Z0, Z1, Z0
	VADDPD        Z2, Z3, Z2
	VADDPD        Z4, Z5, Z4
	VADDPD        Z6, Z7, Z6
	VADDPD        Z8, Z9, Z8
	VADDPD        Z10, Z11, Z10
	VADDPD        Z12, Z13, Z12
	VADDPD        Z14, Z15, Z14
	VADDPD        Z16, Z17, Z16
	VADDPD        Z18, Z19, Z18
	VADDPD        Z20, Z21, Z20
	VADDPD        Z22, Z23, Z22
	VADDPD        Z24, Z25, Z24
	VADDPD        Z26, Z27, Z26
	VADDPD        Z28, Z29, Z28
	VADDPD        Z30, Z31, Z30
	VADDPD        Z0, Z2, Z0
	VADDPD        Z4, Z6, Z4
	VADDPD        Z8, Z10, Z8
	VADDPD        Z12, Z14, Z12
	VADDPD        Z16, Z18, Z16
	VADDPD        Z20, Z22, Z20
	VADDPD        Z24, Z26, Z24
	VADDPD        Z28, Z30, Z28
	VADDPD        Z0, Z4, Z0
	VADDPD        Z8, Z12, Z8
	VADDPD        Z16, Z20, Z16
	VADDPD        Z24, Z28, Z24
	VADDPD        Z0, Z8, Z0
	VEXTRACTI64X4 $0x01, Z0, Y1
	VADDPD        Y1, Y0, Y0
	CMPQ          CX, $0x04
	JL            float64SumDone1
	VADDPD        (AX), Y0, Y0

float64SumDone1:
	VEXTRACTF128 $0x01, Y0, X1
	VADDPD       X1, X0, X0
	VHADDPD      X0, X0, X0
	MOVSD        X0, ret+24(FP)
	RET
