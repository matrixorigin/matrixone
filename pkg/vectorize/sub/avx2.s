// Code generated by command: go run avx2.go -out sub/avx2.s -stubs sub/avx2_stubs.go. DO NOT EDIT.

#include "textflag.h"

// func int8SubAvx2Asm(x []int8, y []int8, r []int8)
// Requires: AVX, AVX2
TEXT ·int8SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int8SubBlockLoop:
	CMPQ    BX, $0x000000c0
	JL      int8SubTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VPSUBB  (CX), Y0, Y0
	VPSUBB  32(CX), Y1, Y1
	VPSUBB  64(CX), Y2, Y2
	VPSUBB  96(CX), Y3, Y3
	VPSUBB  128(CX), Y4, Y4
	VPSUBB  160(CX), Y5, Y5
	VMOVDQU Y0, (DX)
	VMOVDQU Y1, 32(DX)
	VMOVDQU Y2, 64(DX)
	VMOVDQU Y3, 96(DX)
	VMOVDQU Y4, 128(DX)
	VMOVDQU Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x000000c0, BX
	JMP     int8SubBlockLoop

int8SubTailLoop:
	CMPQ    BX, $0x00000020
	JL      int8SubDone
	VMOVDQU (AX), Y0
	VPSUBB  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000020, BX
	JMP     int8SubTailLoop

int8SubDone:
	RET

// func int8SubScalarAvx2Asm(x int8, y []int8, r []int8)
// Requires: AVX, AVX2, SSE2
TEXT ·int8SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVBLSX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTB X0, Y0

int8SubScalarBlockLoop:
	CMPQ    BX, $0x000000c0
	JL      int8SubScalarTailLoop
	VPSUBB  (CX), Y0, Y1
	VPSUBB  32(CX), Y0, Y2
	VPSUBB  64(CX), Y0, Y3
	VPSUBB  96(CX), Y0, Y4
	VPSUBB  128(CX), Y0, Y5
	VPSUBB  160(CX), Y0, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x000000c0, BX
	JMP     int8SubScalarBlockLoop

int8SubScalarTailLoop:
	CMPQ    BX, $0x00000020
	JL      int8SubScalarDone
	VPSUBB  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000020, BX
	JMP     int8SubScalarTailLoop

int8SubScalarDone:
	RET

// func int8SubByScalarAvx2Asm(x int8, y []int8, r []int8)
// Requires: AVX, AVX2, SSE2
TEXT ·int8SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVBLSX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTB X0, Y0

int8SubByScalarBlockLoop:
	CMPQ    BX, $0x000000c0
	JL      int8SubByScalarTailLoop
	VMOVDQU (CX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 64(CX), Y3
	VMOVDQU 96(CX), Y4
	VMOVDQU 128(CX), Y5
	VMOVDQU 160(CX), Y6
	VPSUBB  Y0, Y1, Y1
	VPSUBB  Y0, Y2, Y2
	VPSUBB  Y0, Y3, Y3
	VPSUBB  Y0, Y4, Y4
	VPSUBB  Y0, Y5, Y5
	VPSUBB  Y0, Y6, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x000000c0, BX
	JMP     int8SubByScalarBlockLoop

int8SubByScalarTailLoop:
	CMPQ    BX, $0x00000020
	JL      int8SubByScalarDone
	VMOVDQU (CX), Y1
	VPSUBB  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000020, BX
	JMP     int8SubByScalarTailLoop

int8SubByScalarDone:
	RET

// func int16SubAvx2Asm(x []int16, y []int16, r []int16)
// Requires: AVX, AVX2
TEXT ·int16SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int16SubBlockLoop:
	CMPQ    BX, $0x00000060
	JL      int16SubTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VPSUBW  (CX), Y0, Y0
	VPSUBW  32(CX), Y1, Y1
	VPSUBW  64(CX), Y2, Y2
	VPSUBW  96(CX), Y3, Y3
	VPSUBW  128(CX), Y4, Y4
	VPSUBW  160(CX), Y5, Y5
	VMOVDQU Y0, (DX)
	VMOVDQU Y1, 32(DX)
	VMOVDQU Y2, 64(DX)
	VMOVDQU Y3, 96(DX)
	VMOVDQU Y4, 128(DX)
	VMOVDQU Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000060, BX
	JMP     int16SubBlockLoop

int16SubTailLoop:
	CMPQ    BX, $0x00000010
	JL      int16SubDone
	VMOVDQU (AX), Y0
	VPSUBW  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000010, BX
	JMP     int16SubTailLoop

int16SubDone:
	RET

// func int16SubScalarAvx2Asm(x int16, y []int16, r []int16)
// Requires: AVX, AVX2, SSE2
TEXT ·int16SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVWLSX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTW X0, Y0

int16SubScalarBlockLoop:
	CMPQ    BX, $0x00000060
	JL      int16SubScalarTailLoop
	VPSUBW  (CX), Y0, Y1
	VPSUBW  32(CX), Y0, Y2
	VPSUBW  64(CX), Y0, Y3
	VPSUBW  96(CX), Y0, Y4
	VPSUBW  128(CX), Y0, Y5
	VPSUBW  160(CX), Y0, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000060, BX
	JMP     int16SubScalarBlockLoop

int16SubScalarTailLoop:
	CMPQ    BX, $0x00000010
	JL      int16SubScalarDone
	VPSUBW  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000010, BX
	JMP     int16SubScalarTailLoop

int16SubScalarDone:
	RET

// func int16SubByScalarAvx2Asm(x int16, y []int16, r []int16)
// Requires: AVX, AVX2, SSE2
TEXT ·int16SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVWLSX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTW X0, Y0

int16SubByScalarBlockLoop:
	CMPQ    BX, $0x00000060
	JL      int16SubByScalarTailLoop
	VMOVDQU (CX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 64(CX), Y3
	VMOVDQU 96(CX), Y4
	VMOVDQU 128(CX), Y5
	VMOVDQU 160(CX), Y6
	VPSUBW  Y0, Y1, Y1
	VPSUBW  Y0, Y2, Y2
	VPSUBW  Y0, Y3, Y3
	VPSUBW  Y0, Y4, Y4
	VPSUBW  Y0, Y5, Y5
	VPSUBW  Y0, Y6, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000060, BX
	JMP     int16SubByScalarBlockLoop

int16SubByScalarTailLoop:
	CMPQ    BX, $0x00000010
	JL      int16SubByScalarDone
	VMOVDQU (CX), Y1
	VPSUBW  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000010, BX
	JMP     int16SubByScalarTailLoop

int16SubByScalarDone:
	RET

// func int32SubAvx2Asm(x []int32, y []int32, r []int32)
// Requires: AVX, AVX2
TEXT ·int32SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int32SubBlockLoop:
	CMPQ    BX, $0x00000030
	JL      int32SubTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VPSUBD  (CX), Y0, Y0
	VPSUBD  32(CX), Y1, Y1
	VPSUBD  64(CX), Y2, Y2
	VPSUBD  96(CX), Y3, Y3
	VPSUBD  128(CX), Y4, Y4
	VPSUBD  160(CX), Y5, Y5
	VMOVDQU Y0, (DX)
	VMOVDQU Y1, 32(DX)
	VMOVDQU Y2, 64(DX)
	VMOVDQU Y3, 96(DX)
	VMOVDQU Y4, 128(DX)
	VMOVDQU Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000030, BX
	JMP     int32SubBlockLoop

int32SubTailLoop:
	CMPQ    BX, $0x00000008
	JL      int32SubDone
	VMOVDQU (AX), Y0
	VPSUBD  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     int32SubTailLoop

int32SubDone:
	RET

// func int32SubScalarAvx2Asm(x int32, y []int32, r []int32)
// Requires: AVX, AVX2, SSE2
TEXT ·int32SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVL         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTD X0, Y0

int32SubScalarBlockLoop:
	CMPQ    BX, $0x00000030
	JL      int32SubScalarTailLoop
	VPSUBD  (CX), Y0, Y1
	VPSUBD  32(CX), Y0, Y2
	VPSUBD  64(CX), Y0, Y3
	VPSUBD  96(CX), Y0, Y4
	VPSUBD  128(CX), Y0, Y5
	VPSUBD  160(CX), Y0, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000030, BX
	JMP     int32SubScalarBlockLoop

int32SubScalarTailLoop:
	CMPQ    BX, $0x00000008
	JL      int32SubScalarDone
	VPSUBD  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     int32SubScalarTailLoop

int32SubScalarDone:
	RET

// func int32SubByScalarAvx2Asm(x int32, y []int32, r []int32)
// Requires: AVX, AVX2, SSE2
TEXT ·int32SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVL         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTD X0, Y0

int32SubByScalarBlockLoop:
	CMPQ    BX, $0x00000030
	JL      int32SubByScalarTailLoop
	VMOVDQU (CX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 64(CX), Y3
	VMOVDQU 96(CX), Y4
	VMOVDQU 128(CX), Y5
	VMOVDQU 160(CX), Y6
	VPSUBD  Y0, Y1, Y1
	VPSUBD  Y0, Y2, Y2
	VPSUBD  Y0, Y3, Y3
	VPSUBD  Y0, Y4, Y4
	VPSUBD  Y0, Y5, Y5
	VPSUBD  Y0, Y6, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000030, BX
	JMP     int32SubByScalarBlockLoop

int32SubByScalarTailLoop:
	CMPQ    BX, $0x00000008
	JL      int32SubByScalarDone
	VMOVDQU (CX), Y1
	VPSUBD  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     int32SubByScalarTailLoop

int32SubByScalarDone:
	RET

// func int64SubAvx2Asm(x []int64, y []int64, r []int64)
// Requires: AVX, AVX2
TEXT ·int64SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int64SubBlockLoop:
	CMPQ    BX, $0x00000018
	JL      int64SubTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VPSUBQ  (CX), Y0, Y0
	VPSUBQ  32(CX), Y1, Y1
	VPSUBQ  64(CX), Y2, Y2
	VPSUBQ  96(CX), Y3, Y3
	VPSUBQ  128(CX), Y4, Y4
	VPSUBQ  160(CX), Y5, Y5
	VMOVDQU Y0, (DX)
	VMOVDQU Y1, 32(DX)
	VMOVDQU Y2, 64(DX)
	VMOVDQU Y3, 96(DX)
	VMOVDQU Y4, 128(DX)
	VMOVDQU Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000018, BX
	JMP     int64SubBlockLoop

int64SubTailLoop:
	CMPQ    BX, $0x00000004
	JL      int64SubDone
	VMOVDQU (AX), Y0
	VPSUBQ  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     int64SubTailLoop

int64SubDone:
	RET

// func int64SubScalarAvx2Asm(x int64, y []int64, r []int64)
// Requires: AVX, AVX2, SSE2
TEXT ·int64SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVQ         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVQ         AX, X0
	VPBROADCASTQ X0, Y0

int64SubScalarBlockLoop:
	CMPQ    BX, $0x00000018
	JL      int64SubScalarTailLoop
	VPSUBQ  (CX), Y0, Y1
	VPSUBQ  32(CX), Y0, Y2
	VPSUBQ  64(CX), Y0, Y3
	VPSUBQ  96(CX), Y0, Y4
	VPSUBQ  128(CX), Y0, Y5
	VPSUBQ  160(CX), Y0, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000018, BX
	JMP     int64SubScalarBlockLoop

int64SubScalarTailLoop:
	CMPQ    BX, $0x00000004
	JL      int64SubScalarDone
	VPSUBQ  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     int64SubScalarTailLoop

int64SubScalarDone:
	RET

// func int64SubByScalarAvx2Asm(x int64, y []int64, r []int64)
// Requires: AVX, AVX2, SSE2
TEXT ·int64SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVQ         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVQ         AX, X0
	VPBROADCASTQ X0, Y0

int64SubByScalarBlockLoop:
	CMPQ    BX, $0x00000018
	JL      int64SubByScalarTailLoop
	VMOVDQU (CX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 64(CX), Y3
	VMOVDQU 96(CX), Y4
	VMOVDQU 128(CX), Y5
	VMOVDQU 160(CX), Y6
	VPSUBQ  Y0, Y1, Y1
	VPSUBQ  Y0, Y2, Y2
	VPSUBQ  Y0, Y3, Y3
	VPSUBQ  Y0, Y4, Y4
	VPSUBQ  Y0, Y5, Y5
	VPSUBQ  Y0, Y6, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000018, BX
	JMP     int64SubByScalarBlockLoop

int64SubByScalarTailLoop:
	CMPQ    BX, $0x00000004
	JL      int64SubByScalarDone
	VMOVDQU (CX), Y1
	VPSUBQ  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     int64SubByScalarTailLoop

int64SubByScalarDone:
	RET

// func uint8SubAvx2Asm(x []uint8, y []uint8, r []uint8)
// Requires: AVX, AVX2
TEXT ·uint8SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint8SubBlockLoop:
	CMPQ    BX, $0x000000c0
	JL      uint8SubTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VPSUBB  (CX), Y0, Y0
	VPSUBB  32(CX), Y1, Y1
	VPSUBB  64(CX), Y2, Y2
	VPSUBB  96(CX), Y3, Y3
	VPSUBB  128(CX), Y4, Y4
	VPSUBB  160(CX), Y5, Y5
	VMOVDQU Y0, (DX)
	VMOVDQU Y1, 32(DX)
	VMOVDQU Y2, 64(DX)
	VMOVDQU Y3, 96(DX)
	VMOVDQU Y4, 128(DX)
	VMOVDQU Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x000000c0, BX
	JMP     uint8SubBlockLoop

uint8SubTailLoop:
	CMPQ    BX, $0x00000020
	JL      uint8SubDone
	VMOVDQU (AX), Y0
	VPSUBB  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000020, BX
	JMP     uint8SubTailLoop

uint8SubDone:
	RET

// func uint8SubScalarAvx2Asm(x uint8, y []uint8, r []uint8)
// Requires: AVX, AVX2, SSE2
TEXT ·uint8SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVBLZX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTB X0, Y0

uint8SubScalarBlockLoop:
	CMPQ    BX, $0x000000c0
	JL      uint8SubScalarTailLoop
	VPSUBB  (CX), Y0, Y1
	VPSUBB  32(CX), Y0, Y2
	VPSUBB  64(CX), Y0, Y3
	VPSUBB  96(CX), Y0, Y4
	VPSUBB  128(CX), Y0, Y5
	VPSUBB  160(CX), Y0, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x000000c0, BX
	JMP     uint8SubScalarBlockLoop

uint8SubScalarTailLoop:
	CMPQ    BX, $0x00000020
	JL      uint8SubScalarDone
	VPSUBB  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000020, BX
	JMP     uint8SubScalarTailLoop

uint8SubScalarDone:
	RET

// func uint8SubByScalarAvx2Asm(x uint8, y []uint8, r []uint8)
// Requires: AVX, AVX2, SSE2
TEXT ·uint8SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVBLZX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTB X0, Y0

uint8SubByScalarBlockLoop:
	CMPQ    BX, $0x000000c0
	JL      uint8SubByScalarTailLoop
	VMOVDQU (CX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 64(CX), Y3
	VMOVDQU 96(CX), Y4
	VMOVDQU 128(CX), Y5
	VMOVDQU 160(CX), Y6
	VPSUBB  Y0, Y1, Y1
	VPSUBB  Y0, Y2, Y2
	VPSUBB  Y0, Y3, Y3
	VPSUBB  Y0, Y4, Y4
	VPSUBB  Y0, Y5, Y5
	VPSUBB  Y0, Y6, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x000000c0, BX
	JMP     uint8SubByScalarBlockLoop

uint8SubByScalarTailLoop:
	CMPQ    BX, $0x00000020
	JL      uint8SubByScalarDone
	VMOVDQU (CX), Y1
	VPSUBB  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000020, BX
	JMP     uint8SubByScalarTailLoop

uint8SubByScalarDone:
	RET

// func uint16SubAvx2Asm(x []uint16, y []uint16, r []uint16)
// Requires: AVX, AVX2
TEXT ·uint16SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint16SubBlockLoop:
	CMPQ    BX, $0x00000060
	JL      uint16SubTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VPSUBW  (CX), Y0, Y0
	VPSUBW  32(CX), Y1, Y1
	VPSUBW  64(CX), Y2, Y2
	VPSUBW  96(CX), Y3, Y3
	VPSUBW  128(CX), Y4, Y4
	VPSUBW  160(CX), Y5, Y5
	VMOVDQU Y0, (DX)
	VMOVDQU Y1, 32(DX)
	VMOVDQU Y2, 64(DX)
	VMOVDQU Y3, 96(DX)
	VMOVDQU Y4, 128(DX)
	VMOVDQU Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000060, BX
	JMP     uint16SubBlockLoop

uint16SubTailLoop:
	CMPQ    BX, $0x00000010
	JL      uint16SubDone
	VMOVDQU (AX), Y0
	VPSUBW  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000010, BX
	JMP     uint16SubTailLoop

uint16SubDone:
	RET

// func uint16SubScalarAvx2Asm(x uint16, y []uint16, r []uint16)
// Requires: AVX, AVX2, SSE2
TEXT ·uint16SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVWLZX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTW X0, Y0

uint16SubScalarBlockLoop:
	CMPQ    BX, $0x00000060
	JL      uint16SubScalarTailLoop
	VPSUBW  (CX), Y0, Y1
	VPSUBW  32(CX), Y0, Y2
	VPSUBW  64(CX), Y0, Y3
	VPSUBW  96(CX), Y0, Y4
	VPSUBW  128(CX), Y0, Y5
	VPSUBW  160(CX), Y0, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000060, BX
	JMP     uint16SubScalarBlockLoop

uint16SubScalarTailLoop:
	CMPQ    BX, $0x00000010
	JL      uint16SubScalarDone
	VPSUBW  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000010, BX
	JMP     uint16SubScalarTailLoop

uint16SubScalarDone:
	RET

// func uint16SubByScalarAvx2Asm(x uint16, y []uint16, r []uint16)
// Requires: AVX, AVX2, SSE2
TEXT ·uint16SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVWLZX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTW X0, Y0

uint16SubByScalarBlockLoop:
	CMPQ    BX, $0x00000060
	JL      uint16SubByScalarTailLoop
	VMOVDQU (CX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 64(CX), Y3
	VMOVDQU 96(CX), Y4
	VMOVDQU 128(CX), Y5
	VMOVDQU 160(CX), Y6
	VPSUBW  Y0, Y1, Y1
	VPSUBW  Y0, Y2, Y2
	VPSUBW  Y0, Y3, Y3
	VPSUBW  Y0, Y4, Y4
	VPSUBW  Y0, Y5, Y5
	VPSUBW  Y0, Y6, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000060, BX
	JMP     uint16SubByScalarBlockLoop

uint16SubByScalarTailLoop:
	CMPQ    BX, $0x00000010
	JL      uint16SubByScalarDone
	VMOVDQU (CX), Y1
	VPSUBW  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000010, BX
	JMP     uint16SubByScalarTailLoop

uint16SubByScalarDone:
	RET

// func uint32SubAvx2Asm(x []uint32, y []uint32, r []uint32)
// Requires: AVX, AVX2
TEXT ·uint32SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint32SubBlockLoop:
	CMPQ    BX, $0x00000030
	JL      uint32SubTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VPSUBD  (CX), Y0, Y0
	VPSUBD  32(CX), Y1, Y1
	VPSUBD  64(CX), Y2, Y2
	VPSUBD  96(CX), Y3, Y3
	VPSUBD  128(CX), Y4, Y4
	VPSUBD  160(CX), Y5, Y5
	VMOVDQU Y0, (DX)
	VMOVDQU Y1, 32(DX)
	VMOVDQU Y2, 64(DX)
	VMOVDQU Y3, 96(DX)
	VMOVDQU Y4, 128(DX)
	VMOVDQU Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000030, BX
	JMP     uint32SubBlockLoop

uint32SubTailLoop:
	CMPQ    BX, $0x00000008
	JL      uint32SubDone
	VMOVDQU (AX), Y0
	VPSUBD  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     uint32SubTailLoop

uint32SubDone:
	RET

// func uint32SubScalarAvx2Asm(x uint32, y []uint32, r []uint32)
// Requires: AVX, AVX2, SSE2
TEXT ·uint32SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVL         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTD X0, Y0

uint32SubScalarBlockLoop:
	CMPQ    BX, $0x00000030
	JL      uint32SubScalarTailLoop
	VPSUBD  (CX), Y0, Y1
	VPSUBD  32(CX), Y0, Y2
	VPSUBD  64(CX), Y0, Y3
	VPSUBD  96(CX), Y0, Y4
	VPSUBD  128(CX), Y0, Y5
	VPSUBD  160(CX), Y0, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000030, BX
	JMP     uint32SubScalarBlockLoop

uint32SubScalarTailLoop:
	CMPQ    BX, $0x00000008
	JL      uint32SubScalarDone
	VPSUBD  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     uint32SubScalarTailLoop

uint32SubScalarDone:
	RET

// func uint32SubByScalarAvx2Asm(x uint32, y []uint32, r []uint32)
// Requires: AVX, AVX2, SSE2
TEXT ·uint32SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVL         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTD X0, Y0

uint32SubByScalarBlockLoop:
	CMPQ    BX, $0x00000030
	JL      uint32SubByScalarTailLoop
	VMOVDQU (CX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 64(CX), Y3
	VMOVDQU 96(CX), Y4
	VMOVDQU 128(CX), Y5
	VMOVDQU 160(CX), Y6
	VPSUBD  Y0, Y1, Y1
	VPSUBD  Y0, Y2, Y2
	VPSUBD  Y0, Y3, Y3
	VPSUBD  Y0, Y4, Y4
	VPSUBD  Y0, Y5, Y5
	VPSUBD  Y0, Y6, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000030, BX
	JMP     uint32SubByScalarBlockLoop

uint32SubByScalarTailLoop:
	CMPQ    BX, $0x00000008
	JL      uint32SubByScalarDone
	VMOVDQU (CX), Y1
	VPSUBD  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     uint32SubByScalarTailLoop

uint32SubByScalarDone:
	RET

// func uint64SubAvx2Asm(x []uint64, y []uint64, r []uint64)
// Requires: AVX, AVX2
TEXT ·uint64SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint64SubBlockLoop:
	CMPQ    BX, $0x00000018
	JL      uint64SubTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VPSUBQ  (CX), Y0, Y0
	VPSUBQ  32(CX), Y1, Y1
	VPSUBQ  64(CX), Y2, Y2
	VPSUBQ  96(CX), Y3, Y3
	VPSUBQ  128(CX), Y4, Y4
	VPSUBQ  160(CX), Y5, Y5
	VMOVDQU Y0, (DX)
	VMOVDQU Y1, 32(DX)
	VMOVDQU Y2, 64(DX)
	VMOVDQU Y3, 96(DX)
	VMOVDQU Y4, 128(DX)
	VMOVDQU Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000018, BX
	JMP     uint64SubBlockLoop

uint64SubTailLoop:
	CMPQ    BX, $0x00000004
	JL      uint64SubDone
	VMOVDQU (AX), Y0
	VPSUBQ  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     uint64SubTailLoop

uint64SubDone:
	RET

// func uint64SubScalarAvx2Asm(x uint64, y []uint64, r []uint64)
// Requires: AVX, AVX2, SSE2
TEXT ·uint64SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVQ         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVQ         AX, X0
	VPBROADCASTQ X0, Y0

uint64SubScalarBlockLoop:
	CMPQ    BX, $0x00000018
	JL      uint64SubScalarTailLoop
	VPSUBQ  (CX), Y0, Y1
	VPSUBQ  32(CX), Y0, Y2
	VPSUBQ  64(CX), Y0, Y3
	VPSUBQ  96(CX), Y0, Y4
	VPSUBQ  128(CX), Y0, Y5
	VPSUBQ  160(CX), Y0, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000018, BX
	JMP     uint64SubScalarBlockLoop

uint64SubScalarTailLoop:
	CMPQ    BX, $0x00000004
	JL      uint64SubScalarDone
	VPSUBQ  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     uint64SubScalarTailLoop

uint64SubScalarDone:
	RET

// func uint64SubByScalarAvx2Asm(x uint64, y []uint64, r []uint64)
// Requires: AVX, AVX2, SSE2
TEXT ·uint64SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVQ         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVQ         AX, X0
	VPBROADCASTQ X0, Y0

uint64SubByScalarBlockLoop:
	CMPQ    BX, $0x00000018
	JL      uint64SubByScalarTailLoop
	VMOVDQU (CX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 64(CX), Y3
	VMOVDQU 96(CX), Y4
	VMOVDQU 128(CX), Y5
	VMOVDQU 160(CX), Y6
	VPSUBQ  Y0, Y1, Y1
	VPSUBQ  Y0, Y2, Y2
	VPSUBQ  Y0, Y3, Y3
	VPSUBQ  Y0, Y4, Y4
	VPSUBQ  Y0, Y5, Y5
	VPSUBQ  Y0, Y6, Y6
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000018, BX
	JMP     uint64SubByScalarBlockLoop

uint64SubByScalarTailLoop:
	CMPQ    BX, $0x00000004
	JL      uint64SubByScalarDone
	VMOVDQU (CX), Y1
	VPSUBQ  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     uint64SubByScalarTailLoop

uint64SubByScalarDone:
	RET

// func float32SubAvx2Asm(x []float32, y []float32, r []float32)
// Requires: AVX
TEXT ·float32SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

float32SubBlockLoop:
	CMPQ    BX, $0x00000030
	JL      float32SubTailLoop
	VMOVUPS (AX), Y0
	VMOVUPS 32(AX), Y1
	VMOVUPS 64(AX), Y2
	VMOVUPS 96(AX), Y3
	VMOVUPS 128(AX), Y4
	VMOVUPS 160(AX), Y5
	VSUBPS  (CX), Y0, Y0
	VSUBPS  32(CX), Y1, Y1
	VSUBPS  64(CX), Y2, Y2
	VSUBPS  96(CX), Y3, Y3
	VSUBPS  128(CX), Y4, Y4
	VSUBPS  160(CX), Y5, Y5
	VMOVUPS Y0, (DX)
	VMOVUPS Y1, 32(DX)
	VMOVUPS Y2, 64(DX)
	VMOVUPS Y3, 96(DX)
	VMOVUPS Y4, 128(DX)
	VMOVUPS Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000030, BX
	JMP     float32SubBlockLoop

float32SubTailLoop:
	CMPQ    BX, $0x00000008
	JL      float32SubDone
	VMOVUPS (AX), Y0
	VSUBPS  (CX), Y0, Y0
	VMOVUPS Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     float32SubTailLoop

float32SubDone:
	RET

// func float32SubScalarAvx2Asm(x float32, y []float32, r []float32)
// Requires: AVX, AVX2, SSE
TEXT ·float32SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVSS        x+0(FP), X0
	MOVQ         y_base+8(FP), AX
	MOVQ         r_base+32(FP), CX
	MOVQ         y_len+16(FP), DX
	VBROADCASTSS X0, Y0

float32SubScalarBlockLoop:
	CMPQ    DX, $0x00000030
	JL      float32SubScalarTailLoop
	VSUBPS  (AX), Y0, Y1
	VSUBPS  32(AX), Y0, Y2
	VSUBPS  64(AX), Y0, Y3
	VSUBPS  96(AX), Y0, Y4
	VSUBPS  128(AX), Y0, Y5
	VSUBPS  160(AX), Y0, Y6
	VMOVUPS Y1, (CX)
	VMOVUPS Y2, 32(CX)
	VMOVUPS Y3, 64(CX)
	VMOVUPS Y4, 96(CX)
	VMOVUPS Y5, 128(CX)
	VMOVUPS Y6, 160(CX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	SUBQ    $0x00000030, DX
	JMP     float32SubScalarBlockLoop

float32SubScalarTailLoop:
	CMPQ    DX, $0x00000008
	JL      float32SubScalarDone
	VSUBPS  (AX), Y0, Y1
	VMOVUPS Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000008, DX
	JMP     float32SubScalarTailLoop

float32SubScalarDone:
	RET

// func float32SubByScalarAvx2Asm(x float32, y []float32, r []float32)
// Requires: AVX, AVX2, SSE
TEXT ·float32SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVSS        x+0(FP), X0
	MOVQ         y_base+8(FP), AX
	MOVQ         r_base+32(FP), CX
	MOVQ         y_len+16(FP), DX
	VBROADCASTSS X0, Y0

float32SubByScalarBlockLoop:
	CMPQ    DX, $0x00000030
	JL      float32SubByScalarTailLoop
	VMOVUPS (AX), Y1
	VMOVUPS 32(AX), Y2
	VMOVUPS 64(AX), Y3
	VMOVUPS 96(AX), Y4
	VMOVUPS 128(AX), Y5
	VMOVUPS 160(AX), Y6
	VSUBPS  Y0, Y1, Y1
	VSUBPS  Y0, Y2, Y2
	VSUBPS  Y0, Y3, Y3
	VSUBPS  Y0, Y4, Y4
	VSUBPS  Y0, Y5, Y5
	VSUBPS  Y0, Y6, Y6
	VMOVUPS Y1, (CX)
	VMOVUPS Y2, 32(CX)
	VMOVUPS Y3, 64(CX)
	VMOVUPS Y4, 96(CX)
	VMOVUPS Y5, 128(CX)
	VMOVUPS Y6, 160(CX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	SUBQ    $0x00000030, DX
	JMP     float32SubByScalarBlockLoop

float32SubByScalarTailLoop:
	CMPQ    DX, $0x00000008
	JL      float32SubByScalarDone
	VMOVUPS (AX), Y1
	VSUBPS  Y0, Y1, Y1
	VMOVUPS Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000008, DX
	JMP     float32SubByScalarTailLoop

float32SubByScalarDone:
	RET

// func float64SubAvx2Asm(x []float64, y []float64, r []float64)
// Requires: AVX
TEXT ·float64SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

float64SubBlockLoop:
	CMPQ    BX, $0x00000018
	JL      float64SubTailLoop
	VMOVUPD (AX), Y0
	VMOVUPD 32(AX), Y1
	VMOVUPD 64(AX), Y2
	VMOVUPD 96(AX), Y3
	VMOVUPD 128(AX), Y4
	VMOVUPD 160(AX), Y5
	VSUBPD  (CX), Y0, Y0
	VSUBPD  32(CX), Y1, Y1
	VSUBPD  64(CX), Y2, Y2
	VSUBPD  96(CX), Y3, Y3
	VSUBPD  128(CX), Y4, Y4
	VSUBPD  160(CX), Y5, Y5
	VMOVUPD Y0, (DX)
	VMOVUPD Y1, 32(DX)
	VMOVUPD Y2, 64(DX)
	VMOVUPD Y3, 96(DX)
	VMOVUPD Y4, 128(DX)
	VMOVUPD Y5, 160(DX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	ADDQ    $0x000000c0, DX
	SUBQ    $0x00000018, BX
	JMP     float64SubBlockLoop

float64SubTailLoop:
	CMPQ    BX, $0x00000004
	JL      float64SubDone
	VMOVUPD (AX), Y0
	VSUBPD  (CX), Y0, Y0
	VMOVUPD Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     float64SubTailLoop

float64SubDone:
	RET

// func float64SubScalarAvx2Asm(x float64, y []float64, r []float64)
// Requires: AVX, AVX2, SSE2
TEXT ·float64SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVSD        x+0(FP), X0
	MOVQ         y_base+8(FP), AX
	MOVQ         r_base+32(FP), CX
	MOVQ         y_len+16(FP), DX
	VBROADCASTSD X0, Y0

float64SubScalarBlockLoop:
	CMPQ    DX, $0x00000018
	JL      float64SubScalarTailLoop
	VSUBPD  (AX), Y0, Y1
	VSUBPD  32(AX), Y0, Y2
	VSUBPD  64(AX), Y0, Y3
	VSUBPD  96(AX), Y0, Y4
	VSUBPD  128(AX), Y0, Y5
	VSUBPD  160(AX), Y0, Y6
	VMOVUPD Y1, (CX)
	VMOVUPD Y2, 32(CX)
	VMOVUPD Y3, 64(CX)
	VMOVUPD Y4, 96(CX)
	VMOVUPD Y5, 128(CX)
	VMOVUPD Y6, 160(CX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	SUBQ    $0x00000018, DX
	JMP     float64SubScalarBlockLoop

float64SubScalarTailLoop:
	CMPQ    DX, $0x00000004
	JL      float64SubScalarDone
	VSUBPD  (AX), Y0, Y1
	VMOVUPD Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000004, DX
	JMP     float64SubScalarTailLoop

float64SubScalarDone:
	RET

// func float64SubByScalarAvx2Asm(x float64, y []float64, r []float64)
// Requires: AVX, AVX2, SSE2
TEXT ·float64SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVSD        x+0(FP), X0
	MOVQ         y_base+8(FP), AX
	MOVQ         r_base+32(FP), CX
	MOVQ         y_len+16(FP), DX
	VBROADCASTSD X0, Y0

float64SubByScalarBlockLoop:
	CMPQ    DX, $0x00000018
	JL      float64SubByScalarTailLoop
	VMOVUPD (AX), Y1
	VMOVUPD 32(AX), Y2
	VMOVUPD 64(AX), Y3
	VMOVUPD 96(AX), Y4
	VMOVUPD 128(AX), Y5
	VMOVUPD 160(AX), Y6
	VSUBPD  Y0, Y1, Y1
	VSUBPD  Y0, Y2, Y2
	VSUBPD  Y0, Y3, Y3
	VSUBPD  Y0, Y4, Y4
	VSUBPD  Y0, Y5, Y5
	VSUBPD  Y0, Y6, Y6
	VMOVUPD Y1, (CX)
	VMOVUPD Y2, 32(CX)
	VMOVUPD Y3, 64(CX)
	VMOVUPD Y4, 96(CX)
	VMOVUPD Y5, 128(CX)
	VMOVUPD Y6, 160(CX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	SUBQ    $0x00000018, DX
	JMP     float64SubByScalarBlockLoop

float64SubByScalarTailLoop:
	CMPQ    DX, $0x00000004
	JL      float64SubByScalarDone
	VMOVUPD (AX), Y1
	VSUBPD  Y0, Y1, Y1
	VMOVUPD Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000004, DX
	JMP     float64SubByScalarTailLoop

float64SubByScalarDone:
	RET
