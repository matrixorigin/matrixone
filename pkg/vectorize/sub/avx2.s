// Code generated by command: go run avx2.go -out avx2.s -stubs avx2_stubs.go. DO NOT EDIT.

#include "textflag.h"

// func int8SubAvx2Asm(x []int8, y []int8, r []int8)
// Requires: AVX, AVX2
TEXT ·int8SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int8SubBlockLoop:
	CMPQ    BX, $0x00000200
	JL      int8SubTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VMOVDQU 192(AX), Y6
	VMOVDQU 224(AX), Y7
	VMOVDQU 256(AX), Y8
	VMOVDQU 288(AX), Y9
	VMOVDQU 320(AX), Y10
	VMOVDQU 352(AX), Y11
	VMOVDQU 384(AX), Y12
	VMOVDQU 416(AX), Y13
	VMOVDQU 448(AX), Y14
	VMOVDQU 480(AX), Y15
	VPSUBB  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	VPSUBB  32(CX), Y1, Y1
	VMOVDQU Y1, 32(DX)
	VPSUBB  64(CX), Y2, Y2
	VMOVDQU Y2, 64(DX)
	VPSUBB  96(CX), Y3, Y3
	VMOVDQU Y3, 96(DX)
	VPSUBB  128(CX), Y4, Y4
	VMOVDQU Y4, 128(DX)
	VPSUBB  160(CX), Y5, Y5
	VMOVDQU Y5, 160(DX)
	VPSUBB  192(CX), Y6, Y6
	VMOVDQU Y6, 192(DX)
	VPSUBB  224(CX), Y7, Y7
	VMOVDQU Y7, 224(DX)
	VPSUBB  256(CX), Y8, Y8
	VMOVDQU Y8, 256(DX)
	VPSUBB  288(CX), Y9, Y9
	VMOVDQU Y9, 288(DX)
	VPSUBB  320(CX), Y10, Y10
	VMOVDQU Y10, 320(DX)
	VPSUBB  352(CX), Y11, Y11
	VMOVDQU Y11, 352(DX)
	VPSUBB  384(CX), Y12, Y12
	VMOVDQU Y12, 384(DX)
	VPSUBB  416(CX), Y13, Y13
	VMOVDQU Y13, 416(DX)
	VPSUBB  448(CX), Y14, Y14
	VMOVDQU Y14, 448(DX)
	VPSUBB  480(CX), Y15, Y15
	VMOVDQU Y15, 480(DX)
	ADDQ    $0x00000200, AX
	ADDQ    $0x00000200, CX
	ADDQ    $0x00000200, DX
	SUBQ    $0x00000200, BX
	JMP     int8SubBlockLoop

int8SubTailLoop:
	CMPQ    BX, $0x00000020
	JL      int8SubDone
	VMOVDQU (AX), Y0
	VPSUBB  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000020, BX
	JMP     int8SubTailLoop

int8SubDone:
	CMPQ    BX, $0x00000010
	JL      int8SubDone1
	VMOVDQU (AX), X0
	VPSUBB  (CX), X0, X0
	VMOVDQU X0, (DX)

int8SubDone1:
	RET

// func int8SubScalarAvx2Asm(x int8, y []int8, r []int8)
// Requires: AVX, AVX2, SSE2
TEXT ·int8SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVBLSX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTB X0, Y0

int8SubScalarBlockLoop:
	CMPQ    BX, $0x000001e0
	JL      int8SubScalarTailLoop
	VPSUBB  (CX), Y0, Y1
	VPSUBB  32(CX), Y0, Y2
	VPSUBB  64(CX), Y0, Y3
	VPSUBB  96(CX), Y0, Y4
	VPSUBB  128(CX), Y0, Y5
	VPSUBB  160(CX), Y0, Y6
	VPSUBB  192(CX), Y0, Y7
	VPSUBB  224(CX), Y0, Y8
	VPSUBB  256(CX), Y0, Y9
	VPSUBB  288(CX), Y0, Y10
	VPSUBB  320(CX), Y0, Y11
	VPSUBB  352(CX), Y0, Y12
	VPSUBB  384(CX), Y0, Y13
	VPSUBB  416(CX), Y0, Y14
	VPSUBB  448(CX), Y0, Y15
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	VMOVDQU Y7, 192(DX)
	VMOVDQU Y8, 224(DX)
	VMOVDQU Y9, 256(DX)
	VMOVDQU Y10, 288(DX)
	VMOVDQU Y11, 320(DX)
	VMOVDQU Y12, 352(DX)
	VMOVDQU Y13, 384(DX)
	VMOVDQU Y14, 416(DX)
	VMOVDQU Y15, 448(DX)
	ADDQ    $0x000001e0, CX
	ADDQ    $0x000001e0, DX
	SUBQ    $0x000001e0, BX
	JMP     int8SubScalarBlockLoop

int8SubScalarTailLoop:
	CMPQ    BX, $0x00000020
	JL      int8SubScalarDone
	VPSUBB  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000020, BX
	JMP     int8SubScalarTailLoop

int8SubScalarDone:
	CMPQ    BX, $0x00000010
	JL      int8SubScalarDone1
	VPSUBB  (CX), X0, X1
	VMOVDQU X1, (DX)

int8SubScalarDone1:
	RET

// func int8SubByScalarAvx2Asm(x int8, y []int8, r []int8)
// Requires: AVX, AVX2, SSE2
TEXT ·int8SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVBLSX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTB X0, Y0

int8SubByScalarBlockLoop:
	CMPQ    BX, $0x000001e0
	JL      int8SubByScalarTailLoop
	VMOVDQU (CX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 64(CX), Y3
	VMOVDQU 96(CX), Y4
	VMOVDQU 128(CX), Y5
	VMOVDQU 160(CX), Y6
	VMOVDQU 192(CX), Y7
	VMOVDQU 224(CX), Y8
	VMOVDQU 256(CX), Y9
	VMOVDQU 288(CX), Y10
	VMOVDQU 320(CX), Y11
	VMOVDQU 352(CX), Y12
	VMOVDQU 384(CX), Y13
	VMOVDQU 416(CX), Y14
	VMOVDQU 448(CX), Y15
	VPSUBB  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	VPSUBB  Y0, Y2, Y2
	VMOVDQU Y2, 32(DX)
	VPSUBB  Y0, Y3, Y3
	VMOVDQU Y3, 64(DX)
	VPSUBB  Y0, Y4, Y4
	VMOVDQU Y4, 96(DX)
	VPSUBB  Y0, Y5, Y5
	VMOVDQU Y5, 128(DX)
	VPSUBB  Y0, Y6, Y6
	VMOVDQU Y6, 160(DX)
	VPSUBB  Y0, Y7, Y7
	VMOVDQU Y7, 192(DX)
	VPSUBB  Y0, Y8, Y8
	VMOVDQU Y8, 224(DX)
	VPSUBB  Y0, Y9, Y9
	VMOVDQU Y9, 256(DX)
	VPSUBB  Y0, Y10, Y10
	VMOVDQU Y10, 288(DX)
	VPSUBB  Y0, Y11, Y11
	VMOVDQU Y11, 320(DX)
	VPSUBB  Y0, Y12, Y12
	VMOVDQU Y12, 352(DX)
	VPSUBB  Y0, Y13, Y13
	VMOVDQU Y13, 384(DX)
	VPSUBB  Y0, Y14, Y14
	VMOVDQU Y14, 416(DX)
	VPSUBB  Y0, Y15, Y15
	VMOVDQU Y15, 448(DX)
	ADDQ    $0x000001e0, CX
	ADDQ    $0x000001e0, DX
	SUBQ    $0x000001e0, BX
	JMP     int8SubByScalarBlockLoop

int8SubByScalarTailLoop:
	CMPQ    BX, $0x00000020
	JL      int8SubByScalarDone
	VMOVDQU (CX), Y1
	VPSUBB  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000020, BX
	JMP     int8SubByScalarTailLoop

int8SubByScalarDone:
	CMPQ    BX, $0x00000010
	JL      int8SubByScalarDone1
	VMOVDQU (CX), X1
	VPSUBB  X0, X1, X1
	VMOVDQU X1, (DX)

int8SubByScalarDone1:
	RET

// func int16SubAvx2Asm(x []int16, y []int16, r []int16)
// Requires: AVX, AVX2
TEXT ·int16SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int16SubBlockLoop:
	CMPQ    BX, $0x00000100
	JL      int16SubTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VMOVDQU 192(AX), Y6
	VMOVDQU 224(AX), Y7
	VMOVDQU 256(AX), Y8
	VMOVDQU 288(AX), Y9
	VMOVDQU 320(AX), Y10
	VMOVDQU 352(AX), Y11
	VMOVDQU 384(AX), Y12
	VMOVDQU 416(AX), Y13
	VMOVDQU 448(AX), Y14
	VMOVDQU 480(AX), Y15
	VPSUBW  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	VPSUBW  32(CX), Y1, Y1
	VMOVDQU Y1, 32(DX)
	VPSUBW  64(CX), Y2, Y2
	VMOVDQU Y2, 64(DX)
	VPSUBW  96(CX), Y3, Y3
	VMOVDQU Y3, 96(DX)
	VPSUBW  128(CX), Y4, Y4
	VMOVDQU Y4, 128(DX)
	VPSUBW  160(CX), Y5, Y5
	VMOVDQU Y5, 160(DX)
	VPSUBW  192(CX), Y6, Y6
	VMOVDQU Y6, 192(DX)
	VPSUBW  224(CX), Y7, Y7
	VMOVDQU Y7, 224(DX)
	VPSUBW  256(CX), Y8, Y8
	VMOVDQU Y8, 256(DX)
	VPSUBW  288(CX), Y9, Y9
	VMOVDQU Y9, 288(DX)
	VPSUBW  320(CX), Y10, Y10
	VMOVDQU Y10, 320(DX)
	VPSUBW  352(CX), Y11, Y11
	VMOVDQU Y11, 352(DX)
	VPSUBW  384(CX), Y12, Y12
	VMOVDQU Y12, 384(DX)
	VPSUBW  416(CX), Y13, Y13
	VMOVDQU Y13, 416(DX)
	VPSUBW  448(CX), Y14, Y14
	VMOVDQU Y14, 448(DX)
	VPSUBW  480(CX), Y15, Y15
	VMOVDQU Y15, 480(DX)
	ADDQ    $0x00000200, AX
	ADDQ    $0x00000200, CX
	ADDQ    $0x00000200, DX
	SUBQ    $0x00000100, BX
	JMP     int16SubBlockLoop

int16SubTailLoop:
	CMPQ    BX, $0x00000010
	JL      int16SubDone
	VMOVDQU (AX), Y0
	VPSUBW  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000010, BX
	JMP     int16SubTailLoop

int16SubDone:
	CMPQ    BX, $0x00000008
	JL      int16SubDone1
	VMOVDQU (AX), X0
	VPSUBW  (CX), X0, X0
	VMOVDQU X0, (DX)

int16SubDone1:
	RET

// func int16SubScalarAvx2Asm(x int16, y []int16, r []int16)
// Requires: AVX, AVX2, SSE2
TEXT ·int16SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVWLSX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTW X0, Y0

int16SubScalarBlockLoop:
	CMPQ    BX, $0x000000f0
	JL      int16SubScalarTailLoop
	VPSUBW  (CX), Y0, Y1
	VPSUBW  32(CX), Y0, Y2
	VPSUBW  64(CX), Y0, Y3
	VPSUBW  96(CX), Y0, Y4
	VPSUBW  128(CX), Y0, Y5
	VPSUBW  160(CX), Y0, Y6
	VPSUBW  192(CX), Y0, Y7
	VPSUBW  224(CX), Y0, Y8
	VPSUBW  256(CX), Y0, Y9
	VPSUBW  288(CX), Y0, Y10
	VPSUBW  320(CX), Y0, Y11
	VPSUBW  352(CX), Y0, Y12
	VPSUBW  384(CX), Y0, Y13
	VPSUBW  416(CX), Y0, Y14
	VPSUBW  448(CX), Y0, Y15
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	VMOVDQU Y7, 192(DX)
	VMOVDQU Y8, 224(DX)
	VMOVDQU Y9, 256(DX)
	VMOVDQU Y10, 288(DX)
	VMOVDQU Y11, 320(DX)
	VMOVDQU Y12, 352(DX)
	VMOVDQU Y13, 384(DX)
	VMOVDQU Y14, 416(DX)
	VMOVDQU Y15, 448(DX)
	ADDQ    $0x000001e0, CX
	ADDQ    $0x000001e0, DX
	SUBQ    $0x000000f0, BX
	JMP     int16SubScalarBlockLoop

int16SubScalarTailLoop:
	CMPQ    BX, $0x00000010
	JL      int16SubScalarDone
	VPSUBW  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000010, BX
	JMP     int16SubScalarTailLoop

int16SubScalarDone:
	CMPQ    BX, $0x00000008
	JL      int16SubScalarDone1
	VPSUBW  (CX), X0, X1
	VMOVDQU X1, (DX)

int16SubScalarDone1:
	RET

// func int16SubByScalarAvx2Asm(x int16, y []int16, r []int16)
// Requires: AVX, AVX2, SSE2
TEXT ·int16SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVWLSX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTW X0, Y0

int16SubByScalarBlockLoop:
	CMPQ    BX, $0x000000f0
	JL      int16SubByScalarTailLoop
	VMOVDQU (CX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 64(CX), Y3
	VMOVDQU 96(CX), Y4
	VMOVDQU 128(CX), Y5
	VMOVDQU 160(CX), Y6
	VMOVDQU 192(CX), Y7
	VMOVDQU 224(CX), Y8
	VMOVDQU 256(CX), Y9
	VMOVDQU 288(CX), Y10
	VMOVDQU 320(CX), Y11
	VMOVDQU 352(CX), Y12
	VMOVDQU 384(CX), Y13
	VMOVDQU 416(CX), Y14
	VMOVDQU 448(CX), Y15
	VPSUBW  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	VPSUBW  Y0, Y2, Y2
	VMOVDQU Y2, 32(DX)
	VPSUBW  Y0, Y3, Y3
	VMOVDQU Y3, 64(DX)
	VPSUBW  Y0, Y4, Y4
	VMOVDQU Y4, 96(DX)
	VPSUBW  Y0, Y5, Y5
	VMOVDQU Y5, 128(DX)
	VPSUBW  Y0, Y6, Y6
	VMOVDQU Y6, 160(DX)
	VPSUBW  Y0, Y7, Y7
	VMOVDQU Y7, 192(DX)
	VPSUBW  Y0, Y8, Y8
	VMOVDQU Y8, 224(DX)
	VPSUBW  Y0, Y9, Y9
	VMOVDQU Y9, 256(DX)
	VPSUBW  Y0, Y10, Y10
	VMOVDQU Y10, 288(DX)
	VPSUBW  Y0, Y11, Y11
	VMOVDQU Y11, 320(DX)
	VPSUBW  Y0, Y12, Y12
	VMOVDQU Y12, 352(DX)
	VPSUBW  Y0, Y13, Y13
	VMOVDQU Y13, 384(DX)
	VPSUBW  Y0, Y14, Y14
	VMOVDQU Y14, 416(DX)
	VPSUBW  Y0, Y15, Y15
	VMOVDQU Y15, 448(DX)
	ADDQ    $0x000001e0, CX
	ADDQ    $0x000001e0, DX
	SUBQ    $0x000000f0, BX
	JMP     int16SubByScalarBlockLoop

int16SubByScalarTailLoop:
	CMPQ    BX, $0x00000010
	JL      int16SubByScalarDone
	VMOVDQU (CX), Y1
	VPSUBW  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000010, BX
	JMP     int16SubByScalarTailLoop

int16SubByScalarDone:
	CMPQ    BX, $0x00000008
	JL      int16SubByScalarDone1
	VMOVDQU (CX), X1
	VPSUBW  X0, X1, X1
	VMOVDQU X1, (DX)

int16SubByScalarDone1:
	RET

// func int32SubAvx2Asm(x []int32, y []int32, r []int32)
// Requires: AVX, AVX2
TEXT ·int32SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int32SubBlockLoop:
	CMPQ    BX, $0x00000080
	JL      int32SubTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VMOVDQU 192(AX), Y6
	VMOVDQU 224(AX), Y7
	VMOVDQU 256(AX), Y8
	VMOVDQU 288(AX), Y9
	VMOVDQU 320(AX), Y10
	VMOVDQU 352(AX), Y11
	VMOVDQU 384(AX), Y12
	VMOVDQU 416(AX), Y13
	VMOVDQU 448(AX), Y14
	VMOVDQU 480(AX), Y15
	VPSUBD  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	VPSUBD  32(CX), Y1, Y1
	VMOVDQU Y1, 32(DX)
	VPSUBD  64(CX), Y2, Y2
	VMOVDQU Y2, 64(DX)
	VPSUBD  96(CX), Y3, Y3
	VMOVDQU Y3, 96(DX)
	VPSUBD  128(CX), Y4, Y4
	VMOVDQU Y4, 128(DX)
	VPSUBD  160(CX), Y5, Y5
	VMOVDQU Y5, 160(DX)
	VPSUBD  192(CX), Y6, Y6
	VMOVDQU Y6, 192(DX)
	VPSUBD  224(CX), Y7, Y7
	VMOVDQU Y7, 224(DX)
	VPSUBD  256(CX), Y8, Y8
	VMOVDQU Y8, 256(DX)
	VPSUBD  288(CX), Y9, Y9
	VMOVDQU Y9, 288(DX)
	VPSUBD  320(CX), Y10, Y10
	VMOVDQU Y10, 320(DX)
	VPSUBD  352(CX), Y11, Y11
	VMOVDQU Y11, 352(DX)
	VPSUBD  384(CX), Y12, Y12
	VMOVDQU Y12, 384(DX)
	VPSUBD  416(CX), Y13, Y13
	VMOVDQU Y13, 416(DX)
	VPSUBD  448(CX), Y14, Y14
	VMOVDQU Y14, 448(DX)
	VPSUBD  480(CX), Y15, Y15
	VMOVDQU Y15, 480(DX)
	ADDQ    $0x00000200, AX
	ADDQ    $0x00000200, CX
	ADDQ    $0x00000200, DX
	SUBQ    $0x00000080, BX
	JMP     int32SubBlockLoop

int32SubTailLoop:
	CMPQ    BX, $0x00000008
	JL      int32SubDone
	VMOVDQU (AX), Y0
	VPSUBD  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     int32SubTailLoop

int32SubDone:
	CMPQ    BX, $0x00000004
	JL      int32SubDone1
	VMOVDQU (AX), X0
	VPSUBD  (CX), X0, X0
	VMOVDQU X0, (DX)

int32SubDone1:
	RET

// func int32SubScalarAvx2Asm(x int32, y []int32, r []int32)
// Requires: AVX, AVX2, SSE2
TEXT ·int32SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVL         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTD X0, Y0

int32SubScalarBlockLoop:
	CMPQ    BX, $0x00000078
	JL      int32SubScalarTailLoop
	VPSUBD  (CX), Y0, Y1
	VPSUBD  32(CX), Y0, Y2
	VPSUBD  64(CX), Y0, Y3
	VPSUBD  96(CX), Y0, Y4
	VPSUBD  128(CX), Y0, Y5
	VPSUBD  160(CX), Y0, Y6
	VPSUBD  192(CX), Y0, Y7
	VPSUBD  224(CX), Y0, Y8
	VPSUBD  256(CX), Y0, Y9
	VPSUBD  288(CX), Y0, Y10
	VPSUBD  320(CX), Y0, Y11
	VPSUBD  352(CX), Y0, Y12
	VPSUBD  384(CX), Y0, Y13
	VPSUBD  416(CX), Y0, Y14
	VPSUBD  448(CX), Y0, Y15
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	VMOVDQU Y7, 192(DX)
	VMOVDQU Y8, 224(DX)
	VMOVDQU Y9, 256(DX)
	VMOVDQU Y10, 288(DX)
	VMOVDQU Y11, 320(DX)
	VMOVDQU Y12, 352(DX)
	VMOVDQU Y13, 384(DX)
	VMOVDQU Y14, 416(DX)
	VMOVDQU Y15, 448(DX)
	ADDQ    $0x000001e0, CX
	ADDQ    $0x000001e0, DX
	SUBQ    $0x00000078, BX
	JMP     int32SubScalarBlockLoop

int32SubScalarTailLoop:
	CMPQ    BX, $0x00000008
	JL      int32SubScalarDone
	VPSUBD  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     int32SubScalarTailLoop

int32SubScalarDone:
	CMPQ    BX, $0x00000004
	JL      int32SubScalarDone1
	VPSUBD  (CX), X0, X1
	VMOVDQU X1, (DX)

int32SubScalarDone1:
	RET

// func int32SubByScalarAvx2Asm(x int32, y []int32, r []int32)
// Requires: AVX, AVX2, SSE2
TEXT ·int32SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVL         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTD X0, Y0

int32SubByScalarBlockLoop:
	CMPQ    BX, $0x00000078
	JL      int32SubByScalarTailLoop
	VMOVDQU (CX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 64(CX), Y3
	VMOVDQU 96(CX), Y4
	VMOVDQU 128(CX), Y5
	VMOVDQU 160(CX), Y6
	VMOVDQU 192(CX), Y7
	VMOVDQU 224(CX), Y8
	VMOVDQU 256(CX), Y9
	VMOVDQU 288(CX), Y10
	VMOVDQU 320(CX), Y11
	VMOVDQU 352(CX), Y12
	VMOVDQU 384(CX), Y13
	VMOVDQU 416(CX), Y14
	VMOVDQU 448(CX), Y15
	VPSUBD  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	VPSUBD  Y0, Y2, Y2
	VMOVDQU Y2, 32(DX)
	VPSUBD  Y0, Y3, Y3
	VMOVDQU Y3, 64(DX)
	VPSUBD  Y0, Y4, Y4
	VMOVDQU Y4, 96(DX)
	VPSUBD  Y0, Y5, Y5
	VMOVDQU Y5, 128(DX)
	VPSUBD  Y0, Y6, Y6
	VMOVDQU Y6, 160(DX)
	VPSUBD  Y0, Y7, Y7
	VMOVDQU Y7, 192(DX)
	VPSUBD  Y0, Y8, Y8
	VMOVDQU Y8, 224(DX)
	VPSUBD  Y0, Y9, Y9
	VMOVDQU Y9, 256(DX)
	VPSUBD  Y0, Y10, Y10
	VMOVDQU Y10, 288(DX)
	VPSUBD  Y0, Y11, Y11
	VMOVDQU Y11, 320(DX)
	VPSUBD  Y0, Y12, Y12
	VMOVDQU Y12, 352(DX)
	VPSUBD  Y0, Y13, Y13
	VMOVDQU Y13, 384(DX)
	VPSUBD  Y0, Y14, Y14
	VMOVDQU Y14, 416(DX)
	VPSUBD  Y0, Y15, Y15
	VMOVDQU Y15, 448(DX)
	ADDQ    $0x000001e0, CX
	ADDQ    $0x000001e0, DX
	SUBQ    $0x00000078, BX
	JMP     int32SubByScalarBlockLoop

int32SubByScalarTailLoop:
	CMPQ    BX, $0x00000008
	JL      int32SubByScalarDone
	VMOVDQU (CX), Y1
	VPSUBD  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     int32SubByScalarTailLoop

int32SubByScalarDone:
	CMPQ    BX, $0x00000004
	JL      int32SubByScalarDone1
	VMOVDQU (CX), X1
	VPSUBD  X0, X1, X1
	VMOVDQU X1, (DX)

int32SubByScalarDone1:
	RET

// func int64SubAvx2Asm(x []int64, y []int64, r []int64)
// Requires: AVX, AVX2
TEXT ·int64SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int64SubBlockLoop:
	CMPQ    BX, $0x00000040
	JL      int64SubTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VMOVDQU 192(AX), Y6
	VMOVDQU 224(AX), Y7
	VMOVDQU 256(AX), Y8
	VMOVDQU 288(AX), Y9
	VMOVDQU 320(AX), Y10
	VMOVDQU 352(AX), Y11
	VMOVDQU 384(AX), Y12
	VMOVDQU 416(AX), Y13
	VMOVDQU 448(AX), Y14
	VMOVDQU 480(AX), Y15
	VPSUBQ  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	VPSUBQ  32(CX), Y1, Y1
	VMOVDQU Y1, 32(DX)
	VPSUBQ  64(CX), Y2, Y2
	VMOVDQU Y2, 64(DX)
	VPSUBQ  96(CX), Y3, Y3
	VMOVDQU Y3, 96(DX)
	VPSUBQ  128(CX), Y4, Y4
	VMOVDQU Y4, 128(DX)
	VPSUBQ  160(CX), Y5, Y5
	VMOVDQU Y5, 160(DX)
	VPSUBQ  192(CX), Y6, Y6
	VMOVDQU Y6, 192(DX)
	VPSUBQ  224(CX), Y7, Y7
	VMOVDQU Y7, 224(DX)
	VPSUBQ  256(CX), Y8, Y8
	VMOVDQU Y8, 256(DX)
	VPSUBQ  288(CX), Y9, Y9
	VMOVDQU Y9, 288(DX)
	VPSUBQ  320(CX), Y10, Y10
	VMOVDQU Y10, 320(DX)
	VPSUBQ  352(CX), Y11, Y11
	VMOVDQU Y11, 352(DX)
	VPSUBQ  384(CX), Y12, Y12
	VMOVDQU Y12, 384(DX)
	VPSUBQ  416(CX), Y13, Y13
	VMOVDQU Y13, 416(DX)
	VPSUBQ  448(CX), Y14, Y14
	VMOVDQU Y14, 448(DX)
	VPSUBQ  480(CX), Y15, Y15
	VMOVDQU Y15, 480(DX)
	ADDQ    $0x00000200, AX
	ADDQ    $0x00000200, CX
	ADDQ    $0x00000200, DX
	SUBQ    $0x00000040, BX
	JMP     int64SubBlockLoop

int64SubTailLoop:
	CMPQ    BX, $0x00000004
	JL      int64SubDone
	VMOVDQU (AX), Y0
	VPSUBQ  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     int64SubTailLoop

int64SubDone:
	CMPQ    BX, $0x00000002
	JL      int64SubDone1
	VMOVDQU (AX), X0
	VPSUBQ  (CX), X0, X0
	VMOVDQU X0, (DX)

int64SubDone1:
	RET

// func int64SubScalarAvx2Asm(x int64, y []int64, r []int64)
// Requires: AVX, AVX2, SSE2
TEXT ·int64SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVQ         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVQ         AX, X0
	VPBROADCASTQ X0, Y0

int64SubScalarBlockLoop:
	CMPQ    BX, $0x0000003c
	JL      int64SubScalarTailLoop
	VPSUBQ  (CX), Y0, Y1
	VPSUBQ  32(CX), Y0, Y2
	VPSUBQ  64(CX), Y0, Y3
	VPSUBQ  96(CX), Y0, Y4
	VPSUBQ  128(CX), Y0, Y5
	VPSUBQ  160(CX), Y0, Y6
	VPSUBQ  192(CX), Y0, Y7
	VPSUBQ  224(CX), Y0, Y8
	VPSUBQ  256(CX), Y0, Y9
	VPSUBQ  288(CX), Y0, Y10
	VPSUBQ  320(CX), Y0, Y11
	VPSUBQ  352(CX), Y0, Y12
	VPSUBQ  384(CX), Y0, Y13
	VPSUBQ  416(CX), Y0, Y14
	VPSUBQ  448(CX), Y0, Y15
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	VMOVDQU Y7, 192(DX)
	VMOVDQU Y8, 224(DX)
	VMOVDQU Y9, 256(DX)
	VMOVDQU Y10, 288(DX)
	VMOVDQU Y11, 320(DX)
	VMOVDQU Y12, 352(DX)
	VMOVDQU Y13, 384(DX)
	VMOVDQU Y14, 416(DX)
	VMOVDQU Y15, 448(DX)
	ADDQ    $0x000001e0, CX
	ADDQ    $0x000001e0, DX
	SUBQ    $0x0000003c, BX
	JMP     int64SubScalarBlockLoop

int64SubScalarTailLoop:
	CMPQ    BX, $0x00000004
	JL      int64SubScalarDone
	VPSUBQ  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     int64SubScalarTailLoop

int64SubScalarDone:
	CMPQ    BX, $0x00000002
	JL      int64SubScalarDone1
	VPSUBQ  (CX), X0, X1
	VMOVDQU X1, (DX)

int64SubScalarDone1:
	RET

// func int64SubByScalarAvx2Asm(x int64, y []int64, r []int64)
// Requires: AVX, AVX2, SSE2
TEXT ·int64SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVQ         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVQ         AX, X0
	VPBROADCASTQ X0, Y0

int64SubByScalarBlockLoop:
	CMPQ    BX, $0x0000003c
	JL      int64SubByScalarTailLoop
	VMOVDQU (CX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 64(CX), Y3
	VMOVDQU 96(CX), Y4
	VMOVDQU 128(CX), Y5
	VMOVDQU 160(CX), Y6
	VMOVDQU 192(CX), Y7
	VMOVDQU 224(CX), Y8
	VMOVDQU 256(CX), Y9
	VMOVDQU 288(CX), Y10
	VMOVDQU 320(CX), Y11
	VMOVDQU 352(CX), Y12
	VMOVDQU 384(CX), Y13
	VMOVDQU 416(CX), Y14
	VMOVDQU 448(CX), Y15
	VPSUBQ  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	VPSUBQ  Y0, Y2, Y2
	VMOVDQU Y2, 32(DX)
	VPSUBQ  Y0, Y3, Y3
	VMOVDQU Y3, 64(DX)
	VPSUBQ  Y0, Y4, Y4
	VMOVDQU Y4, 96(DX)
	VPSUBQ  Y0, Y5, Y5
	VMOVDQU Y5, 128(DX)
	VPSUBQ  Y0, Y6, Y6
	VMOVDQU Y6, 160(DX)
	VPSUBQ  Y0, Y7, Y7
	VMOVDQU Y7, 192(DX)
	VPSUBQ  Y0, Y8, Y8
	VMOVDQU Y8, 224(DX)
	VPSUBQ  Y0, Y9, Y9
	VMOVDQU Y9, 256(DX)
	VPSUBQ  Y0, Y10, Y10
	VMOVDQU Y10, 288(DX)
	VPSUBQ  Y0, Y11, Y11
	VMOVDQU Y11, 320(DX)
	VPSUBQ  Y0, Y12, Y12
	VMOVDQU Y12, 352(DX)
	VPSUBQ  Y0, Y13, Y13
	VMOVDQU Y13, 384(DX)
	VPSUBQ  Y0, Y14, Y14
	VMOVDQU Y14, 416(DX)
	VPSUBQ  Y0, Y15, Y15
	VMOVDQU Y15, 448(DX)
	ADDQ    $0x000001e0, CX
	ADDQ    $0x000001e0, DX
	SUBQ    $0x0000003c, BX
	JMP     int64SubByScalarBlockLoop

int64SubByScalarTailLoop:
	CMPQ    BX, $0x00000004
	JL      int64SubByScalarDone
	VMOVDQU (CX), Y1
	VPSUBQ  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     int64SubByScalarTailLoop

int64SubByScalarDone:
	CMPQ    BX, $0x00000002
	JL      int64SubByScalarDone1
	VMOVDQU (CX), X1
	VPSUBQ  X0, X1, X1
	VMOVDQU X1, (DX)

int64SubByScalarDone1:
	RET

// func uint8SubAvx2Asm(x []uint8, y []uint8, r []uint8)
// Requires: AVX, AVX2
TEXT ·uint8SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint8SubBlockLoop:
	CMPQ    BX, $0x00000200
	JL      uint8SubTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VMOVDQU 192(AX), Y6
	VMOVDQU 224(AX), Y7
	VMOVDQU 256(AX), Y8
	VMOVDQU 288(AX), Y9
	VMOVDQU 320(AX), Y10
	VMOVDQU 352(AX), Y11
	VMOVDQU 384(AX), Y12
	VMOVDQU 416(AX), Y13
	VMOVDQU 448(AX), Y14
	VMOVDQU 480(AX), Y15
	VPSUBB  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	VPSUBB  32(CX), Y1, Y1
	VMOVDQU Y1, 32(DX)
	VPSUBB  64(CX), Y2, Y2
	VMOVDQU Y2, 64(DX)
	VPSUBB  96(CX), Y3, Y3
	VMOVDQU Y3, 96(DX)
	VPSUBB  128(CX), Y4, Y4
	VMOVDQU Y4, 128(DX)
	VPSUBB  160(CX), Y5, Y5
	VMOVDQU Y5, 160(DX)
	VPSUBB  192(CX), Y6, Y6
	VMOVDQU Y6, 192(DX)
	VPSUBB  224(CX), Y7, Y7
	VMOVDQU Y7, 224(DX)
	VPSUBB  256(CX), Y8, Y8
	VMOVDQU Y8, 256(DX)
	VPSUBB  288(CX), Y9, Y9
	VMOVDQU Y9, 288(DX)
	VPSUBB  320(CX), Y10, Y10
	VMOVDQU Y10, 320(DX)
	VPSUBB  352(CX), Y11, Y11
	VMOVDQU Y11, 352(DX)
	VPSUBB  384(CX), Y12, Y12
	VMOVDQU Y12, 384(DX)
	VPSUBB  416(CX), Y13, Y13
	VMOVDQU Y13, 416(DX)
	VPSUBB  448(CX), Y14, Y14
	VMOVDQU Y14, 448(DX)
	VPSUBB  480(CX), Y15, Y15
	VMOVDQU Y15, 480(DX)
	ADDQ    $0x00000200, AX
	ADDQ    $0x00000200, CX
	ADDQ    $0x00000200, DX
	SUBQ    $0x00000200, BX
	JMP     uint8SubBlockLoop

uint8SubTailLoop:
	CMPQ    BX, $0x00000020
	JL      uint8SubDone
	VMOVDQU (AX), Y0
	VPSUBB  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000020, BX
	JMP     uint8SubTailLoop

uint8SubDone:
	CMPQ    BX, $0x00000010
	JL      uint8SubDone1
	VMOVDQU (AX), X0
	VPSUBB  (CX), X0, X0
	VMOVDQU X0, (DX)

uint8SubDone1:
	RET

// func uint8SubScalarAvx2Asm(x uint8, y []uint8, r []uint8)
// Requires: AVX, AVX2, SSE2
TEXT ·uint8SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVBLZX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTB X0, Y0

uint8SubScalarBlockLoop:
	CMPQ    BX, $0x000001e0
	JL      uint8SubScalarTailLoop
	VPSUBB  (CX), Y0, Y1
	VPSUBB  32(CX), Y0, Y2
	VPSUBB  64(CX), Y0, Y3
	VPSUBB  96(CX), Y0, Y4
	VPSUBB  128(CX), Y0, Y5
	VPSUBB  160(CX), Y0, Y6
	VPSUBB  192(CX), Y0, Y7
	VPSUBB  224(CX), Y0, Y8
	VPSUBB  256(CX), Y0, Y9
	VPSUBB  288(CX), Y0, Y10
	VPSUBB  320(CX), Y0, Y11
	VPSUBB  352(CX), Y0, Y12
	VPSUBB  384(CX), Y0, Y13
	VPSUBB  416(CX), Y0, Y14
	VPSUBB  448(CX), Y0, Y15
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	VMOVDQU Y7, 192(DX)
	VMOVDQU Y8, 224(DX)
	VMOVDQU Y9, 256(DX)
	VMOVDQU Y10, 288(DX)
	VMOVDQU Y11, 320(DX)
	VMOVDQU Y12, 352(DX)
	VMOVDQU Y13, 384(DX)
	VMOVDQU Y14, 416(DX)
	VMOVDQU Y15, 448(DX)
	ADDQ    $0x000001e0, CX
	ADDQ    $0x000001e0, DX
	SUBQ    $0x000001e0, BX
	JMP     uint8SubScalarBlockLoop

uint8SubScalarTailLoop:
	CMPQ    BX, $0x00000020
	JL      uint8SubScalarDone
	VPSUBB  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000020, BX
	JMP     uint8SubScalarTailLoop

uint8SubScalarDone:
	CMPQ    BX, $0x00000010
	JL      uint8SubScalarDone1
	VPSUBB  (CX), X0, X1
	VMOVDQU X1, (DX)

uint8SubScalarDone1:
	RET

// func uint8SubByScalarAvx2Asm(x uint8, y []uint8, r []uint8)
// Requires: AVX, AVX2, SSE2
TEXT ·uint8SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVBLZX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTB X0, Y0

uint8SubByScalarBlockLoop:
	CMPQ    BX, $0x000001e0
	JL      uint8SubByScalarTailLoop
	VMOVDQU (CX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 64(CX), Y3
	VMOVDQU 96(CX), Y4
	VMOVDQU 128(CX), Y5
	VMOVDQU 160(CX), Y6
	VMOVDQU 192(CX), Y7
	VMOVDQU 224(CX), Y8
	VMOVDQU 256(CX), Y9
	VMOVDQU 288(CX), Y10
	VMOVDQU 320(CX), Y11
	VMOVDQU 352(CX), Y12
	VMOVDQU 384(CX), Y13
	VMOVDQU 416(CX), Y14
	VMOVDQU 448(CX), Y15
	VPSUBB  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	VPSUBB  Y0, Y2, Y2
	VMOVDQU Y2, 32(DX)
	VPSUBB  Y0, Y3, Y3
	VMOVDQU Y3, 64(DX)
	VPSUBB  Y0, Y4, Y4
	VMOVDQU Y4, 96(DX)
	VPSUBB  Y0, Y5, Y5
	VMOVDQU Y5, 128(DX)
	VPSUBB  Y0, Y6, Y6
	VMOVDQU Y6, 160(DX)
	VPSUBB  Y0, Y7, Y7
	VMOVDQU Y7, 192(DX)
	VPSUBB  Y0, Y8, Y8
	VMOVDQU Y8, 224(DX)
	VPSUBB  Y0, Y9, Y9
	VMOVDQU Y9, 256(DX)
	VPSUBB  Y0, Y10, Y10
	VMOVDQU Y10, 288(DX)
	VPSUBB  Y0, Y11, Y11
	VMOVDQU Y11, 320(DX)
	VPSUBB  Y0, Y12, Y12
	VMOVDQU Y12, 352(DX)
	VPSUBB  Y0, Y13, Y13
	VMOVDQU Y13, 384(DX)
	VPSUBB  Y0, Y14, Y14
	VMOVDQU Y14, 416(DX)
	VPSUBB  Y0, Y15, Y15
	VMOVDQU Y15, 448(DX)
	ADDQ    $0x000001e0, CX
	ADDQ    $0x000001e0, DX
	SUBQ    $0x000001e0, BX
	JMP     uint8SubByScalarBlockLoop

uint8SubByScalarTailLoop:
	CMPQ    BX, $0x00000020
	JL      uint8SubByScalarDone
	VMOVDQU (CX), Y1
	VPSUBB  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000020, BX
	JMP     uint8SubByScalarTailLoop

uint8SubByScalarDone:
	CMPQ    BX, $0x00000010
	JL      uint8SubByScalarDone1
	VMOVDQU (CX), X1
	VPSUBB  X0, X1, X1
	VMOVDQU X1, (DX)

uint8SubByScalarDone1:
	RET

// func uint16SubAvx2Asm(x []uint16, y []uint16, r []uint16)
// Requires: AVX, AVX2
TEXT ·uint16SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint16SubBlockLoop:
	CMPQ    BX, $0x00000100
	JL      uint16SubTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VMOVDQU 192(AX), Y6
	VMOVDQU 224(AX), Y7
	VMOVDQU 256(AX), Y8
	VMOVDQU 288(AX), Y9
	VMOVDQU 320(AX), Y10
	VMOVDQU 352(AX), Y11
	VMOVDQU 384(AX), Y12
	VMOVDQU 416(AX), Y13
	VMOVDQU 448(AX), Y14
	VMOVDQU 480(AX), Y15
	VPSUBW  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	VPSUBW  32(CX), Y1, Y1
	VMOVDQU Y1, 32(DX)
	VPSUBW  64(CX), Y2, Y2
	VMOVDQU Y2, 64(DX)
	VPSUBW  96(CX), Y3, Y3
	VMOVDQU Y3, 96(DX)
	VPSUBW  128(CX), Y4, Y4
	VMOVDQU Y4, 128(DX)
	VPSUBW  160(CX), Y5, Y5
	VMOVDQU Y5, 160(DX)
	VPSUBW  192(CX), Y6, Y6
	VMOVDQU Y6, 192(DX)
	VPSUBW  224(CX), Y7, Y7
	VMOVDQU Y7, 224(DX)
	VPSUBW  256(CX), Y8, Y8
	VMOVDQU Y8, 256(DX)
	VPSUBW  288(CX), Y9, Y9
	VMOVDQU Y9, 288(DX)
	VPSUBW  320(CX), Y10, Y10
	VMOVDQU Y10, 320(DX)
	VPSUBW  352(CX), Y11, Y11
	VMOVDQU Y11, 352(DX)
	VPSUBW  384(CX), Y12, Y12
	VMOVDQU Y12, 384(DX)
	VPSUBW  416(CX), Y13, Y13
	VMOVDQU Y13, 416(DX)
	VPSUBW  448(CX), Y14, Y14
	VMOVDQU Y14, 448(DX)
	VPSUBW  480(CX), Y15, Y15
	VMOVDQU Y15, 480(DX)
	ADDQ    $0x00000200, AX
	ADDQ    $0x00000200, CX
	ADDQ    $0x00000200, DX
	SUBQ    $0x00000100, BX
	JMP     uint16SubBlockLoop

uint16SubTailLoop:
	CMPQ    BX, $0x00000010
	JL      uint16SubDone
	VMOVDQU (AX), Y0
	VPSUBW  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000010, BX
	JMP     uint16SubTailLoop

uint16SubDone:
	CMPQ    BX, $0x00000008
	JL      uint16SubDone1
	VMOVDQU (AX), X0
	VPSUBW  (CX), X0, X0
	VMOVDQU X0, (DX)

uint16SubDone1:
	RET

// func uint16SubScalarAvx2Asm(x uint16, y []uint16, r []uint16)
// Requires: AVX, AVX2, SSE2
TEXT ·uint16SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVWLZX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTW X0, Y0

uint16SubScalarBlockLoop:
	CMPQ    BX, $0x000000f0
	JL      uint16SubScalarTailLoop
	VPSUBW  (CX), Y0, Y1
	VPSUBW  32(CX), Y0, Y2
	VPSUBW  64(CX), Y0, Y3
	VPSUBW  96(CX), Y0, Y4
	VPSUBW  128(CX), Y0, Y5
	VPSUBW  160(CX), Y0, Y6
	VPSUBW  192(CX), Y0, Y7
	VPSUBW  224(CX), Y0, Y8
	VPSUBW  256(CX), Y0, Y9
	VPSUBW  288(CX), Y0, Y10
	VPSUBW  320(CX), Y0, Y11
	VPSUBW  352(CX), Y0, Y12
	VPSUBW  384(CX), Y0, Y13
	VPSUBW  416(CX), Y0, Y14
	VPSUBW  448(CX), Y0, Y15
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	VMOVDQU Y7, 192(DX)
	VMOVDQU Y8, 224(DX)
	VMOVDQU Y9, 256(DX)
	VMOVDQU Y10, 288(DX)
	VMOVDQU Y11, 320(DX)
	VMOVDQU Y12, 352(DX)
	VMOVDQU Y13, 384(DX)
	VMOVDQU Y14, 416(DX)
	VMOVDQU Y15, 448(DX)
	ADDQ    $0x000001e0, CX
	ADDQ    $0x000001e0, DX
	SUBQ    $0x000000f0, BX
	JMP     uint16SubScalarBlockLoop

uint16SubScalarTailLoop:
	CMPQ    BX, $0x00000010
	JL      uint16SubScalarDone
	VPSUBW  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000010, BX
	JMP     uint16SubScalarTailLoop

uint16SubScalarDone:
	CMPQ    BX, $0x00000008
	JL      uint16SubScalarDone1
	VPSUBW  (CX), X0, X1
	VMOVDQU X1, (DX)

uint16SubScalarDone1:
	RET

// func uint16SubByScalarAvx2Asm(x uint16, y []uint16, r []uint16)
// Requires: AVX, AVX2, SSE2
TEXT ·uint16SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVWLZX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTW X0, Y0

uint16SubByScalarBlockLoop:
	CMPQ    BX, $0x000000f0
	JL      uint16SubByScalarTailLoop
	VMOVDQU (CX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 64(CX), Y3
	VMOVDQU 96(CX), Y4
	VMOVDQU 128(CX), Y5
	VMOVDQU 160(CX), Y6
	VMOVDQU 192(CX), Y7
	VMOVDQU 224(CX), Y8
	VMOVDQU 256(CX), Y9
	VMOVDQU 288(CX), Y10
	VMOVDQU 320(CX), Y11
	VMOVDQU 352(CX), Y12
	VMOVDQU 384(CX), Y13
	VMOVDQU 416(CX), Y14
	VMOVDQU 448(CX), Y15
	VPSUBW  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	VPSUBW  Y0, Y2, Y2
	VMOVDQU Y2, 32(DX)
	VPSUBW  Y0, Y3, Y3
	VMOVDQU Y3, 64(DX)
	VPSUBW  Y0, Y4, Y4
	VMOVDQU Y4, 96(DX)
	VPSUBW  Y0, Y5, Y5
	VMOVDQU Y5, 128(DX)
	VPSUBW  Y0, Y6, Y6
	VMOVDQU Y6, 160(DX)
	VPSUBW  Y0, Y7, Y7
	VMOVDQU Y7, 192(DX)
	VPSUBW  Y0, Y8, Y8
	VMOVDQU Y8, 224(DX)
	VPSUBW  Y0, Y9, Y9
	VMOVDQU Y9, 256(DX)
	VPSUBW  Y0, Y10, Y10
	VMOVDQU Y10, 288(DX)
	VPSUBW  Y0, Y11, Y11
	VMOVDQU Y11, 320(DX)
	VPSUBW  Y0, Y12, Y12
	VMOVDQU Y12, 352(DX)
	VPSUBW  Y0, Y13, Y13
	VMOVDQU Y13, 384(DX)
	VPSUBW  Y0, Y14, Y14
	VMOVDQU Y14, 416(DX)
	VPSUBW  Y0, Y15, Y15
	VMOVDQU Y15, 448(DX)
	ADDQ    $0x000001e0, CX
	ADDQ    $0x000001e0, DX
	SUBQ    $0x000000f0, BX
	JMP     uint16SubByScalarBlockLoop

uint16SubByScalarTailLoop:
	CMPQ    BX, $0x00000010
	JL      uint16SubByScalarDone
	VMOVDQU (CX), Y1
	VPSUBW  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000010, BX
	JMP     uint16SubByScalarTailLoop

uint16SubByScalarDone:
	CMPQ    BX, $0x00000008
	JL      uint16SubByScalarDone1
	VMOVDQU (CX), X1
	VPSUBW  X0, X1, X1
	VMOVDQU X1, (DX)

uint16SubByScalarDone1:
	RET

// func uint32SubAvx2Asm(x []uint32, y []uint32, r []uint32)
// Requires: AVX, AVX2
TEXT ·uint32SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint32SubBlockLoop:
	CMPQ    BX, $0x00000080
	JL      uint32SubTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VMOVDQU 192(AX), Y6
	VMOVDQU 224(AX), Y7
	VMOVDQU 256(AX), Y8
	VMOVDQU 288(AX), Y9
	VMOVDQU 320(AX), Y10
	VMOVDQU 352(AX), Y11
	VMOVDQU 384(AX), Y12
	VMOVDQU 416(AX), Y13
	VMOVDQU 448(AX), Y14
	VMOVDQU 480(AX), Y15
	VPSUBD  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	VPSUBD  32(CX), Y1, Y1
	VMOVDQU Y1, 32(DX)
	VPSUBD  64(CX), Y2, Y2
	VMOVDQU Y2, 64(DX)
	VPSUBD  96(CX), Y3, Y3
	VMOVDQU Y3, 96(DX)
	VPSUBD  128(CX), Y4, Y4
	VMOVDQU Y4, 128(DX)
	VPSUBD  160(CX), Y5, Y5
	VMOVDQU Y5, 160(DX)
	VPSUBD  192(CX), Y6, Y6
	VMOVDQU Y6, 192(DX)
	VPSUBD  224(CX), Y7, Y7
	VMOVDQU Y7, 224(DX)
	VPSUBD  256(CX), Y8, Y8
	VMOVDQU Y8, 256(DX)
	VPSUBD  288(CX), Y9, Y9
	VMOVDQU Y9, 288(DX)
	VPSUBD  320(CX), Y10, Y10
	VMOVDQU Y10, 320(DX)
	VPSUBD  352(CX), Y11, Y11
	VMOVDQU Y11, 352(DX)
	VPSUBD  384(CX), Y12, Y12
	VMOVDQU Y12, 384(DX)
	VPSUBD  416(CX), Y13, Y13
	VMOVDQU Y13, 416(DX)
	VPSUBD  448(CX), Y14, Y14
	VMOVDQU Y14, 448(DX)
	VPSUBD  480(CX), Y15, Y15
	VMOVDQU Y15, 480(DX)
	ADDQ    $0x00000200, AX
	ADDQ    $0x00000200, CX
	ADDQ    $0x00000200, DX
	SUBQ    $0x00000080, BX
	JMP     uint32SubBlockLoop

uint32SubTailLoop:
	CMPQ    BX, $0x00000008
	JL      uint32SubDone
	VMOVDQU (AX), Y0
	VPSUBD  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     uint32SubTailLoop

uint32SubDone:
	CMPQ    BX, $0x00000004
	JL      uint32SubDone1
	VMOVDQU (AX), X0
	VPSUBD  (CX), X0, X0
	VMOVDQU X0, (DX)

uint32SubDone1:
	RET

// func uint32SubScalarAvx2Asm(x uint32, y []uint32, r []uint32)
// Requires: AVX, AVX2, SSE2
TEXT ·uint32SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVL         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTD X0, Y0

uint32SubScalarBlockLoop:
	CMPQ    BX, $0x00000078
	JL      uint32SubScalarTailLoop
	VPSUBD  (CX), Y0, Y1
	VPSUBD  32(CX), Y0, Y2
	VPSUBD  64(CX), Y0, Y3
	VPSUBD  96(CX), Y0, Y4
	VPSUBD  128(CX), Y0, Y5
	VPSUBD  160(CX), Y0, Y6
	VPSUBD  192(CX), Y0, Y7
	VPSUBD  224(CX), Y0, Y8
	VPSUBD  256(CX), Y0, Y9
	VPSUBD  288(CX), Y0, Y10
	VPSUBD  320(CX), Y0, Y11
	VPSUBD  352(CX), Y0, Y12
	VPSUBD  384(CX), Y0, Y13
	VPSUBD  416(CX), Y0, Y14
	VPSUBD  448(CX), Y0, Y15
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	VMOVDQU Y7, 192(DX)
	VMOVDQU Y8, 224(DX)
	VMOVDQU Y9, 256(DX)
	VMOVDQU Y10, 288(DX)
	VMOVDQU Y11, 320(DX)
	VMOVDQU Y12, 352(DX)
	VMOVDQU Y13, 384(DX)
	VMOVDQU Y14, 416(DX)
	VMOVDQU Y15, 448(DX)
	ADDQ    $0x000001e0, CX
	ADDQ    $0x000001e0, DX
	SUBQ    $0x00000078, BX
	JMP     uint32SubScalarBlockLoop

uint32SubScalarTailLoop:
	CMPQ    BX, $0x00000008
	JL      uint32SubScalarDone
	VPSUBD  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     uint32SubScalarTailLoop

uint32SubScalarDone:
	CMPQ    BX, $0x00000004
	JL      uint32SubScalarDone1
	VPSUBD  (CX), X0, X1
	VMOVDQU X1, (DX)

uint32SubScalarDone1:
	RET

// func uint32SubByScalarAvx2Asm(x uint32, y []uint32, r []uint32)
// Requires: AVX, AVX2, SSE2
TEXT ·uint32SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVL         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTD X0, Y0

uint32SubByScalarBlockLoop:
	CMPQ    BX, $0x00000078
	JL      uint32SubByScalarTailLoop
	VMOVDQU (CX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 64(CX), Y3
	VMOVDQU 96(CX), Y4
	VMOVDQU 128(CX), Y5
	VMOVDQU 160(CX), Y6
	VMOVDQU 192(CX), Y7
	VMOVDQU 224(CX), Y8
	VMOVDQU 256(CX), Y9
	VMOVDQU 288(CX), Y10
	VMOVDQU 320(CX), Y11
	VMOVDQU 352(CX), Y12
	VMOVDQU 384(CX), Y13
	VMOVDQU 416(CX), Y14
	VMOVDQU 448(CX), Y15
	VPSUBD  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	VPSUBD  Y0, Y2, Y2
	VMOVDQU Y2, 32(DX)
	VPSUBD  Y0, Y3, Y3
	VMOVDQU Y3, 64(DX)
	VPSUBD  Y0, Y4, Y4
	VMOVDQU Y4, 96(DX)
	VPSUBD  Y0, Y5, Y5
	VMOVDQU Y5, 128(DX)
	VPSUBD  Y0, Y6, Y6
	VMOVDQU Y6, 160(DX)
	VPSUBD  Y0, Y7, Y7
	VMOVDQU Y7, 192(DX)
	VPSUBD  Y0, Y8, Y8
	VMOVDQU Y8, 224(DX)
	VPSUBD  Y0, Y9, Y9
	VMOVDQU Y9, 256(DX)
	VPSUBD  Y0, Y10, Y10
	VMOVDQU Y10, 288(DX)
	VPSUBD  Y0, Y11, Y11
	VMOVDQU Y11, 320(DX)
	VPSUBD  Y0, Y12, Y12
	VMOVDQU Y12, 352(DX)
	VPSUBD  Y0, Y13, Y13
	VMOVDQU Y13, 384(DX)
	VPSUBD  Y0, Y14, Y14
	VMOVDQU Y14, 416(DX)
	VPSUBD  Y0, Y15, Y15
	VMOVDQU Y15, 448(DX)
	ADDQ    $0x000001e0, CX
	ADDQ    $0x000001e0, DX
	SUBQ    $0x00000078, BX
	JMP     uint32SubByScalarBlockLoop

uint32SubByScalarTailLoop:
	CMPQ    BX, $0x00000008
	JL      uint32SubByScalarDone
	VMOVDQU (CX), Y1
	VPSUBD  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     uint32SubByScalarTailLoop

uint32SubByScalarDone:
	CMPQ    BX, $0x00000004
	JL      uint32SubByScalarDone1
	VMOVDQU (CX), X1
	VPSUBD  X0, X1, X1
	VMOVDQU X1, (DX)

uint32SubByScalarDone1:
	RET

// func uint64SubAvx2Asm(x []uint64, y []uint64, r []uint64)
// Requires: AVX, AVX2
TEXT ·uint64SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint64SubBlockLoop:
	CMPQ    BX, $0x00000040
	JL      uint64SubTailLoop
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4
	VMOVDQU 160(AX), Y5
	VMOVDQU 192(AX), Y6
	VMOVDQU 224(AX), Y7
	VMOVDQU 256(AX), Y8
	VMOVDQU 288(AX), Y9
	VMOVDQU 320(AX), Y10
	VMOVDQU 352(AX), Y11
	VMOVDQU 384(AX), Y12
	VMOVDQU 416(AX), Y13
	VMOVDQU 448(AX), Y14
	VMOVDQU 480(AX), Y15
	VPSUBQ  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	VPSUBQ  32(CX), Y1, Y1
	VMOVDQU Y1, 32(DX)
	VPSUBQ  64(CX), Y2, Y2
	VMOVDQU Y2, 64(DX)
	VPSUBQ  96(CX), Y3, Y3
	VMOVDQU Y3, 96(DX)
	VPSUBQ  128(CX), Y4, Y4
	VMOVDQU Y4, 128(DX)
	VPSUBQ  160(CX), Y5, Y5
	VMOVDQU Y5, 160(DX)
	VPSUBQ  192(CX), Y6, Y6
	VMOVDQU Y6, 192(DX)
	VPSUBQ  224(CX), Y7, Y7
	VMOVDQU Y7, 224(DX)
	VPSUBQ  256(CX), Y8, Y8
	VMOVDQU Y8, 256(DX)
	VPSUBQ  288(CX), Y9, Y9
	VMOVDQU Y9, 288(DX)
	VPSUBQ  320(CX), Y10, Y10
	VMOVDQU Y10, 320(DX)
	VPSUBQ  352(CX), Y11, Y11
	VMOVDQU Y11, 352(DX)
	VPSUBQ  384(CX), Y12, Y12
	VMOVDQU Y12, 384(DX)
	VPSUBQ  416(CX), Y13, Y13
	VMOVDQU Y13, 416(DX)
	VPSUBQ  448(CX), Y14, Y14
	VMOVDQU Y14, 448(DX)
	VPSUBQ  480(CX), Y15, Y15
	VMOVDQU Y15, 480(DX)
	ADDQ    $0x00000200, AX
	ADDQ    $0x00000200, CX
	ADDQ    $0x00000200, DX
	SUBQ    $0x00000040, BX
	JMP     uint64SubBlockLoop

uint64SubTailLoop:
	CMPQ    BX, $0x00000004
	JL      uint64SubDone
	VMOVDQU (AX), Y0
	VPSUBQ  (CX), Y0, Y0
	VMOVDQU Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     uint64SubTailLoop

uint64SubDone:
	CMPQ    BX, $0x00000002
	JL      uint64SubDone1
	VMOVDQU (AX), X0
	VPSUBQ  (CX), X0, X0
	VMOVDQU X0, (DX)

uint64SubDone1:
	RET

// func uint64SubScalarAvx2Asm(x uint64, y []uint64, r []uint64)
// Requires: AVX, AVX2, SSE2
TEXT ·uint64SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVQ         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVQ         AX, X0
	VPBROADCASTQ X0, Y0

uint64SubScalarBlockLoop:
	CMPQ    BX, $0x0000003c
	JL      uint64SubScalarTailLoop
	VPSUBQ  (CX), Y0, Y1
	VPSUBQ  32(CX), Y0, Y2
	VPSUBQ  64(CX), Y0, Y3
	VPSUBQ  96(CX), Y0, Y4
	VPSUBQ  128(CX), Y0, Y5
	VPSUBQ  160(CX), Y0, Y6
	VPSUBQ  192(CX), Y0, Y7
	VPSUBQ  224(CX), Y0, Y8
	VPSUBQ  256(CX), Y0, Y9
	VPSUBQ  288(CX), Y0, Y10
	VPSUBQ  320(CX), Y0, Y11
	VPSUBQ  352(CX), Y0, Y12
	VPSUBQ  384(CX), Y0, Y13
	VPSUBQ  416(CX), Y0, Y14
	VPSUBQ  448(CX), Y0, Y15
	VMOVDQU Y1, (DX)
	VMOVDQU Y2, 32(DX)
	VMOVDQU Y3, 64(DX)
	VMOVDQU Y4, 96(DX)
	VMOVDQU Y5, 128(DX)
	VMOVDQU Y6, 160(DX)
	VMOVDQU Y7, 192(DX)
	VMOVDQU Y8, 224(DX)
	VMOVDQU Y9, 256(DX)
	VMOVDQU Y10, 288(DX)
	VMOVDQU Y11, 320(DX)
	VMOVDQU Y12, 352(DX)
	VMOVDQU Y13, 384(DX)
	VMOVDQU Y14, 416(DX)
	VMOVDQU Y15, 448(DX)
	ADDQ    $0x000001e0, CX
	ADDQ    $0x000001e0, DX
	SUBQ    $0x0000003c, BX
	JMP     uint64SubScalarBlockLoop

uint64SubScalarTailLoop:
	CMPQ    BX, $0x00000004
	JL      uint64SubScalarDone
	VPSUBQ  (CX), Y0, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     uint64SubScalarTailLoop

uint64SubScalarDone:
	CMPQ    BX, $0x00000002
	JL      uint64SubScalarDone1
	VPSUBQ  (CX), X0, X1
	VMOVDQU X1, (DX)

uint64SubScalarDone1:
	RET

// func uint64SubByScalarAvx2Asm(x uint64, y []uint64, r []uint64)
// Requires: AVX, AVX2, SSE2
TEXT ·uint64SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVQ         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVQ         AX, X0
	VPBROADCASTQ X0, Y0

uint64SubByScalarBlockLoop:
	CMPQ    BX, $0x0000003c
	JL      uint64SubByScalarTailLoop
	VMOVDQU (CX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 64(CX), Y3
	VMOVDQU 96(CX), Y4
	VMOVDQU 128(CX), Y5
	VMOVDQU 160(CX), Y6
	VMOVDQU 192(CX), Y7
	VMOVDQU 224(CX), Y8
	VMOVDQU 256(CX), Y9
	VMOVDQU 288(CX), Y10
	VMOVDQU 320(CX), Y11
	VMOVDQU 352(CX), Y12
	VMOVDQU 384(CX), Y13
	VMOVDQU 416(CX), Y14
	VMOVDQU 448(CX), Y15
	VPSUBQ  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	VPSUBQ  Y0, Y2, Y2
	VMOVDQU Y2, 32(DX)
	VPSUBQ  Y0, Y3, Y3
	VMOVDQU Y3, 64(DX)
	VPSUBQ  Y0, Y4, Y4
	VMOVDQU Y4, 96(DX)
	VPSUBQ  Y0, Y5, Y5
	VMOVDQU Y5, 128(DX)
	VPSUBQ  Y0, Y6, Y6
	VMOVDQU Y6, 160(DX)
	VPSUBQ  Y0, Y7, Y7
	VMOVDQU Y7, 192(DX)
	VPSUBQ  Y0, Y8, Y8
	VMOVDQU Y8, 224(DX)
	VPSUBQ  Y0, Y9, Y9
	VMOVDQU Y9, 256(DX)
	VPSUBQ  Y0, Y10, Y10
	VMOVDQU Y10, 288(DX)
	VPSUBQ  Y0, Y11, Y11
	VMOVDQU Y11, 320(DX)
	VPSUBQ  Y0, Y12, Y12
	VMOVDQU Y12, 352(DX)
	VPSUBQ  Y0, Y13, Y13
	VMOVDQU Y13, 384(DX)
	VPSUBQ  Y0, Y14, Y14
	VMOVDQU Y14, 416(DX)
	VPSUBQ  Y0, Y15, Y15
	VMOVDQU Y15, 448(DX)
	ADDQ    $0x000001e0, CX
	ADDQ    $0x000001e0, DX
	SUBQ    $0x0000003c, BX
	JMP     uint64SubByScalarBlockLoop

uint64SubByScalarTailLoop:
	CMPQ    BX, $0x00000004
	JL      uint64SubByScalarDone
	VMOVDQU (CX), Y1
	VPSUBQ  Y0, Y1, Y1
	VMOVDQU Y1, (DX)
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     uint64SubByScalarTailLoop

uint64SubByScalarDone:
	CMPQ    BX, $0x00000002
	JL      uint64SubByScalarDone1
	VMOVDQU (CX), X1
	VPSUBQ  X0, X1, X1
	VMOVDQU X1, (DX)

uint64SubByScalarDone1:
	RET

// func float32SubAvx2Asm(x []float32, y []float32, r []float32)
// Requires: AVX
TEXT ·float32SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

float32SubBlockLoop:
	CMPQ    BX, $0x00000080
	JL      float32SubTailLoop
	VMOVUPS (AX), Y0
	VMOVUPS 32(AX), Y1
	VMOVUPS 64(AX), Y2
	VMOVUPS 96(AX), Y3
	VMOVUPS 128(AX), Y4
	VMOVUPS 160(AX), Y5
	VMOVUPS 192(AX), Y6
	VMOVUPS 224(AX), Y7
	VMOVUPS 256(AX), Y8
	VMOVUPS 288(AX), Y9
	VMOVUPS 320(AX), Y10
	VMOVUPS 352(AX), Y11
	VMOVUPS 384(AX), Y12
	VMOVUPS 416(AX), Y13
	VMOVUPS 448(AX), Y14
	VMOVUPS 480(AX), Y15
	VSUBPS  (CX), Y0, Y0
	VMOVUPS Y0, (DX)
	VSUBPS  32(CX), Y1, Y1
	VMOVUPS Y1, 32(DX)
	VSUBPS  64(CX), Y2, Y2
	VMOVUPS Y2, 64(DX)
	VSUBPS  96(CX), Y3, Y3
	VMOVUPS Y3, 96(DX)
	VSUBPS  128(CX), Y4, Y4
	VMOVUPS Y4, 128(DX)
	VSUBPS  160(CX), Y5, Y5
	VMOVUPS Y5, 160(DX)
	VSUBPS  192(CX), Y6, Y6
	VMOVUPS Y6, 192(DX)
	VSUBPS  224(CX), Y7, Y7
	VMOVUPS Y7, 224(DX)
	VSUBPS  256(CX), Y8, Y8
	VMOVUPS Y8, 256(DX)
	VSUBPS  288(CX), Y9, Y9
	VMOVUPS Y9, 288(DX)
	VSUBPS  320(CX), Y10, Y10
	VMOVUPS Y10, 320(DX)
	VSUBPS  352(CX), Y11, Y11
	VMOVUPS Y11, 352(DX)
	VSUBPS  384(CX), Y12, Y12
	VMOVUPS Y12, 384(DX)
	VSUBPS  416(CX), Y13, Y13
	VMOVUPS Y13, 416(DX)
	VSUBPS  448(CX), Y14, Y14
	VMOVUPS Y14, 448(DX)
	VSUBPS  480(CX), Y15, Y15
	VMOVUPS Y15, 480(DX)
	ADDQ    $0x00000200, AX
	ADDQ    $0x00000200, CX
	ADDQ    $0x00000200, DX
	SUBQ    $0x00000080, BX
	JMP     float32SubBlockLoop

float32SubTailLoop:
	CMPQ    BX, $0x00000008
	JL      float32SubDone
	VMOVUPS (AX), Y0
	VSUBPS  (CX), Y0, Y0
	VMOVUPS Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     float32SubTailLoop

float32SubDone:
	CMPQ    BX, $0x00000004
	JL      float32SubDone1
	VMOVUPS (AX), X0
	VSUBPS  (CX), X0, X0
	VMOVUPS X0, (DX)

float32SubDone1:
	RET

// func float32SubScalarAvx2Asm(x float32, y []float32, r []float32)
// Requires: AVX, AVX2, SSE
TEXT ·float32SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVSS        x+0(FP), X0
	MOVQ         y_base+8(FP), AX
	MOVQ         r_base+32(FP), CX
	MOVQ         y_len+16(FP), DX
	VBROADCASTSS X0, Y0

float32SubScalarBlockLoop:
	CMPQ    DX, $0x00000078
	JL      float32SubScalarTailLoop
	VSUBPS  (AX), Y0, Y1
	VSUBPS  32(AX), Y0, Y2
	VSUBPS  64(AX), Y0, Y3
	VSUBPS  96(AX), Y0, Y4
	VSUBPS  128(AX), Y0, Y5
	VSUBPS  160(AX), Y0, Y6
	VSUBPS  192(AX), Y0, Y7
	VSUBPS  224(AX), Y0, Y8
	VSUBPS  256(AX), Y0, Y9
	VSUBPS  288(AX), Y0, Y10
	VSUBPS  320(AX), Y0, Y11
	VSUBPS  352(AX), Y0, Y12
	VSUBPS  384(AX), Y0, Y13
	VSUBPS  416(AX), Y0, Y14
	VSUBPS  448(AX), Y0, Y15
	VMOVUPS Y1, (CX)
	VMOVUPS Y2, 32(CX)
	VMOVUPS Y3, 64(CX)
	VMOVUPS Y4, 96(CX)
	VMOVUPS Y5, 128(CX)
	VMOVUPS Y6, 160(CX)
	VMOVUPS Y7, 192(CX)
	VMOVUPS Y8, 224(CX)
	VMOVUPS Y9, 256(CX)
	VMOVUPS Y10, 288(CX)
	VMOVUPS Y11, 320(CX)
	VMOVUPS Y12, 352(CX)
	VMOVUPS Y13, 384(CX)
	VMOVUPS Y14, 416(CX)
	VMOVUPS Y15, 448(CX)
	ADDQ    $0x000001e0, AX
	ADDQ    $0x000001e0, CX
	SUBQ    $0x00000078, DX
	JMP     float32SubScalarBlockLoop

float32SubScalarTailLoop:
	CMPQ    DX, $0x00000008
	JL      float32SubScalarDone
	VSUBPS  (AX), Y0, Y1
	VMOVUPS Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000008, DX
	JMP     float32SubScalarTailLoop

float32SubScalarDone:
	CMPQ    DX, $0x00000004
	JL      float32SubScalarDone1
	VSUBPS  (AX), X0, X1
	VMOVUPS X1, (CX)

float32SubScalarDone1:
	RET

// func float32SubByScalarAvx2Asm(x float32, y []float32, r []float32)
// Requires: AVX, AVX2, SSE
TEXT ·float32SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVSS        x+0(FP), X0
	MOVQ         y_base+8(FP), AX
	MOVQ         r_base+32(FP), CX
	MOVQ         y_len+16(FP), DX
	VBROADCASTSS X0, Y0

float32SubByScalarBlockLoop:
	CMPQ    DX, $0x00000078
	JL      float32SubByScalarTailLoop
	VMOVUPS (AX), Y1
	VMOVUPS 32(AX), Y2
	VMOVUPS 64(AX), Y3
	VMOVUPS 96(AX), Y4
	VMOVUPS 128(AX), Y5
	VMOVUPS 160(AX), Y6
	VMOVUPS 192(AX), Y7
	VMOVUPS 224(AX), Y8
	VMOVUPS 256(AX), Y9
	VMOVUPS 288(AX), Y10
	VMOVUPS 320(AX), Y11
	VMOVUPS 352(AX), Y12
	VMOVUPS 384(AX), Y13
	VMOVUPS 416(AX), Y14
	VMOVUPS 448(AX), Y15
	VSUBPS  Y0, Y1, Y1
	VMOVUPS Y1, (CX)
	VSUBPS  Y0, Y2, Y2
	VMOVUPS Y2, 32(CX)
	VSUBPS  Y0, Y3, Y3
	VMOVUPS Y3, 64(CX)
	VSUBPS  Y0, Y4, Y4
	VMOVUPS Y4, 96(CX)
	VSUBPS  Y0, Y5, Y5
	VMOVUPS Y5, 128(CX)
	VSUBPS  Y0, Y6, Y6
	VMOVUPS Y6, 160(CX)
	VSUBPS  Y0, Y7, Y7
	VMOVUPS Y7, 192(CX)
	VSUBPS  Y0, Y8, Y8
	VMOVUPS Y8, 224(CX)
	VSUBPS  Y0, Y9, Y9
	VMOVUPS Y9, 256(CX)
	VSUBPS  Y0, Y10, Y10
	VMOVUPS Y10, 288(CX)
	VSUBPS  Y0, Y11, Y11
	VMOVUPS Y11, 320(CX)
	VSUBPS  Y0, Y12, Y12
	VMOVUPS Y12, 352(CX)
	VSUBPS  Y0, Y13, Y13
	VMOVUPS Y13, 384(CX)
	VSUBPS  Y0, Y14, Y14
	VMOVUPS Y14, 416(CX)
	VSUBPS  Y0, Y15, Y15
	VMOVUPS Y15, 448(CX)
	ADDQ    $0x000001e0, AX
	ADDQ    $0x000001e0, CX
	SUBQ    $0x00000078, DX
	JMP     float32SubByScalarBlockLoop

float32SubByScalarTailLoop:
	CMPQ    DX, $0x00000008
	JL      float32SubByScalarDone
	VMOVUPS (AX), Y1
	VSUBPS  Y0, Y1, Y1
	VMOVUPS Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000008, DX
	JMP     float32SubByScalarTailLoop

float32SubByScalarDone:
	CMPQ    DX, $0x00000004
	JL      float32SubByScalarDone1
	VMOVUPS (AX), X1
	VSUBPS  X0, X1, X1
	VMOVUPS X1, (CX)

float32SubByScalarDone1:
	RET

// func float64SubAvx2Asm(x []float64, y []float64, r []float64)
// Requires: AVX
TEXT ·float64SubAvx2Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

float64SubBlockLoop:
	CMPQ    BX, $0x00000040
	JL      float64SubTailLoop
	VMOVUPD (AX), Y0
	VMOVUPD 32(AX), Y1
	VMOVUPD 64(AX), Y2
	VMOVUPD 96(AX), Y3
	VMOVUPD 128(AX), Y4
	VMOVUPD 160(AX), Y5
	VMOVUPD 192(AX), Y6
	VMOVUPD 224(AX), Y7
	VMOVUPD 256(AX), Y8
	VMOVUPD 288(AX), Y9
	VMOVUPD 320(AX), Y10
	VMOVUPD 352(AX), Y11
	VMOVUPD 384(AX), Y12
	VMOVUPD 416(AX), Y13
	VMOVUPD 448(AX), Y14
	VMOVUPD 480(AX), Y15
	VSUBPD  (CX), Y0, Y0
	VMOVUPD Y0, (DX)
	VSUBPD  32(CX), Y1, Y1
	VMOVUPD Y1, 32(DX)
	VSUBPD  64(CX), Y2, Y2
	VMOVUPD Y2, 64(DX)
	VSUBPD  96(CX), Y3, Y3
	VMOVUPD Y3, 96(DX)
	VSUBPD  128(CX), Y4, Y4
	VMOVUPD Y4, 128(DX)
	VSUBPD  160(CX), Y5, Y5
	VMOVUPD Y5, 160(DX)
	VSUBPD  192(CX), Y6, Y6
	VMOVUPD Y6, 192(DX)
	VSUBPD  224(CX), Y7, Y7
	VMOVUPD Y7, 224(DX)
	VSUBPD  256(CX), Y8, Y8
	VMOVUPD Y8, 256(DX)
	VSUBPD  288(CX), Y9, Y9
	VMOVUPD Y9, 288(DX)
	VSUBPD  320(CX), Y10, Y10
	VMOVUPD Y10, 320(DX)
	VSUBPD  352(CX), Y11, Y11
	VMOVUPD Y11, 352(DX)
	VSUBPD  384(CX), Y12, Y12
	VMOVUPD Y12, 384(DX)
	VSUBPD  416(CX), Y13, Y13
	VMOVUPD Y13, 416(DX)
	VSUBPD  448(CX), Y14, Y14
	VMOVUPD Y14, 448(DX)
	VSUBPD  480(CX), Y15, Y15
	VMOVUPD Y15, 480(DX)
	ADDQ    $0x00000200, AX
	ADDQ    $0x00000200, CX
	ADDQ    $0x00000200, DX
	SUBQ    $0x00000040, BX
	JMP     float64SubBlockLoop

float64SubTailLoop:
	CMPQ    BX, $0x00000004
	JL      float64SubDone
	VMOVUPD (AX), Y0
	VSUBPD  (CX), Y0, Y0
	VMOVUPD Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000004, BX
	JMP     float64SubTailLoop

float64SubDone:
	CMPQ    BX, $0x00000002
	JL      float64SubDone1
	VMOVUPD (AX), X0
	VSUBPD  (CX), X0, X0
	VMOVUPD X0, (DX)

float64SubDone1:
	RET

// func float64SubScalarAvx2Asm(x float64, y []float64, r []float64)
// Requires: AVX, AVX2, SSE2
TEXT ·float64SubScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVSD        x+0(FP), X0
	MOVQ         y_base+8(FP), AX
	MOVQ         r_base+32(FP), CX
	MOVQ         y_len+16(FP), DX
	VBROADCASTSD X0, Y0

float64SubScalarBlockLoop:
	CMPQ    DX, $0x0000003c
	JL      float64SubScalarTailLoop
	VSUBPD  (AX), Y0, Y1
	VSUBPD  32(AX), Y0, Y2
	VSUBPD  64(AX), Y0, Y3
	VSUBPD  96(AX), Y0, Y4
	VSUBPD  128(AX), Y0, Y5
	VSUBPD  160(AX), Y0, Y6
	VSUBPD  192(AX), Y0, Y7
	VSUBPD  224(AX), Y0, Y8
	VSUBPD  256(AX), Y0, Y9
	VSUBPD  288(AX), Y0, Y10
	VSUBPD  320(AX), Y0, Y11
	VSUBPD  352(AX), Y0, Y12
	VSUBPD  384(AX), Y0, Y13
	VSUBPD  416(AX), Y0, Y14
	VSUBPD  448(AX), Y0, Y15
	VMOVUPD Y1, (CX)
	VMOVUPD Y2, 32(CX)
	VMOVUPD Y3, 64(CX)
	VMOVUPD Y4, 96(CX)
	VMOVUPD Y5, 128(CX)
	VMOVUPD Y6, 160(CX)
	VMOVUPD Y7, 192(CX)
	VMOVUPD Y8, 224(CX)
	VMOVUPD Y9, 256(CX)
	VMOVUPD Y10, 288(CX)
	VMOVUPD Y11, 320(CX)
	VMOVUPD Y12, 352(CX)
	VMOVUPD Y13, 384(CX)
	VMOVUPD Y14, 416(CX)
	VMOVUPD Y15, 448(CX)
	ADDQ    $0x000001e0, AX
	ADDQ    $0x000001e0, CX
	SUBQ    $0x0000003c, DX
	JMP     float64SubScalarBlockLoop

float64SubScalarTailLoop:
	CMPQ    DX, $0x00000004
	JL      float64SubScalarDone
	VSUBPD  (AX), Y0, Y1
	VMOVUPD Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000004, DX
	JMP     float64SubScalarTailLoop

float64SubScalarDone:
	CMPQ    DX, $0x00000002
	JL      float64SubScalarDone1
	VSUBPD  (AX), X0, X1
	VMOVUPD X1, (CX)

float64SubScalarDone1:
	RET

// func float64SubByScalarAvx2Asm(x float64, y []float64, r []float64)
// Requires: AVX, AVX2, SSE2
TEXT ·float64SubByScalarAvx2Asm(SB), NOSPLIT, $0-56
	MOVSD        x+0(FP), X0
	MOVQ         y_base+8(FP), AX
	MOVQ         r_base+32(FP), CX
	MOVQ         y_len+16(FP), DX
	VBROADCASTSD X0, Y0

float64SubByScalarBlockLoop:
	CMPQ    DX, $0x0000003c
	JL      float64SubByScalarTailLoop
	VMOVUPD (AX), Y1
	VMOVUPD 32(AX), Y2
	VMOVUPD 64(AX), Y3
	VMOVUPD 96(AX), Y4
	VMOVUPD 128(AX), Y5
	VMOVUPD 160(AX), Y6
	VMOVUPD 192(AX), Y7
	VMOVUPD 224(AX), Y8
	VMOVUPD 256(AX), Y9
	VMOVUPD 288(AX), Y10
	VMOVUPD 320(AX), Y11
	VMOVUPD 352(AX), Y12
	VMOVUPD 384(AX), Y13
	VMOVUPD 416(AX), Y14
	VMOVUPD 448(AX), Y15
	VSUBPD  Y0, Y1, Y1
	VMOVUPD Y1, (CX)
	VSUBPD  Y0, Y2, Y2
	VMOVUPD Y2, 32(CX)
	VSUBPD  Y0, Y3, Y3
	VMOVUPD Y3, 64(CX)
	VSUBPD  Y0, Y4, Y4
	VMOVUPD Y4, 96(CX)
	VSUBPD  Y0, Y5, Y5
	VMOVUPD Y5, 128(CX)
	VSUBPD  Y0, Y6, Y6
	VMOVUPD Y6, 160(CX)
	VSUBPD  Y0, Y7, Y7
	VMOVUPD Y7, 192(CX)
	VSUBPD  Y0, Y8, Y8
	VMOVUPD Y8, 224(CX)
	VSUBPD  Y0, Y9, Y9
	VMOVUPD Y9, 256(CX)
	VSUBPD  Y0, Y10, Y10
	VMOVUPD Y10, 288(CX)
	VSUBPD  Y0, Y11, Y11
	VMOVUPD Y11, 320(CX)
	VSUBPD  Y0, Y12, Y12
	VMOVUPD Y12, 352(CX)
	VSUBPD  Y0, Y13, Y13
	VMOVUPD Y13, 384(CX)
	VSUBPD  Y0, Y14, Y14
	VMOVUPD Y14, 416(CX)
	VSUBPD  Y0, Y15, Y15
	VMOVUPD Y15, 448(CX)
	ADDQ    $0x000001e0, AX
	ADDQ    $0x000001e0, CX
	SUBQ    $0x0000003c, DX
	JMP     float64SubByScalarBlockLoop

float64SubByScalarTailLoop:
	CMPQ    DX, $0x00000004
	JL      float64SubByScalarDone
	VMOVUPD (AX), Y1
	VSUBPD  Y0, Y1, Y1
	VMOVUPD Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000004, DX
	JMP     float64SubByScalarTailLoop

float64SubByScalarDone:
	CMPQ    DX, $0x00000002
	JL      float64SubByScalarDone1
	VMOVUPD (AX), X1
	VSUBPD  X0, X1, X1
	VMOVUPD X1, (CX)

float64SubByScalarDone1:
	RET
