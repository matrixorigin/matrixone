// Code generated by command: go run avx512.go -out minus/avx512.s -stubs minus/avx512_stubs.go. DO NOT EDIT.

#include "textflag.h"

// func int8MinusAvx512Asm(x []int8, y []int8, r []int8)
// Requires: AVX512BW, AVX512F
TEXT ·int8MinusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int8MinusBlockLoop:
	CMPQ      BX, $0x00000300
	JL        int8MinusTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPSUBB    (CX), Z0, Z0
	VPSUBB    64(CX), Z1, Z1
	VPSUBB    128(CX), Z2, Z2
	VPSUBB    192(CX), Z3, Z3
	VPSUBB    256(CX), Z4, Z4
	VPSUBB    320(CX), Z5, Z5
	VPSUBB    384(CX), Z6, Z6
	VPSUBB    448(CX), Z7, Z7
	VPSUBB    512(CX), Z8, Z8
	VPSUBB    576(CX), Z9, Z9
	VPSUBB    640(CX), Z10, Z10
	VPSUBB    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000300, BX
	JMP       int8MinusBlockLoop

int8MinusTailLoop:
	CMPQ      BX, $0x00000040
	JL        int8MinusDone
	VMOVDQU32 (AX), Z0
	VPSUBB    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000040, BX
	JMP       int8MinusTailLoop

int8MinusDone:
	RET

// func int8MinusScalarAvx512Asm(x int8, y []int8, r []int8)
// Requires: AVX512BW, AVX512F, SSE2
TEXT ·int8MinusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVBLSX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTB X0, Z0

int8MinusScalarBlockLoop:
	CMPQ      BX, $0x00000300
	JL        int8MinusScalarTailLoop
	VMOVDQU32 (CX), Z1
	VMOVDQU32 64(CX), Z2
	VMOVDQU32 128(CX), Z3
	VMOVDQU32 192(CX), Z4
	VMOVDQU32 256(CX), Z5
	VMOVDQU32 320(CX), Z6
	VMOVDQU32 384(CX), Z7
	VMOVDQU32 448(CX), Z8
	VMOVDQU32 512(CX), Z9
	VMOVDQU32 576(CX), Z10
	VMOVDQU32 640(CX), Z11
	VMOVDQU32 704(CX), Z12
	VPSUBB    Z0, Z1, Z1
	VPSUBB    Z0, Z2, Z2
	VPSUBB    Z0, Z3, Z3
	VPSUBB    Z0, Z4, Z4
	VPSUBB    Z0, Z5, Z5
	VPSUBB    Z0, Z6, Z6
	VPSUBB    Z0, Z7, Z7
	VPSUBB    Z0, Z8, Z8
	VPSUBB    Z0, Z9, Z9
	VPSUBB    Z0, Z10, Z10
	VPSUBB    Z0, Z11, Z11
	VPSUBB    Z0, Z12, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000300, BX
	JMP       int8MinusScalarBlockLoop

int8MinusScalarTailLoop:
	CMPQ      BX, $0x00000040
	JL        int8MinusScalarDone
	VMOVDQU32 (CX), Z1
	VPSUBB    Z0, Z1, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000040, BX
	JMP       int8MinusScalarTailLoop

int8MinusScalarDone:
	RET

// func int8MinusByScalarAvx512Asm(x int8, y []int8, r []int8)
// Requires: AVX512BW, AVX512F, SSE2
TEXT ·int8MinusByScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVBLSX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTB X0, Z0

int8MinusByScalarBlockLoop:
	CMPQ      BX, $0x00000300
	JL        int8MinusByScalarTailLoop
	VPSUBB    (CX), Z0, Z1
	VPSUBB    64(CX), Z0, Z2
	VPSUBB    128(CX), Z0, Z3
	VPSUBB    192(CX), Z0, Z4
	VPSUBB    256(CX), Z0, Z5
	VPSUBB    320(CX), Z0, Z6
	VPSUBB    384(CX), Z0, Z7
	VPSUBB    448(CX), Z0, Z8
	VPSUBB    512(CX), Z0, Z9
	VPSUBB    576(CX), Z0, Z10
	VPSUBB    640(CX), Z0, Z11
	VPSUBB    704(CX), Z0, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000300, BX
	JMP       int8MinusByScalarBlockLoop

int8MinusByScalarTailLoop:
	CMPQ      BX, $0x00000040
	JL        int8MinusByScalarDone
	VPSUBB    (CX), Z0, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000040, BX
	JMP       int8MinusByScalarTailLoop

int8MinusByScalarDone:
	RET

// func int16MinusAvx512Asm(x []int16, y []int16, r []int16)
// Requires: AVX512BW, AVX512F
TEXT ·int16MinusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int16MinusBlockLoop:
	CMPQ      BX, $0x00000180
	JL        int16MinusTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPSUBW    (CX), Z0, Z0
	VPSUBW    64(CX), Z1, Z1
	VPSUBW    128(CX), Z2, Z2
	VPSUBW    192(CX), Z3, Z3
	VPSUBW    256(CX), Z4, Z4
	VPSUBW    320(CX), Z5, Z5
	VPSUBW    384(CX), Z6, Z6
	VPSUBW    448(CX), Z7, Z7
	VPSUBW    512(CX), Z8, Z8
	VPSUBW    576(CX), Z9, Z9
	VPSUBW    640(CX), Z10, Z10
	VPSUBW    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000180, BX
	JMP       int16MinusBlockLoop

int16MinusTailLoop:
	CMPQ      BX, $0x00000020
	JL        int16MinusDone
	VMOVDQU32 (AX), Z0
	VPSUBW    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000020, BX
	JMP       int16MinusTailLoop

int16MinusDone:
	RET

// func int16MinusScalarAvx512Asm(x int16, y []int16, r []int16)
// Requires: AVX512BW, AVX512F, SSE2
TEXT ·int16MinusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVWLSX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTW X0, Z0

int16MinusScalarBlockLoop:
	CMPQ      BX, $0x00000180
	JL        int16MinusScalarTailLoop
	VMOVDQU32 (CX), Z1
	VMOVDQU32 64(CX), Z2
	VMOVDQU32 128(CX), Z3
	VMOVDQU32 192(CX), Z4
	VMOVDQU32 256(CX), Z5
	VMOVDQU32 320(CX), Z6
	VMOVDQU32 384(CX), Z7
	VMOVDQU32 448(CX), Z8
	VMOVDQU32 512(CX), Z9
	VMOVDQU32 576(CX), Z10
	VMOVDQU32 640(CX), Z11
	VMOVDQU32 704(CX), Z12
	VPSUBW    Z0, Z1, Z1
	VPSUBW    Z0, Z2, Z2
	VPSUBW    Z0, Z3, Z3
	VPSUBW    Z0, Z4, Z4
	VPSUBW    Z0, Z5, Z5
	VPSUBW    Z0, Z6, Z6
	VPSUBW    Z0, Z7, Z7
	VPSUBW    Z0, Z8, Z8
	VPSUBW    Z0, Z9, Z9
	VPSUBW    Z0, Z10, Z10
	VPSUBW    Z0, Z11, Z11
	VPSUBW    Z0, Z12, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000180, BX
	JMP       int16MinusScalarBlockLoop

int16MinusScalarTailLoop:
	CMPQ      BX, $0x00000020
	JL        int16MinusScalarDone
	VMOVDQU32 (CX), Z1
	VPSUBW    Z0, Z1, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000020, BX
	JMP       int16MinusScalarTailLoop

int16MinusScalarDone:
	RET

// func int16MinusByScalarAvx512Asm(x int16, y []int16, r []int16)
// Requires: AVX512BW, AVX512F, SSE2
TEXT ·int16MinusByScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVWLSX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTW X0, Z0

int16MinusByScalarBlockLoop:
	CMPQ      BX, $0x00000180
	JL        int16MinusByScalarTailLoop
	VPSUBW    (CX), Z0, Z1
	VPSUBW    64(CX), Z0, Z2
	VPSUBW    128(CX), Z0, Z3
	VPSUBW    192(CX), Z0, Z4
	VPSUBW    256(CX), Z0, Z5
	VPSUBW    320(CX), Z0, Z6
	VPSUBW    384(CX), Z0, Z7
	VPSUBW    448(CX), Z0, Z8
	VPSUBW    512(CX), Z0, Z9
	VPSUBW    576(CX), Z0, Z10
	VPSUBW    640(CX), Z0, Z11
	VPSUBW    704(CX), Z0, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000180, BX
	JMP       int16MinusByScalarBlockLoop

int16MinusByScalarTailLoop:
	CMPQ      BX, $0x00000020
	JL        int16MinusByScalarDone
	VPSUBW    (CX), Z0, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000020, BX
	JMP       int16MinusByScalarTailLoop

int16MinusByScalarDone:
	RET

// func int32MinusAvx512Asm(x []int32, y []int32, r []int32)
// Requires: AVX512F
TEXT ·int32MinusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int32MinusBlockLoop:
	CMPQ      BX, $0x000000c0
	JL        int32MinusTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPSUBD    (CX), Z0, Z0
	VPSUBD    64(CX), Z1, Z1
	VPSUBD    128(CX), Z2, Z2
	VPSUBD    192(CX), Z3, Z3
	VPSUBD    256(CX), Z4, Z4
	VPSUBD    320(CX), Z5, Z5
	VPSUBD    384(CX), Z6, Z6
	VPSUBD    448(CX), Z7, Z7
	VPSUBD    512(CX), Z8, Z8
	VPSUBD    576(CX), Z9, Z9
	VPSUBD    640(CX), Z10, Z10
	VPSUBD    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x000000c0, BX
	JMP       int32MinusBlockLoop

int32MinusTailLoop:
	CMPQ      BX, $0x00000010
	JL        int32MinusDone
	VMOVDQU32 (AX), Z0
	VPSUBD    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000010, BX
	JMP       int32MinusTailLoop

int32MinusDone:
	RET

// func int32MinusScalarAvx512Asm(x int32, y []int32, r []int32)
// Requires: AVX512F, SSE2
TEXT ·int32MinusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVL         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTD X0, Z0

int32MinusScalarBlockLoop:
	CMPQ      BX, $0x000000c0
	JL        int32MinusScalarTailLoop
	VMOVDQU32 (CX), Z1
	VMOVDQU32 64(CX), Z2
	VMOVDQU32 128(CX), Z3
	VMOVDQU32 192(CX), Z4
	VMOVDQU32 256(CX), Z5
	VMOVDQU32 320(CX), Z6
	VMOVDQU32 384(CX), Z7
	VMOVDQU32 448(CX), Z8
	VMOVDQU32 512(CX), Z9
	VMOVDQU32 576(CX), Z10
	VMOVDQU32 640(CX), Z11
	VMOVDQU32 704(CX), Z12
	VPSUBD    Z0, Z1, Z1
	VPSUBD    Z0, Z2, Z2
	VPSUBD    Z0, Z3, Z3
	VPSUBD    Z0, Z4, Z4
	VPSUBD    Z0, Z5, Z5
	VPSUBD    Z0, Z6, Z6
	VPSUBD    Z0, Z7, Z7
	VPSUBD    Z0, Z8, Z8
	VPSUBD    Z0, Z9, Z9
	VPSUBD    Z0, Z10, Z10
	VPSUBD    Z0, Z11, Z11
	VPSUBD    Z0, Z12, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x000000c0, BX
	JMP       int32MinusScalarBlockLoop

int32MinusScalarTailLoop:
	CMPQ      BX, $0x00000010
	JL        int32MinusScalarDone
	VMOVDQU32 (CX), Z1
	VPSUBD    Z0, Z1, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000010, BX
	JMP       int32MinusScalarTailLoop

int32MinusScalarDone:
	RET

// func int32MinusByScalarAvx512Asm(x int32, y []int32, r []int32)
// Requires: AVX512F, SSE2
TEXT ·int32MinusByScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVL         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTD X0, Z0

int32MinusByScalarBlockLoop:
	CMPQ      BX, $0x000000c0
	JL        int32MinusByScalarTailLoop
	VPSUBD    (CX), Z0, Z1
	VPSUBD    64(CX), Z0, Z2
	VPSUBD    128(CX), Z0, Z3
	VPSUBD    192(CX), Z0, Z4
	VPSUBD    256(CX), Z0, Z5
	VPSUBD    320(CX), Z0, Z6
	VPSUBD    384(CX), Z0, Z7
	VPSUBD    448(CX), Z0, Z8
	VPSUBD    512(CX), Z0, Z9
	VPSUBD    576(CX), Z0, Z10
	VPSUBD    640(CX), Z0, Z11
	VPSUBD    704(CX), Z0, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x000000c0, BX
	JMP       int32MinusByScalarBlockLoop

int32MinusByScalarTailLoop:
	CMPQ      BX, $0x00000010
	JL        int32MinusByScalarDone
	VPSUBD    (CX), Z0, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000010, BX
	JMP       int32MinusByScalarTailLoop

int32MinusByScalarDone:
	RET

// func int64MinusAvx512Asm(x []int64, y []int64, r []int64)
// Requires: AVX512F
TEXT ·int64MinusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

int64MinusBlockLoop:
	CMPQ      BX, $0x00000060
	JL        int64MinusTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPSUBQ    (CX), Z0, Z0
	VPSUBQ    64(CX), Z1, Z1
	VPSUBQ    128(CX), Z2, Z2
	VPSUBQ    192(CX), Z3, Z3
	VPSUBQ    256(CX), Z4, Z4
	VPSUBQ    320(CX), Z5, Z5
	VPSUBQ    384(CX), Z6, Z6
	VPSUBQ    448(CX), Z7, Z7
	VPSUBQ    512(CX), Z8, Z8
	VPSUBQ    576(CX), Z9, Z9
	VPSUBQ    640(CX), Z10, Z10
	VPSUBQ    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000060, BX
	JMP       int64MinusBlockLoop

int64MinusTailLoop:
	CMPQ      BX, $0x00000008
	JL        int64MinusDone
	VMOVDQU32 (AX), Z0
	VPSUBQ    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000008, BX
	JMP       int64MinusTailLoop

int64MinusDone:
	RET

// func int64MinusScalarAvx512Asm(x int64, y []int64, r []int64)
// Requires: AVX512F, SSE2
TEXT ·int64MinusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVQ         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVQ         AX, X0
	VPBROADCASTQ X0, Z0

int64MinusScalarBlockLoop:
	CMPQ      BX, $0x00000060
	JL        int64MinusScalarTailLoop
	VMOVDQU32 (CX), Z1
	VMOVDQU32 64(CX), Z2
	VMOVDQU32 128(CX), Z3
	VMOVDQU32 192(CX), Z4
	VMOVDQU32 256(CX), Z5
	VMOVDQU32 320(CX), Z6
	VMOVDQU32 384(CX), Z7
	VMOVDQU32 448(CX), Z8
	VMOVDQU32 512(CX), Z9
	VMOVDQU32 576(CX), Z10
	VMOVDQU32 640(CX), Z11
	VMOVDQU32 704(CX), Z12
	VPSUBQ    Z0, Z1, Z1
	VPSUBQ    Z0, Z2, Z2
	VPSUBQ    Z0, Z3, Z3
	VPSUBQ    Z0, Z4, Z4
	VPSUBQ    Z0, Z5, Z5
	VPSUBQ    Z0, Z6, Z6
	VPSUBQ    Z0, Z7, Z7
	VPSUBQ    Z0, Z8, Z8
	VPSUBQ    Z0, Z9, Z9
	VPSUBQ    Z0, Z10, Z10
	VPSUBQ    Z0, Z11, Z11
	VPSUBQ    Z0, Z12, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000060, BX
	JMP       int64MinusScalarBlockLoop

int64MinusScalarTailLoop:
	CMPQ      BX, $0x00000008
	JL        int64MinusScalarDone
	VMOVDQU32 (CX), Z1
	VPSUBQ    Z0, Z1, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000008, BX
	JMP       int64MinusScalarTailLoop

int64MinusScalarDone:
	RET

// func int64MinusByScalarAvx512Asm(x int64, y []int64, r []int64)
// Requires: AVX512F, SSE2
TEXT ·int64MinusByScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVQ         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVQ         AX, X0
	VPBROADCASTQ X0, Z0

int64MinusByScalarBlockLoop:
	CMPQ      BX, $0x00000060
	JL        int64MinusByScalarTailLoop
	VPSUBQ    (CX), Z0, Z1
	VPSUBQ    64(CX), Z0, Z2
	VPSUBQ    128(CX), Z0, Z3
	VPSUBQ    192(CX), Z0, Z4
	VPSUBQ    256(CX), Z0, Z5
	VPSUBQ    320(CX), Z0, Z6
	VPSUBQ    384(CX), Z0, Z7
	VPSUBQ    448(CX), Z0, Z8
	VPSUBQ    512(CX), Z0, Z9
	VPSUBQ    576(CX), Z0, Z10
	VPSUBQ    640(CX), Z0, Z11
	VPSUBQ    704(CX), Z0, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000060, BX
	JMP       int64MinusByScalarBlockLoop

int64MinusByScalarTailLoop:
	CMPQ      BX, $0x00000008
	JL        int64MinusByScalarDone
	VPSUBQ    (CX), Z0, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000008, BX
	JMP       int64MinusByScalarTailLoop

int64MinusByScalarDone:
	RET

// func uint8MinusAvx512Asm(x []uint8, y []uint8, r []uint8)
// Requires: AVX512BW, AVX512F
TEXT ·uint8MinusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint8MinusBlockLoop:
	CMPQ      BX, $0x00000300
	JL        uint8MinusTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPSUBB    (CX), Z0, Z0
	VPSUBB    64(CX), Z1, Z1
	VPSUBB    128(CX), Z2, Z2
	VPSUBB    192(CX), Z3, Z3
	VPSUBB    256(CX), Z4, Z4
	VPSUBB    320(CX), Z5, Z5
	VPSUBB    384(CX), Z6, Z6
	VPSUBB    448(CX), Z7, Z7
	VPSUBB    512(CX), Z8, Z8
	VPSUBB    576(CX), Z9, Z9
	VPSUBB    640(CX), Z10, Z10
	VPSUBB    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000300, BX
	JMP       uint8MinusBlockLoop

uint8MinusTailLoop:
	CMPQ      BX, $0x00000040
	JL        uint8MinusDone
	VMOVDQU32 (AX), Z0
	VPSUBB    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000040, BX
	JMP       uint8MinusTailLoop

uint8MinusDone:
	RET

// func uint8MinusScalarAvx512Asm(x uint8, y []uint8, r []uint8)
// Requires: AVX512BW, AVX512F, SSE2
TEXT ·uint8MinusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVBLZX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTB X0, Z0

uint8MinusScalarBlockLoop:
	CMPQ      BX, $0x00000300
	JL        uint8MinusScalarTailLoop
	VMOVDQU32 (CX), Z1
	VMOVDQU32 64(CX), Z2
	VMOVDQU32 128(CX), Z3
	VMOVDQU32 192(CX), Z4
	VMOVDQU32 256(CX), Z5
	VMOVDQU32 320(CX), Z6
	VMOVDQU32 384(CX), Z7
	VMOVDQU32 448(CX), Z8
	VMOVDQU32 512(CX), Z9
	VMOVDQU32 576(CX), Z10
	VMOVDQU32 640(CX), Z11
	VMOVDQU32 704(CX), Z12
	VPSUBB    Z0, Z1, Z1
	VPSUBB    Z0, Z2, Z2
	VPSUBB    Z0, Z3, Z3
	VPSUBB    Z0, Z4, Z4
	VPSUBB    Z0, Z5, Z5
	VPSUBB    Z0, Z6, Z6
	VPSUBB    Z0, Z7, Z7
	VPSUBB    Z0, Z8, Z8
	VPSUBB    Z0, Z9, Z9
	VPSUBB    Z0, Z10, Z10
	VPSUBB    Z0, Z11, Z11
	VPSUBB    Z0, Z12, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000300, BX
	JMP       uint8MinusScalarBlockLoop

uint8MinusScalarTailLoop:
	CMPQ      BX, $0x00000040
	JL        uint8MinusScalarDone
	VMOVDQU32 (CX), Z1
	VPSUBB    Z0, Z1, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000040, BX
	JMP       uint8MinusScalarTailLoop

uint8MinusScalarDone:
	RET

// func uint8MinusByScalarAvx512Asm(x uint8, y []uint8, r []uint8)
// Requires: AVX512BW, AVX512F, SSE2
TEXT ·uint8MinusByScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVBLZX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTB X0, Z0

uint8MinusByScalarBlockLoop:
	CMPQ      BX, $0x00000300
	JL        uint8MinusByScalarTailLoop
	VPSUBB    (CX), Z0, Z1
	VPSUBB    64(CX), Z0, Z2
	VPSUBB    128(CX), Z0, Z3
	VPSUBB    192(CX), Z0, Z4
	VPSUBB    256(CX), Z0, Z5
	VPSUBB    320(CX), Z0, Z6
	VPSUBB    384(CX), Z0, Z7
	VPSUBB    448(CX), Z0, Z8
	VPSUBB    512(CX), Z0, Z9
	VPSUBB    576(CX), Z0, Z10
	VPSUBB    640(CX), Z0, Z11
	VPSUBB    704(CX), Z0, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000300, BX
	JMP       uint8MinusByScalarBlockLoop

uint8MinusByScalarTailLoop:
	CMPQ      BX, $0x00000040
	JL        uint8MinusByScalarDone
	VPSUBB    (CX), Z0, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000040, BX
	JMP       uint8MinusByScalarTailLoop

uint8MinusByScalarDone:
	RET

// func uint16MinusAvx512Asm(x []uint16, y []uint16, r []uint16)
// Requires: AVX512BW, AVX512F
TEXT ·uint16MinusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint16MinusBlockLoop:
	CMPQ      BX, $0x00000180
	JL        uint16MinusTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPSUBW    (CX), Z0, Z0
	VPSUBW    64(CX), Z1, Z1
	VPSUBW    128(CX), Z2, Z2
	VPSUBW    192(CX), Z3, Z3
	VPSUBW    256(CX), Z4, Z4
	VPSUBW    320(CX), Z5, Z5
	VPSUBW    384(CX), Z6, Z6
	VPSUBW    448(CX), Z7, Z7
	VPSUBW    512(CX), Z8, Z8
	VPSUBW    576(CX), Z9, Z9
	VPSUBW    640(CX), Z10, Z10
	VPSUBW    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000180, BX
	JMP       uint16MinusBlockLoop

uint16MinusTailLoop:
	CMPQ      BX, $0x00000020
	JL        uint16MinusDone
	VMOVDQU32 (AX), Z0
	VPSUBW    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000020, BX
	JMP       uint16MinusTailLoop

uint16MinusDone:
	RET

// func uint16MinusScalarAvx512Asm(x uint16, y []uint16, r []uint16)
// Requires: AVX512BW, AVX512F, SSE2
TEXT ·uint16MinusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVWLZX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTW X0, Z0

uint16MinusScalarBlockLoop:
	CMPQ      BX, $0x00000180
	JL        uint16MinusScalarTailLoop
	VMOVDQU32 (CX), Z1
	VMOVDQU32 64(CX), Z2
	VMOVDQU32 128(CX), Z3
	VMOVDQU32 192(CX), Z4
	VMOVDQU32 256(CX), Z5
	VMOVDQU32 320(CX), Z6
	VMOVDQU32 384(CX), Z7
	VMOVDQU32 448(CX), Z8
	VMOVDQU32 512(CX), Z9
	VMOVDQU32 576(CX), Z10
	VMOVDQU32 640(CX), Z11
	VMOVDQU32 704(CX), Z12
	VPSUBW    Z0, Z1, Z1
	VPSUBW    Z0, Z2, Z2
	VPSUBW    Z0, Z3, Z3
	VPSUBW    Z0, Z4, Z4
	VPSUBW    Z0, Z5, Z5
	VPSUBW    Z0, Z6, Z6
	VPSUBW    Z0, Z7, Z7
	VPSUBW    Z0, Z8, Z8
	VPSUBW    Z0, Z9, Z9
	VPSUBW    Z0, Z10, Z10
	VPSUBW    Z0, Z11, Z11
	VPSUBW    Z0, Z12, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000180, BX
	JMP       uint16MinusScalarBlockLoop

uint16MinusScalarTailLoop:
	CMPQ      BX, $0x00000020
	JL        uint16MinusScalarDone
	VMOVDQU32 (CX), Z1
	VPSUBW    Z0, Z1, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000020, BX
	JMP       uint16MinusScalarTailLoop

uint16MinusScalarDone:
	RET

// func uint16MinusByScalarAvx512Asm(x uint16, y []uint16, r []uint16)
// Requires: AVX512BW, AVX512F, SSE2
TEXT ·uint16MinusByScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVWLZX      x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTW X0, Z0

uint16MinusByScalarBlockLoop:
	CMPQ      BX, $0x00000180
	JL        uint16MinusByScalarTailLoop
	VPSUBW    (CX), Z0, Z1
	VPSUBW    64(CX), Z0, Z2
	VPSUBW    128(CX), Z0, Z3
	VPSUBW    192(CX), Z0, Z4
	VPSUBW    256(CX), Z0, Z5
	VPSUBW    320(CX), Z0, Z6
	VPSUBW    384(CX), Z0, Z7
	VPSUBW    448(CX), Z0, Z8
	VPSUBW    512(CX), Z0, Z9
	VPSUBW    576(CX), Z0, Z10
	VPSUBW    640(CX), Z0, Z11
	VPSUBW    704(CX), Z0, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000180, BX
	JMP       uint16MinusByScalarBlockLoop

uint16MinusByScalarTailLoop:
	CMPQ      BX, $0x00000020
	JL        uint16MinusByScalarDone
	VPSUBW    (CX), Z0, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000020, BX
	JMP       uint16MinusByScalarTailLoop

uint16MinusByScalarDone:
	RET

// func uint32MinusAvx512Asm(x []uint32, y []uint32, r []uint32)
// Requires: AVX512F
TEXT ·uint32MinusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint32MinusBlockLoop:
	CMPQ      BX, $0x000000c0
	JL        uint32MinusTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPSUBD    (CX), Z0, Z0
	VPSUBD    64(CX), Z1, Z1
	VPSUBD    128(CX), Z2, Z2
	VPSUBD    192(CX), Z3, Z3
	VPSUBD    256(CX), Z4, Z4
	VPSUBD    320(CX), Z5, Z5
	VPSUBD    384(CX), Z6, Z6
	VPSUBD    448(CX), Z7, Z7
	VPSUBD    512(CX), Z8, Z8
	VPSUBD    576(CX), Z9, Z9
	VPSUBD    640(CX), Z10, Z10
	VPSUBD    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x000000c0, BX
	JMP       uint32MinusBlockLoop

uint32MinusTailLoop:
	CMPQ      BX, $0x00000010
	JL        uint32MinusDone
	VMOVDQU32 (AX), Z0
	VPSUBD    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000010, BX
	JMP       uint32MinusTailLoop

uint32MinusDone:
	RET

// func uint32MinusScalarAvx512Asm(x uint32, y []uint32, r []uint32)
// Requires: AVX512F, SSE2
TEXT ·uint32MinusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVL         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTD X0, Z0

uint32MinusScalarBlockLoop:
	CMPQ      BX, $0x000000c0
	JL        uint32MinusScalarTailLoop
	VMOVDQU32 (CX), Z1
	VMOVDQU32 64(CX), Z2
	VMOVDQU32 128(CX), Z3
	VMOVDQU32 192(CX), Z4
	VMOVDQU32 256(CX), Z5
	VMOVDQU32 320(CX), Z6
	VMOVDQU32 384(CX), Z7
	VMOVDQU32 448(CX), Z8
	VMOVDQU32 512(CX), Z9
	VMOVDQU32 576(CX), Z10
	VMOVDQU32 640(CX), Z11
	VMOVDQU32 704(CX), Z12
	VPSUBD    Z0, Z1, Z1
	VPSUBD    Z0, Z2, Z2
	VPSUBD    Z0, Z3, Z3
	VPSUBD    Z0, Z4, Z4
	VPSUBD    Z0, Z5, Z5
	VPSUBD    Z0, Z6, Z6
	VPSUBD    Z0, Z7, Z7
	VPSUBD    Z0, Z8, Z8
	VPSUBD    Z0, Z9, Z9
	VPSUBD    Z0, Z10, Z10
	VPSUBD    Z0, Z11, Z11
	VPSUBD    Z0, Z12, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x000000c0, BX
	JMP       uint32MinusScalarBlockLoop

uint32MinusScalarTailLoop:
	CMPQ      BX, $0x00000010
	JL        uint32MinusScalarDone
	VMOVDQU32 (CX), Z1
	VPSUBD    Z0, Z1, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000010, BX
	JMP       uint32MinusScalarTailLoop

uint32MinusScalarDone:
	RET

// func uint32MinusByScalarAvx512Asm(x uint32, y []uint32, r []uint32)
// Requires: AVX512F, SSE2
TEXT ·uint32MinusByScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVL         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVD         AX, X0
	VPBROADCASTD X0, Z0

uint32MinusByScalarBlockLoop:
	CMPQ      BX, $0x000000c0
	JL        uint32MinusByScalarTailLoop
	VPSUBD    (CX), Z0, Z1
	VPSUBD    64(CX), Z0, Z2
	VPSUBD    128(CX), Z0, Z3
	VPSUBD    192(CX), Z0, Z4
	VPSUBD    256(CX), Z0, Z5
	VPSUBD    320(CX), Z0, Z6
	VPSUBD    384(CX), Z0, Z7
	VPSUBD    448(CX), Z0, Z8
	VPSUBD    512(CX), Z0, Z9
	VPSUBD    576(CX), Z0, Z10
	VPSUBD    640(CX), Z0, Z11
	VPSUBD    704(CX), Z0, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x000000c0, BX
	JMP       uint32MinusByScalarBlockLoop

uint32MinusByScalarTailLoop:
	CMPQ      BX, $0x00000010
	JL        uint32MinusByScalarDone
	VPSUBD    (CX), Z0, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000010, BX
	JMP       uint32MinusByScalarTailLoop

uint32MinusByScalarDone:
	RET

// func uint64MinusAvx512Asm(x []uint64, y []uint64, r []uint64)
// Requires: AVX512F
TEXT ·uint64MinusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

uint64MinusBlockLoop:
	CMPQ      BX, $0x00000060
	JL        uint64MinusTailLoop
	VMOVDQU32 (AX), Z0
	VMOVDQU32 64(AX), Z1
	VMOVDQU32 128(AX), Z2
	VMOVDQU32 192(AX), Z3
	VMOVDQU32 256(AX), Z4
	VMOVDQU32 320(AX), Z5
	VMOVDQU32 384(AX), Z6
	VMOVDQU32 448(AX), Z7
	VMOVDQU32 512(AX), Z8
	VMOVDQU32 576(AX), Z9
	VMOVDQU32 640(AX), Z10
	VMOVDQU32 704(AX), Z11
	VPSUBQ    (CX), Z0, Z0
	VPSUBQ    64(CX), Z1, Z1
	VPSUBQ    128(CX), Z2, Z2
	VPSUBQ    192(CX), Z3, Z3
	VPSUBQ    256(CX), Z4, Z4
	VPSUBQ    320(CX), Z5, Z5
	VPSUBQ    384(CX), Z6, Z6
	VPSUBQ    448(CX), Z7, Z7
	VPSUBQ    512(CX), Z8, Z8
	VPSUBQ    576(CX), Z9, Z9
	VPSUBQ    640(CX), Z10, Z10
	VPSUBQ    704(CX), Z11, Z11
	VMOVDQU32 Z0, (DX)
	VMOVDQU32 Z1, 64(DX)
	VMOVDQU32 Z2, 128(DX)
	VMOVDQU32 Z3, 192(DX)
	VMOVDQU32 Z4, 256(DX)
	VMOVDQU32 Z5, 320(DX)
	VMOVDQU32 Z6, 384(DX)
	VMOVDQU32 Z7, 448(DX)
	VMOVDQU32 Z8, 512(DX)
	VMOVDQU32 Z9, 576(DX)
	VMOVDQU32 Z10, 640(DX)
	VMOVDQU32 Z11, 704(DX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000060, BX
	JMP       uint64MinusBlockLoop

uint64MinusTailLoop:
	CMPQ      BX, $0x00000008
	JL        uint64MinusDone
	VMOVDQU32 (AX), Z0
	VPSUBQ    (CX), Z0, Z0
	VMOVDQU32 Z0, (DX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000008, BX
	JMP       uint64MinusTailLoop

uint64MinusDone:
	RET

// func uint64MinusScalarAvx512Asm(x uint64, y []uint64, r []uint64)
// Requires: AVX512F, SSE2
TEXT ·uint64MinusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVQ         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVQ         AX, X0
	VPBROADCASTQ X0, Z0

uint64MinusScalarBlockLoop:
	CMPQ      BX, $0x00000060
	JL        uint64MinusScalarTailLoop
	VMOVDQU32 (CX), Z1
	VMOVDQU32 64(CX), Z2
	VMOVDQU32 128(CX), Z3
	VMOVDQU32 192(CX), Z4
	VMOVDQU32 256(CX), Z5
	VMOVDQU32 320(CX), Z6
	VMOVDQU32 384(CX), Z7
	VMOVDQU32 448(CX), Z8
	VMOVDQU32 512(CX), Z9
	VMOVDQU32 576(CX), Z10
	VMOVDQU32 640(CX), Z11
	VMOVDQU32 704(CX), Z12
	VPSUBQ    Z0, Z1, Z1
	VPSUBQ    Z0, Z2, Z2
	VPSUBQ    Z0, Z3, Z3
	VPSUBQ    Z0, Z4, Z4
	VPSUBQ    Z0, Z5, Z5
	VPSUBQ    Z0, Z6, Z6
	VPSUBQ    Z0, Z7, Z7
	VPSUBQ    Z0, Z8, Z8
	VPSUBQ    Z0, Z9, Z9
	VPSUBQ    Z0, Z10, Z10
	VPSUBQ    Z0, Z11, Z11
	VPSUBQ    Z0, Z12, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000060, BX
	JMP       uint64MinusScalarBlockLoop

uint64MinusScalarTailLoop:
	CMPQ      BX, $0x00000008
	JL        uint64MinusScalarDone
	VMOVDQU32 (CX), Z1
	VPSUBQ    Z0, Z1, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000008, BX
	JMP       uint64MinusScalarTailLoop

uint64MinusScalarDone:
	RET

// func uint64MinusByScalarAvx512Asm(x uint64, y []uint64, r []uint64)
// Requires: AVX512F, SSE2
TEXT ·uint64MinusByScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVQ         x+0(FP), AX
	MOVQ         y_base+8(FP), CX
	MOVQ         r_base+32(FP), DX
	MOVQ         y_len+16(FP), BX
	MOVQ         AX, X0
	VPBROADCASTQ X0, Z0

uint64MinusByScalarBlockLoop:
	CMPQ      BX, $0x00000060
	JL        uint64MinusByScalarTailLoop
	VPSUBQ    (CX), Z0, Z1
	VPSUBQ    64(CX), Z0, Z2
	VPSUBQ    128(CX), Z0, Z3
	VPSUBQ    192(CX), Z0, Z4
	VPSUBQ    256(CX), Z0, Z5
	VPSUBQ    320(CX), Z0, Z6
	VPSUBQ    384(CX), Z0, Z7
	VPSUBQ    448(CX), Z0, Z8
	VPSUBQ    512(CX), Z0, Z9
	VPSUBQ    576(CX), Z0, Z10
	VPSUBQ    640(CX), Z0, Z11
	VPSUBQ    704(CX), Z0, Z12
	VMOVDQU32 Z1, (DX)
	VMOVDQU32 Z2, 64(DX)
	VMOVDQU32 Z3, 128(DX)
	VMOVDQU32 Z4, 192(DX)
	VMOVDQU32 Z5, 256(DX)
	VMOVDQU32 Z6, 320(DX)
	VMOVDQU32 Z7, 384(DX)
	VMOVDQU32 Z8, 448(DX)
	VMOVDQU32 Z9, 512(DX)
	VMOVDQU32 Z10, 576(DX)
	VMOVDQU32 Z11, 640(DX)
	VMOVDQU32 Z12, 704(DX)
	ADDQ      $0x00000300, CX
	ADDQ      $0x00000300, DX
	SUBQ      $0x00000060, BX
	JMP       uint64MinusByScalarBlockLoop

uint64MinusByScalarTailLoop:
	CMPQ      BX, $0x00000008
	JL        uint64MinusByScalarDone
	VPSUBQ    (CX), Z0, Z1
	VMOVDQU32 Z1, (DX)
	ADDQ      $0x00000040, CX
	ADDQ      $0x00000040, DX
	SUBQ      $0x00000008, BX
	JMP       uint64MinusByScalarTailLoop

uint64MinusByScalarDone:
	RET

// func float32MinusAvx512Asm(x []float32, y []float32, r []float32)
// Requires: AVX512F
TEXT ·float32MinusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

float32MinusBlockLoop:
	CMPQ    BX, $0x000000c0
	JL      float32MinusTailLoop
	VMOVUPS (AX), Z0
	VMOVUPS 64(AX), Z1
	VMOVUPS 128(AX), Z2
	VMOVUPS 192(AX), Z3
	VMOVUPS 256(AX), Z4
	VMOVUPS 320(AX), Z5
	VMOVUPS 384(AX), Z6
	VMOVUPS 448(AX), Z7
	VMOVUPS 512(AX), Z8
	VMOVUPS 576(AX), Z9
	VMOVUPS 640(AX), Z10
	VMOVUPS 704(AX), Z11
	VSUBPS  (CX), Z0, Z0
	VSUBPS  64(CX), Z1, Z1
	VSUBPS  128(CX), Z2, Z2
	VSUBPS  192(CX), Z3, Z3
	VSUBPS  256(CX), Z4, Z4
	VSUBPS  320(CX), Z5, Z5
	VSUBPS  384(CX), Z6, Z6
	VSUBPS  448(CX), Z7, Z7
	VSUBPS  512(CX), Z8, Z8
	VSUBPS  576(CX), Z9, Z9
	VSUBPS  640(CX), Z10, Z10
	VSUBPS  704(CX), Z11, Z11
	VMOVUPS Z0, (DX)
	VMOVUPS Z1, 64(DX)
	VMOVUPS Z2, 128(DX)
	VMOVUPS Z3, 192(DX)
	VMOVUPS Z4, 256(DX)
	VMOVUPS Z5, 320(DX)
	VMOVUPS Z6, 384(DX)
	VMOVUPS Z7, 448(DX)
	VMOVUPS Z8, 512(DX)
	VMOVUPS Z9, 576(DX)
	VMOVUPS Z10, 640(DX)
	VMOVUPS Z11, 704(DX)
	ADDQ    $0x00000300, AX
	ADDQ    $0x00000300, CX
	ADDQ    $0x00000300, DX
	SUBQ    $0x000000c0, BX
	JMP     float32MinusBlockLoop

float32MinusTailLoop:
	CMPQ    BX, $0x00000010
	JL      float32MinusDone
	VMOVUPS (AX), Z0
	VSUBPS  (CX), Z0, Z0
	VMOVUPS Z0, (DX)
	ADDQ    $0x00000040, AX
	ADDQ    $0x00000040, CX
	ADDQ    $0x00000040, DX
	SUBQ    $0x00000010, BX
	JMP     float32MinusTailLoop

float32MinusDone:
	RET

// func float32MinusScalarAvx512Asm(x float32, y []float32, r []float32)
// Requires: AVX512F, SSE
TEXT ·float32MinusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVSS        x+0(FP), X0
	MOVQ         y_base+8(FP), AX
	MOVQ         r_base+32(FP), CX
	MOVQ         y_len+16(FP), DX
	VBROADCASTSS X0, Z0

float32MinusScalarBlockLoop:
	CMPQ    DX, $0x000000c0
	JL      float32MinusScalarTailLoop
	VMOVUPS (AX), Z1
	VMOVUPS 64(AX), Z2
	VMOVUPS 128(AX), Z3
	VMOVUPS 192(AX), Z4
	VMOVUPS 256(AX), Z5
	VMOVUPS 320(AX), Z6
	VMOVUPS 384(AX), Z7
	VMOVUPS 448(AX), Z8
	VMOVUPS 512(AX), Z9
	VMOVUPS 576(AX), Z10
	VMOVUPS 640(AX), Z11
	VMOVUPS 704(AX), Z12
	VSUBPS  Z0, Z1, Z1
	VSUBPS  Z0, Z2, Z2
	VSUBPS  Z0, Z3, Z3
	VSUBPS  Z0, Z4, Z4
	VSUBPS  Z0, Z5, Z5
	VSUBPS  Z0, Z6, Z6
	VSUBPS  Z0, Z7, Z7
	VSUBPS  Z0, Z8, Z8
	VSUBPS  Z0, Z9, Z9
	VSUBPS  Z0, Z10, Z10
	VSUBPS  Z0, Z11, Z11
	VSUBPS  Z0, Z12, Z12
	VMOVUPS Z1, (CX)
	VMOVUPS Z2, 64(CX)
	VMOVUPS Z3, 128(CX)
	VMOVUPS Z4, 192(CX)
	VMOVUPS Z5, 256(CX)
	VMOVUPS Z6, 320(CX)
	VMOVUPS Z7, 384(CX)
	VMOVUPS Z8, 448(CX)
	VMOVUPS Z9, 512(CX)
	VMOVUPS Z10, 576(CX)
	VMOVUPS Z11, 640(CX)
	VMOVUPS Z12, 704(CX)
	ADDQ    $0x00000300, AX
	ADDQ    $0x00000300, CX
	SUBQ    $0x000000c0, DX
	JMP     float32MinusScalarBlockLoop

float32MinusScalarTailLoop:
	CMPQ    DX, $0x00000010
	JL      float32MinusScalarDone
	VMOVUPS (AX), Z1
	VSUBPS  Z0, Z1, Z1
	VMOVUPS Z1, (CX)
	ADDQ    $0x00000040, AX
	ADDQ    $0x00000040, CX
	SUBQ    $0x00000010, DX
	JMP     float32MinusScalarTailLoop

float32MinusScalarDone:
	RET

// func float32MinusByScalarAvx512Asm(x float32, y []float32, r []float32)
// Requires: AVX512F, SSE
TEXT ·float32MinusByScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVSS        x+0(FP), X0
	MOVQ         y_base+8(FP), AX
	MOVQ         r_base+32(FP), CX
	MOVQ         y_len+16(FP), DX
	VBROADCASTSS X0, Z0

float32MinusByScalarBlockLoop:
	CMPQ    DX, $0x000000c0
	JL      float32MinusByScalarTailLoop
	VSUBPS  (AX), Z0, Z1
	VSUBPS  64(AX), Z0, Z2
	VSUBPS  128(AX), Z0, Z3
	VSUBPS  192(AX), Z0, Z4
	VSUBPS  256(AX), Z0, Z5
	VSUBPS  320(AX), Z0, Z6
	VSUBPS  384(AX), Z0, Z7
	VSUBPS  448(AX), Z0, Z8
	VSUBPS  512(AX), Z0, Z9
	VSUBPS  576(AX), Z0, Z10
	VSUBPS  640(AX), Z0, Z11
	VSUBPS  704(AX), Z0, Z12
	VMOVUPS Z1, (CX)
	VMOVUPS Z2, 64(CX)
	VMOVUPS Z3, 128(CX)
	VMOVUPS Z4, 192(CX)
	VMOVUPS Z5, 256(CX)
	VMOVUPS Z6, 320(CX)
	VMOVUPS Z7, 384(CX)
	VMOVUPS Z8, 448(CX)
	VMOVUPS Z9, 512(CX)
	VMOVUPS Z10, 576(CX)
	VMOVUPS Z11, 640(CX)
	VMOVUPS Z12, 704(CX)
	ADDQ    $0x00000300, AX
	ADDQ    $0x00000300, CX
	SUBQ    $0x000000c0, DX
	JMP     float32MinusByScalarBlockLoop

float32MinusByScalarTailLoop:
	CMPQ    DX, $0x00000010
	JL      float32MinusByScalarDone
	VSUBPS  (AX), Z0, Z1
	VMOVUPS Z1, (CX)
	ADDQ    $0x00000040, AX
	ADDQ    $0x00000040, CX
	SUBQ    $0x00000010, DX
	JMP     float32MinusByScalarTailLoop

float32MinusByScalarDone:
	RET

// func float64MinusAvx512Asm(x []float64, y []float64, r []float64)
// Requires: AVX512F
TEXT ·float64MinusAvx512Asm(SB), NOSPLIT, $0-72
	MOVQ x_base+0(FP), AX
	MOVQ y_base+24(FP), CX
	MOVQ r_base+48(FP), DX
	MOVQ x_len+8(FP), BX

float64MinusBlockLoop:
	CMPQ    BX, $0x00000060
	JL      float64MinusTailLoop
	VMOVUPD (AX), Z0
	VMOVUPD 64(AX), Z1
	VMOVUPD 128(AX), Z2
	VMOVUPD 192(AX), Z3
	VMOVUPD 256(AX), Z4
	VMOVUPD 320(AX), Z5
	VMOVUPD 384(AX), Z6
	VMOVUPD 448(AX), Z7
	VMOVUPD 512(AX), Z8
	VMOVUPD 576(AX), Z9
	VMOVUPD 640(AX), Z10
	VMOVUPD 704(AX), Z11
	VSUBPD  (CX), Z0, Z0
	VSUBPD  64(CX), Z1, Z1
	VSUBPD  128(CX), Z2, Z2
	VSUBPD  192(CX), Z3, Z3
	VSUBPD  256(CX), Z4, Z4
	VSUBPD  320(CX), Z5, Z5
	VSUBPD  384(CX), Z6, Z6
	VSUBPD  448(CX), Z7, Z7
	VSUBPD  512(CX), Z8, Z8
	VSUBPD  576(CX), Z9, Z9
	VSUBPD  640(CX), Z10, Z10
	VSUBPD  704(CX), Z11, Z11
	VMOVUPD Z0, (DX)
	VMOVUPD Z1, 64(DX)
	VMOVUPD Z2, 128(DX)
	VMOVUPD Z3, 192(DX)
	VMOVUPD Z4, 256(DX)
	VMOVUPD Z5, 320(DX)
	VMOVUPD Z6, 384(DX)
	VMOVUPD Z7, 448(DX)
	VMOVUPD Z8, 512(DX)
	VMOVUPD Z9, 576(DX)
	VMOVUPD Z10, 640(DX)
	VMOVUPD Z11, 704(DX)
	ADDQ    $0x00000300, AX
	ADDQ    $0x00000300, CX
	ADDQ    $0x00000300, DX
	SUBQ    $0x00000060, BX
	JMP     float64MinusBlockLoop

float64MinusTailLoop:
	CMPQ    BX, $0x00000008
	JL      float64MinusDone
	VMOVUPD (AX), Z0
	VSUBPD  (CX), Z0, Z0
	VMOVUPD Z0, (DX)
	ADDQ    $0x00000040, AX
	ADDQ    $0x00000040, CX
	ADDQ    $0x00000040, DX
	SUBQ    $0x00000008, BX
	JMP     float64MinusTailLoop

float64MinusDone:
	RET

// func float64MinusScalarAvx512Asm(x float64, y []float64, r []float64)
// Requires: AVX512F, SSE2
TEXT ·float64MinusScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVSD        x+0(FP), X0
	MOVQ         y_base+8(FP), AX
	MOVQ         r_base+32(FP), CX
	MOVQ         y_len+16(FP), DX
	VBROADCASTSD X0, Z0

float64MinusScalarBlockLoop:
	CMPQ    DX, $0x00000060
	JL      float64MinusScalarTailLoop
	VMOVUPD (AX), Z1
	VMOVUPD 64(AX), Z2
	VMOVUPD 128(AX), Z3
	VMOVUPD 192(AX), Z4
	VMOVUPD 256(AX), Z5
	VMOVUPD 320(AX), Z6
	VMOVUPD 384(AX), Z7
	VMOVUPD 448(AX), Z8
	VMOVUPD 512(AX), Z9
	VMOVUPD 576(AX), Z10
	VMOVUPD 640(AX), Z11
	VMOVUPD 704(AX), Z12
	VSUBPD  Z0, Z1, Z1
	VSUBPD  Z0, Z2, Z2
	VSUBPD  Z0, Z3, Z3
	VSUBPD  Z0, Z4, Z4
	VSUBPD  Z0, Z5, Z5
	VSUBPD  Z0, Z6, Z6
	VSUBPD  Z0, Z7, Z7
	VSUBPD  Z0, Z8, Z8
	VSUBPD  Z0, Z9, Z9
	VSUBPD  Z0, Z10, Z10
	VSUBPD  Z0, Z11, Z11
	VSUBPD  Z0, Z12, Z12
	VMOVUPD Z1, (CX)
	VMOVUPD Z2, 64(CX)
	VMOVUPD Z3, 128(CX)
	VMOVUPD Z4, 192(CX)
	VMOVUPD Z5, 256(CX)
	VMOVUPD Z6, 320(CX)
	VMOVUPD Z7, 384(CX)
	VMOVUPD Z8, 448(CX)
	VMOVUPD Z9, 512(CX)
	VMOVUPD Z10, 576(CX)
	VMOVUPD Z11, 640(CX)
	VMOVUPD Z12, 704(CX)
	ADDQ    $0x00000300, AX
	ADDQ    $0x00000300, CX
	SUBQ    $0x00000060, DX
	JMP     float64MinusScalarBlockLoop

float64MinusScalarTailLoop:
	CMPQ    DX, $0x00000008
	JL      float64MinusScalarDone
	VMOVUPD (AX), Z1
	VSUBPD  Z0, Z1, Z1
	VMOVUPD Z1, (CX)
	ADDQ    $0x00000040, AX
	ADDQ    $0x00000040, CX
	SUBQ    $0x00000008, DX
	JMP     float64MinusScalarTailLoop

float64MinusScalarDone:
	RET

// func float64MinusByScalarAvx512Asm(x float64, y []float64, r []float64)
// Requires: AVX512F, SSE2
TEXT ·float64MinusByScalarAvx512Asm(SB), NOSPLIT, $0-56
	MOVSD        x+0(FP), X0
	MOVQ         y_base+8(FP), AX
	MOVQ         r_base+32(FP), CX
	MOVQ         y_len+16(FP), DX
	VBROADCASTSD X0, Z0

float64MinusByScalarBlockLoop:
	CMPQ    DX, $0x00000060
	JL      float64MinusByScalarTailLoop
	VSUBPD  (AX), Z0, Z1
	VSUBPD  64(AX), Z0, Z2
	VSUBPD  128(AX), Z0, Z3
	VSUBPD  192(AX), Z0, Z4
	VSUBPD  256(AX), Z0, Z5
	VSUBPD  320(AX), Z0, Z6
	VSUBPD  384(AX), Z0, Z7
	VSUBPD  448(AX), Z0, Z8
	VSUBPD  512(AX), Z0, Z9
	VSUBPD  576(AX), Z0, Z10
	VSUBPD  640(AX), Z0, Z11
	VSUBPD  704(AX), Z0, Z12
	VMOVUPD Z1, (CX)
	VMOVUPD Z2, 64(CX)
	VMOVUPD Z3, 128(CX)
	VMOVUPD Z4, 192(CX)
	VMOVUPD Z5, 256(CX)
	VMOVUPD Z6, 320(CX)
	VMOVUPD Z7, 384(CX)
	VMOVUPD Z8, 448(CX)
	VMOVUPD Z9, 512(CX)
	VMOVUPD Z10, 576(CX)
	VMOVUPD Z11, 640(CX)
	VMOVUPD Z12, 704(CX)
	ADDQ    $0x00000300, AX
	ADDQ    $0x00000300, CX
	SUBQ    $0x00000060, DX
	JMP     float64MinusByScalarBlockLoop

float64MinusByScalarTailLoop:
	CMPQ    DX, $0x00000008
	JL      float64MinusByScalarDone
	VSUBPD  (AX), Z0, Z1
	VMOVUPD Z1, (CX)
	ADDQ    $0x00000040, AX
	ADDQ    $0x00000040, CX
	SUBQ    $0x00000008, DX
	JMP     float64MinusByScalarTailLoop

float64MinusByScalarDone:
	RET
