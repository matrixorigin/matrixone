// Code generated by command: go run avx512.go -out neg/avx512.s -stubs neg/avx512_stubs.go. DO NOT EDIT.

#include "textflag.h"

// func int8NegAvx512Asm(x []int8, r []int8)
// Requires: AVX512BW, AVX512F
TEXT ·int8NegAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ   x_base+0(FP), AX
	MOVQ   r_base+24(FP), CX
	MOVQ   x_len+8(FP), DX
	VPXORD Z0, Z0, Z0

int8NegBlockLoop:
	CMPQ      DX, $0x00000300
	JL        int8NegTailLoop
	VPSUBB    (AX), Z0, Z1
	VPSUBB    64(AX), Z0, Z2
	VPSUBB    128(AX), Z0, Z3
	VPSUBB    192(AX), Z0, Z4
	VPSUBB    256(AX), Z0, Z5
	VPSUBB    320(AX), Z0, Z6
	VPSUBB    384(AX), Z0, Z7
	VPSUBB    448(AX), Z0, Z8
	VPSUBB    512(AX), Z0, Z9
	VPSUBB    576(AX), Z0, Z10
	VPSUBB    640(AX), Z0, Z11
	VPSUBB    704(AX), Z0, Z12
	VMOVDQU32 Z1, (CX)
	VMOVDQU32 Z2, 64(CX)
	VMOVDQU32 Z3, 128(CX)
	VMOVDQU32 Z4, 192(CX)
	VMOVDQU32 Z5, 256(CX)
	VMOVDQU32 Z6, 320(CX)
	VMOVDQU32 Z7, 384(CX)
	VMOVDQU32 Z8, 448(CX)
	VMOVDQU32 Z9, 512(CX)
	VMOVDQU32 Z10, 576(CX)
	VMOVDQU32 Z11, 640(CX)
	VMOVDQU32 Z12, 704(CX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	SUBQ      $0x00000300, DX
	JMP       int8NegBlockLoop

int8NegTailLoop:
	CMPQ      DX, $0x00000040
	JL        int8NegDone
	VPSUBB    (AX), Z0, Z1
	VMOVDQU32 Z1, (CX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000040, DX
	JMP       int8NegTailLoop

int8NegDone:
	RET

// func int16NegAvx512Asm(x []int16, r []int16)
// Requires: AVX512BW, AVX512F
TEXT ·int16NegAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ   x_base+0(FP), AX
	MOVQ   r_base+24(FP), CX
	MOVQ   x_len+8(FP), DX
	VPXORD Z0, Z0, Z0

int16NegBlockLoop:
	CMPQ      DX, $0x00000180
	JL        int16NegTailLoop
	VPSUBW    (AX), Z0, Z1
	VPSUBW    64(AX), Z0, Z2
	VPSUBW    128(AX), Z0, Z3
	VPSUBW    192(AX), Z0, Z4
	VPSUBW    256(AX), Z0, Z5
	VPSUBW    320(AX), Z0, Z6
	VPSUBW    384(AX), Z0, Z7
	VPSUBW    448(AX), Z0, Z8
	VPSUBW    512(AX), Z0, Z9
	VPSUBW    576(AX), Z0, Z10
	VPSUBW    640(AX), Z0, Z11
	VPSUBW    704(AX), Z0, Z12
	VMOVDQU32 Z1, (CX)
	VMOVDQU32 Z2, 64(CX)
	VMOVDQU32 Z3, 128(CX)
	VMOVDQU32 Z4, 192(CX)
	VMOVDQU32 Z5, 256(CX)
	VMOVDQU32 Z6, 320(CX)
	VMOVDQU32 Z7, 384(CX)
	VMOVDQU32 Z8, 448(CX)
	VMOVDQU32 Z9, 512(CX)
	VMOVDQU32 Z10, 576(CX)
	VMOVDQU32 Z11, 640(CX)
	VMOVDQU32 Z12, 704(CX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	SUBQ      $0x00000180, DX
	JMP       int16NegBlockLoop

int16NegTailLoop:
	CMPQ      DX, $0x00000020
	JL        int16NegDone
	VPSUBW    (AX), Z0, Z1
	VMOVDQU32 Z1, (CX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000020, DX
	JMP       int16NegTailLoop

int16NegDone:
	RET

// func int32NegAvx512Asm(x []int32, r []int32)
// Requires: AVX512F
TEXT ·int32NegAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ   x_base+0(FP), AX
	MOVQ   r_base+24(FP), CX
	MOVQ   x_len+8(FP), DX
	VPXORD Z0, Z0, Z0

int32NegBlockLoop:
	CMPQ      DX, $0x000000c0
	JL        int32NegTailLoop
	VPSUBD    (AX), Z0, Z1
	VPSUBD    64(AX), Z0, Z2
	VPSUBD    128(AX), Z0, Z3
	VPSUBD    192(AX), Z0, Z4
	VPSUBD    256(AX), Z0, Z5
	VPSUBD    320(AX), Z0, Z6
	VPSUBD    384(AX), Z0, Z7
	VPSUBD    448(AX), Z0, Z8
	VPSUBD    512(AX), Z0, Z9
	VPSUBD    576(AX), Z0, Z10
	VPSUBD    640(AX), Z0, Z11
	VPSUBD    704(AX), Z0, Z12
	VMOVDQU32 Z1, (CX)
	VMOVDQU32 Z2, 64(CX)
	VMOVDQU32 Z3, 128(CX)
	VMOVDQU32 Z4, 192(CX)
	VMOVDQU32 Z5, 256(CX)
	VMOVDQU32 Z6, 320(CX)
	VMOVDQU32 Z7, 384(CX)
	VMOVDQU32 Z8, 448(CX)
	VMOVDQU32 Z9, 512(CX)
	VMOVDQU32 Z10, 576(CX)
	VMOVDQU32 Z11, 640(CX)
	VMOVDQU32 Z12, 704(CX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	SUBQ      $0x000000c0, DX
	JMP       int32NegBlockLoop

int32NegTailLoop:
	CMPQ      DX, $0x00000010
	JL        int32NegDone
	VPSUBD    (AX), Z0, Z1
	VMOVDQU32 Z1, (CX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000010, DX
	JMP       int32NegTailLoop

int32NegDone:
	RET

// func int64NegAvx512Asm(x []int64, r []int64)
// Requires: AVX512F
TEXT ·int64NegAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ   x_base+0(FP), AX
	MOVQ   r_base+24(FP), CX
	MOVQ   x_len+8(FP), DX
	VPXORD Z0, Z0, Z0

int64NegBlockLoop:
	CMPQ      DX, $0x00000060
	JL        int64NegTailLoop
	VPSUBQ    (AX), Z0, Z1
	VPSUBQ    64(AX), Z0, Z2
	VPSUBQ    128(AX), Z0, Z3
	VPSUBQ    192(AX), Z0, Z4
	VPSUBQ    256(AX), Z0, Z5
	VPSUBQ    320(AX), Z0, Z6
	VPSUBQ    384(AX), Z0, Z7
	VPSUBQ    448(AX), Z0, Z8
	VPSUBQ    512(AX), Z0, Z9
	VPSUBQ    576(AX), Z0, Z10
	VPSUBQ    640(AX), Z0, Z11
	VPSUBQ    704(AX), Z0, Z12
	VMOVDQU32 Z1, (CX)
	VMOVDQU32 Z2, 64(CX)
	VMOVDQU32 Z3, 128(CX)
	VMOVDQU32 Z4, 192(CX)
	VMOVDQU32 Z5, 256(CX)
	VMOVDQU32 Z6, 320(CX)
	VMOVDQU32 Z7, 384(CX)
	VMOVDQU32 Z8, 448(CX)
	VMOVDQU32 Z9, 512(CX)
	VMOVDQU32 Z10, 576(CX)
	VMOVDQU32 Z11, 640(CX)
	VMOVDQU32 Z12, 704(CX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	SUBQ      $0x00000060, DX
	JMP       int64NegBlockLoop

int64NegTailLoop:
	CMPQ      DX, $0x00000008
	JL        int64NegDone
	VPSUBQ    (AX), Z0, Z1
	VMOVDQU32 Z1, (CX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000008, DX
	JMP       int64NegTailLoop

int64NegDone:
	RET

// func float32NegAvx512Asm(x []float32, r []float32)
// Requires: AVX512F, SSE2
TEXT ·float32NegAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVL         $0x80000000, BX
	MOVD         BX, X0
	VPBROADCASTD X0, Z0

float32NegBlockLoop:
	CMPQ      DX, $0x000000c0
	JL        float32NegTailLoop
	VPXORD    (AX), Z0, Z1
	VPXORD    64(AX), Z0, Z2
	VPXORD    128(AX), Z0, Z3
	VPXORD    192(AX), Z0, Z4
	VPXORD    256(AX), Z0, Z5
	VPXORD    320(AX), Z0, Z6
	VPXORD    384(AX), Z0, Z7
	VPXORD    448(AX), Z0, Z8
	VPXORD    512(AX), Z0, Z9
	VPXORD    576(AX), Z0, Z10
	VPXORD    640(AX), Z0, Z11
	VPXORD    704(AX), Z0, Z12
	VMOVDQU32 Z1, (CX)
	VMOVDQU32 Z2, 64(CX)
	VMOVDQU32 Z3, 128(CX)
	VMOVDQU32 Z4, 192(CX)
	VMOVDQU32 Z5, 256(CX)
	VMOVDQU32 Z6, 320(CX)
	VMOVDQU32 Z7, 384(CX)
	VMOVDQU32 Z8, 448(CX)
	VMOVDQU32 Z9, 512(CX)
	VMOVDQU32 Z10, 576(CX)
	VMOVDQU32 Z11, 640(CX)
	VMOVDQU32 Z12, 704(CX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	SUBQ      $0x000000c0, DX
	JMP       float32NegBlockLoop

float32NegTailLoop:
	CMPQ      DX, $0x00000010
	JL        float32NegDone
	VPXORD    (AX), Z0, Z1
	VMOVDQU32 Z1, (CX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000010, DX
	JMP       float32NegTailLoop

float32NegDone:
	RET

// func float64NegAvx512Asm(x []float64, r []float64)
// Requires: AVX512F, SSE2
TEXT ·float64NegAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x8000000000000000, BX
	MOVQ         BX, X0
	VPBROADCASTQ X0, Z0

float64NegBlockLoop:
	CMPQ      DX, $0x00000060
	JL        float64NegTailLoop
	VPXORD    (AX), Z0, Z1
	VPXORD    64(AX), Z0, Z2
	VPXORD    128(AX), Z0, Z3
	VPXORD    192(AX), Z0, Z4
	VPXORD    256(AX), Z0, Z5
	VPXORD    320(AX), Z0, Z6
	VPXORD    384(AX), Z0, Z7
	VPXORD    448(AX), Z0, Z8
	VPXORD    512(AX), Z0, Z9
	VPXORD    576(AX), Z0, Z10
	VPXORD    640(AX), Z0, Z11
	VPXORD    704(AX), Z0, Z12
	VMOVDQU32 Z1, (CX)
	VMOVDQU32 Z2, 64(CX)
	VMOVDQU32 Z3, 128(CX)
	VMOVDQU32 Z4, 192(CX)
	VMOVDQU32 Z5, 256(CX)
	VMOVDQU32 Z6, 320(CX)
	VMOVDQU32 Z7, 384(CX)
	VMOVDQU32 Z8, 448(CX)
	VMOVDQU32 Z9, 512(CX)
	VMOVDQU32 Z10, 576(CX)
	VMOVDQU32 Z11, 640(CX)
	VMOVDQU32 Z12, 704(CX)
	ADDQ      $0x00000300, AX
	ADDQ      $0x00000300, CX
	SUBQ      $0x00000060, DX
	JMP       float64NegBlockLoop

float64NegTailLoop:
	CMPQ      DX, $0x00000008
	JL        float64NegDone
	VPXORD    (AX), Z0, Z1
	VMOVDQU32 Z1, (CX)
	ADDQ      $0x00000040, AX
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000008, DX
	JMP       float64NegTailLoop

float64NegDone:
	RET
