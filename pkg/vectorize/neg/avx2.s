// Code generated by command: go run avx2.go -out avx2.s -stubs avx2_stubs.go. DO NOT EDIT.
// +build amd64

#include "textflag.h"

// func int8NegAvx2Asm(x []int8, r []int8)
// Requires: AVX, AVX2
TEXT ·int8NegAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), AX
	MOVQ  r_base+24(FP), CX
	MOVQ  x_len+8(FP), DX
	VPXOR Y0, Y0, Y0

int8NegBlockLoop:
	CMPQ    DX, $0x000001e0
	JL      int8NegTailLoop
	VPSUBB  (AX), Y0, Y1
	VPSUBB  32(AX), Y0, Y2
	VPSUBB  64(AX), Y0, Y3
	VPSUBB  96(AX), Y0, Y4
	VPSUBB  128(AX), Y0, Y5
	VPSUBB  160(AX), Y0, Y6
	VPSUBB  192(AX), Y0, Y7
	VPSUBB  224(AX), Y0, Y8
	VPSUBB  256(AX), Y0, Y9
	VPSUBB  288(AX), Y0, Y10
	VPSUBB  320(AX), Y0, Y11
	VPSUBB  352(AX), Y0, Y12
	VPSUBB  384(AX), Y0, Y13
	VPSUBB  416(AX), Y0, Y14
	VPSUBB  448(AX), Y0, Y15
	VMOVDQU Y1, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y3, 64(CX)
	VMOVDQU Y4, 96(CX)
	VMOVDQU Y5, 128(CX)
	VMOVDQU Y6, 160(CX)
	VMOVDQU Y7, 192(CX)
	VMOVDQU Y8, 224(CX)
	VMOVDQU Y9, 256(CX)
	VMOVDQU Y10, 288(CX)
	VMOVDQU Y11, 320(CX)
	VMOVDQU Y12, 352(CX)
	VMOVDQU Y13, 384(CX)
	VMOVDQU Y14, 416(CX)
	VMOVDQU Y15, 448(CX)
	ADDQ    $0x000001e0, AX
	ADDQ    $0x000001e0, CX
	SUBQ    $0x000001e0, DX
	JMP     int8NegBlockLoop

int8NegTailLoop:
	CMPQ    DX, $0x00000020
	JL      int8NegDone
	VPSUBB  (AX), Y0, Y1
	VMOVDQU Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000020, DX
	JMP     int8NegTailLoop

int8NegDone:
	CMPQ    DX, $0x00000010
	JL      int8NegDone1
	VPSUBB  (AX), X0, X1
	VMOVDQU X1, (CX)

int8NegDone1:
	RET

// func int16NegAvx2Asm(x []int16, r []int16)
// Requires: AVX, AVX2
TEXT ·int16NegAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), AX
	MOVQ  r_base+24(FP), CX
	MOVQ  x_len+8(FP), DX
	VPXOR Y0, Y0, Y0

int16NegBlockLoop:
	CMPQ    DX, $0x000000f0
	JL      int16NegTailLoop
	VPSUBW  (AX), Y0, Y1
	VPSUBW  32(AX), Y0, Y2
	VPSUBW  64(AX), Y0, Y3
	VPSUBW  96(AX), Y0, Y4
	VPSUBW  128(AX), Y0, Y5
	VPSUBW  160(AX), Y0, Y6
	VPSUBW  192(AX), Y0, Y7
	VPSUBW  224(AX), Y0, Y8
	VPSUBW  256(AX), Y0, Y9
	VPSUBW  288(AX), Y0, Y10
	VPSUBW  320(AX), Y0, Y11
	VPSUBW  352(AX), Y0, Y12
	VPSUBW  384(AX), Y0, Y13
	VPSUBW  416(AX), Y0, Y14
	VPSUBW  448(AX), Y0, Y15
	VMOVDQU Y1, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y3, 64(CX)
	VMOVDQU Y4, 96(CX)
	VMOVDQU Y5, 128(CX)
	VMOVDQU Y6, 160(CX)
	VMOVDQU Y7, 192(CX)
	VMOVDQU Y8, 224(CX)
	VMOVDQU Y9, 256(CX)
	VMOVDQU Y10, 288(CX)
	VMOVDQU Y11, 320(CX)
	VMOVDQU Y12, 352(CX)
	VMOVDQU Y13, 384(CX)
	VMOVDQU Y14, 416(CX)
	VMOVDQU Y15, 448(CX)
	ADDQ    $0x000001e0, AX
	ADDQ    $0x000001e0, CX
	SUBQ    $0x000000f0, DX
	JMP     int16NegBlockLoop

int16NegTailLoop:
	CMPQ    DX, $0x00000010
	JL      int16NegDone
	VPSUBW  (AX), Y0, Y1
	VMOVDQU Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000010, DX
	JMP     int16NegTailLoop

int16NegDone:
	CMPQ    DX, $0x00000008
	JL      int16NegDone1
	VPSUBW  (AX), X0, X1
	VMOVDQU X1, (CX)

int16NegDone1:
	RET

// func int32NegAvx2Asm(x []int32, r []int32)
// Requires: AVX, AVX2
TEXT ·int32NegAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), AX
	MOVQ  r_base+24(FP), CX
	MOVQ  x_len+8(FP), DX
	VPXOR Y0, Y0, Y0

int32NegBlockLoop:
	CMPQ    DX, $0x00000078
	JL      int32NegTailLoop
	VPSUBD  (AX), Y0, Y1
	VPSUBD  32(AX), Y0, Y2
	VPSUBD  64(AX), Y0, Y3
	VPSUBD  96(AX), Y0, Y4
	VPSUBD  128(AX), Y0, Y5
	VPSUBD  160(AX), Y0, Y6
	VPSUBD  192(AX), Y0, Y7
	VPSUBD  224(AX), Y0, Y8
	VPSUBD  256(AX), Y0, Y9
	VPSUBD  288(AX), Y0, Y10
	VPSUBD  320(AX), Y0, Y11
	VPSUBD  352(AX), Y0, Y12
	VPSUBD  384(AX), Y0, Y13
	VPSUBD  416(AX), Y0, Y14
	VPSUBD  448(AX), Y0, Y15
	VMOVDQU Y1, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y3, 64(CX)
	VMOVDQU Y4, 96(CX)
	VMOVDQU Y5, 128(CX)
	VMOVDQU Y6, 160(CX)
	VMOVDQU Y7, 192(CX)
	VMOVDQU Y8, 224(CX)
	VMOVDQU Y9, 256(CX)
	VMOVDQU Y10, 288(CX)
	VMOVDQU Y11, 320(CX)
	VMOVDQU Y12, 352(CX)
	VMOVDQU Y13, 384(CX)
	VMOVDQU Y14, 416(CX)
	VMOVDQU Y15, 448(CX)
	ADDQ    $0x000001e0, AX
	ADDQ    $0x000001e0, CX
	SUBQ    $0x00000078, DX
	JMP     int32NegBlockLoop

int32NegTailLoop:
	CMPQ    DX, $0x00000008
	JL      int32NegDone
	VPSUBD  (AX), Y0, Y1
	VMOVDQU Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000008, DX
	JMP     int32NegTailLoop

int32NegDone:
	CMPQ    DX, $0x00000004
	JL      int32NegDone1
	VPSUBD  (AX), X0, X1
	VMOVDQU X1, (CX)

int32NegDone1:
	RET

// func int64NegAvx2Asm(x []int64, r []int64)
// Requires: AVX, AVX2
TEXT ·int64NegAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), AX
	MOVQ  r_base+24(FP), CX
	MOVQ  x_len+8(FP), DX
	VPXOR Y0, Y0, Y0

int64NegBlockLoop:
	CMPQ    DX, $0x0000003c
	JL      int64NegTailLoop
	VPSUBQ  (AX), Y0, Y1
	VPSUBQ  32(AX), Y0, Y2
	VPSUBQ  64(AX), Y0, Y3
	VPSUBQ  96(AX), Y0, Y4
	VPSUBQ  128(AX), Y0, Y5
	VPSUBQ  160(AX), Y0, Y6
	VPSUBQ  192(AX), Y0, Y7
	VPSUBQ  224(AX), Y0, Y8
	VPSUBQ  256(AX), Y0, Y9
	VPSUBQ  288(AX), Y0, Y10
	VPSUBQ  320(AX), Y0, Y11
	VPSUBQ  352(AX), Y0, Y12
	VPSUBQ  384(AX), Y0, Y13
	VPSUBQ  416(AX), Y0, Y14
	VPSUBQ  448(AX), Y0, Y15
	VMOVDQU Y1, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y3, 64(CX)
	VMOVDQU Y4, 96(CX)
	VMOVDQU Y5, 128(CX)
	VMOVDQU Y6, 160(CX)
	VMOVDQU Y7, 192(CX)
	VMOVDQU Y8, 224(CX)
	VMOVDQU Y9, 256(CX)
	VMOVDQU Y10, 288(CX)
	VMOVDQU Y11, 320(CX)
	VMOVDQU Y12, 352(CX)
	VMOVDQU Y13, 384(CX)
	VMOVDQU Y14, 416(CX)
	VMOVDQU Y15, 448(CX)
	ADDQ    $0x000001e0, AX
	ADDQ    $0x000001e0, CX
	SUBQ    $0x0000003c, DX
	JMP     int64NegBlockLoop

int64NegTailLoop:
	CMPQ    DX, $0x00000004
	JL      int64NegDone
	VPSUBQ  (AX), Y0, Y1
	VMOVDQU Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000004, DX
	JMP     int64NegTailLoop

int64NegDone:
	CMPQ    DX, $0x00000002
	JL      int64NegDone1
	VPSUBQ  (AX), X0, X1
	VMOVDQU X1, (CX)

int64NegDone1:
	RET

// func float32NegAvx2Asm(x []float32, r []float32)
// Requires: AVX, AVX2, SSE2
TEXT ·float32NegAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVL         $0x80000000, BX
	MOVD         BX, X0
	VPBROADCASTD X0, Y0

float32NegBlockLoop:
	CMPQ    DX, $0x00000078
	JL      float32NegTailLoop
	VPXOR   (AX), Y0, Y1
	VPXOR   32(AX), Y0, Y2
	VPXOR   64(AX), Y0, Y3
	VPXOR   96(AX), Y0, Y4
	VPXOR   128(AX), Y0, Y5
	VPXOR   160(AX), Y0, Y6
	VPXOR   192(AX), Y0, Y7
	VPXOR   224(AX), Y0, Y8
	VPXOR   256(AX), Y0, Y9
	VPXOR   288(AX), Y0, Y10
	VPXOR   320(AX), Y0, Y11
	VPXOR   352(AX), Y0, Y12
	VPXOR   384(AX), Y0, Y13
	VPXOR   416(AX), Y0, Y14
	VPXOR   448(AX), Y0, Y15
	VMOVDQU Y1, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y3, 64(CX)
	VMOVDQU Y4, 96(CX)
	VMOVDQU Y5, 128(CX)
	VMOVDQU Y6, 160(CX)
	VMOVDQU Y7, 192(CX)
	VMOVDQU Y8, 224(CX)
	VMOVDQU Y9, 256(CX)
	VMOVDQU Y10, 288(CX)
	VMOVDQU Y11, 320(CX)
	VMOVDQU Y12, 352(CX)
	VMOVDQU Y13, 384(CX)
	VMOVDQU Y14, 416(CX)
	VMOVDQU Y15, 448(CX)
	ADDQ    $0x000001e0, AX
	ADDQ    $0x000001e0, CX
	SUBQ    $0x00000078, DX
	JMP     float32NegBlockLoop

float32NegTailLoop:
	CMPQ    DX, $0x00000008
	JL      float32NegDone
	VPXOR   (AX), Y0, Y1
	VMOVDQU Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000008, DX
	JMP     float32NegTailLoop

float32NegDone:
	CMPQ    DX, $0x00000004
	JL      float32NegDone1
	VPXOR   (AX), X0, X1
	VMOVDQU X1, (CX)

float32NegDone1:
	RET

// func float64NegAvx2Asm(x []float64, r []float64)
// Requires: AVX, AVX2, SSE2
TEXT ·float64NegAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x8000000000000000, BX
	MOVQ         BX, X0
	VPBROADCASTQ X0, Y0

float64NegBlockLoop:
	CMPQ    DX, $0x0000003c
	JL      float64NegTailLoop
	VPXOR   (AX), Y0, Y1
	VPXOR   32(AX), Y0, Y2
	VPXOR   64(AX), Y0, Y3
	VPXOR   96(AX), Y0, Y4
	VPXOR   128(AX), Y0, Y5
	VPXOR   160(AX), Y0, Y6
	VPXOR   192(AX), Y0, Y7
	VPXOR   224(AX), Y0, Y8
	VPXOR   256(AX), Y0, Y9
	VPXOR   288(AX), Y0, Y10
	VPXOR   320(AX), Y0, Y11
	VPXOR   352(AX), Y0, Y12
	VPXOR   384(AX), Y0, Y13
	VPXOR   416(AX), Y0, Y14
	VPXOR   448(AX), Y0, Y15
	VMOVDQU Y1, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y3, 64(CX)
	VMOVDQU Y4, 96(CX)
	VMOVDQU Y5, 128(CX)
	VMOVDQU Y6, 160(CX)
	VMOVDQU Y7, 192(CX)
	VMOVDQU Y8, 224(CX)
	VMOVDQU Y9, 256(CX)
	VMOVDQU Y10, 288(CX)
	VMOVDQU Y11, 320(CX)
	VMOVDQU Y12, 352(CX)
	VMOVDQU Y13, 384(CX)
	VMOVDQU Y14, 416(CX)
	VMOVDQU Y15, 448(CX)
	ADDQ    $0x000001e0, AX
	ADDQ    $0x000001e0, CX
	SUBQ    $0x0000003c, DX
	JMP     float64NegBlockLoop

float64NegTailLoop:
	CMPQ    DX, $0x00000004
	JL      float64NegDone
	VPXOR   (AX), Y0, Y1
	VMOVDQU Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000004, DX
	JMP     float64NegTailLoop

float64NegDone:
	CMPQ    DX, $0x00000002
	JL      float64NegDone1
	VPXOR   (AX), X0, X1
	VMOVDQU X1, (CX)

float64NegDone1:
	RET
