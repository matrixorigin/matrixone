// Code generated by command: go run avx2.go -out avx2.s -stubs avx2_stubs.go. DO NOT EDIT.

#include "textflag.h"

// func int8NegAvx2Asm(x []int8, r []int8)
// Requires: AVX, AVX2
TEXT ·int8NegAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), AX
	MOVQ  r_base+24(FP), CX
	MOVQ  x_len+8(FP), DX
	VPXOR Y0, Y0, Y0

int8NegBlockLoop:
	CMPQ    DX, $0x000000c0
	JL      int8NegTailLoop
	VMOVDQU (AX), Y1
	VMOVDQU 32(AX), Y2
	VMOVDQU 64(AX), Y3
	VMOVDQU 96(AX), Y4
	VMOVDQU 128(AX), Y5
	VMOVDQU 160(AX), Y6
	VPSUBB  Y0, Y1, Y1
	VPSUBB  Y0, Y2, Y2
	VPSUBB  Y0, Y3, Y3
	VPSUBB  Y0, Y4, Y4
	VPSUBB  Y0, Y5, Y5
	VPSUBB  Y0, Y6, Y6
	VMOVDQU Y1, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y3, 64(CX)
	VMOVDQU Y4, 96(CX)
	VMOVDQU Y5, 128(CX)
	VMOVDQU Y6, 160(CX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	SUBQ    $0x000000c0, DX
	JMP     int8NegBlockLoop

int8NegTailLoop:
	CMPQ    DX, $0x00000020
	JL      int8NegDone
	VMOVDQU (AX), Y1
	VPSUBB  Y0, Y1, Y1
	VMOVDQU Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000020, DX
	JMP     int8NegTailLoop

int8NegDone:
	RET

// func int16NegAvx2Asm(x []int16, r []int16)
// Requires: AVX, AVX2
TEXT ·int16NegAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), AX
	MOVQ  r_base+24(FP), CX
	MOVQ  x_len+8(FP), DX
	VPXOR Y0, Y0, Y0

int16NegBlockLoop:
	CMPQ    DX, $0x00000060
	JL      int16NegTailLoop
	VMOVDQU (AX), Y1
	VMOVDQU 32(AX), Y2
	VMOVDQU 64(AX), Y3
	VMOVDQU 96(AX), Y4
	VMOVDQU 128(AX), Y5
	VMOVDQU 160(AX), Y6
	VPSUBW  Y0, Y1, Y1
	VPSUBW  Y0, Y2, Y2
	VPSUBW  Y0, Y3, Y3
	VPSUBW  Y0, Y4, Y4
	VPSUBW  Y0, Y5, Y5
	VPSUBW  Y0, Y6, Y6
	VMOVDQU Y1, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y3, 64(CX)
	VMOVDQU Y4, 96(CX)
	VMOVDQU Y5, 128(CX)
	VMOVDQU Y6, 160(CX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	SUBQ    $0x00000060, DX
	JMP     int16NegBlockLoop

int16NegTailLoop:
	CMPQ    DX, $0x00000010
	JL      int16NegDone
	VMOVDQU (AX), Y1
	VPSUBW  Y0, Y1, Y1
	VMOVDQU Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000010, DX
	JMP     int16NegTailLoop

int16NegDone:
	RET

// func int32NegAvx2Asm(x []int32, r []int32)
// Requires: AVX, AVX2
TEXT ·int32NegAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), AX
	MOVQ  r_base+24(FP), CX
	MOVQ  x_len+8(FP), DX
	VPXOR Y0, Y0, Y0

int32NegBlockLoop:
	CMPQ    DX, $0x00000030
	JL      int32NegTailLoop
	VMOVDQU (AX), Y1
	VMOVDQU 32(AX), Y2
	VMOVDQU 64(AX), Y3
	VMOVDQU 96(AX), Y4
	VMOVDQU 128(AX), Y5
	VMOVDQU 160(AX), Y6
	VPSUBD  Y0, Y1, Y1
	VPSUBD  Y0, Y2, Y2
	VPSUBD  Y0, Y3, Y3
	VPSUBD  Y0, Y4, Y4
	VPSUBD  Y0, Y5, Y5
	VPSUBD  Y0, Y6, Y6
	VMOVDQU Y1, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y3, 64(CX)
	VMOVDQU Y4, 96(CX)
	VMOVDQU Y5, 128(CX)
	VMOVDQU Y6, 160(CX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	SUBQ    $0x00000030, DX
	JMP     int32NegBlockLoop

int32NegTailLoop:
	CMPQ    DX, $0x00000008
	JL      int32NegDone
	VMOVDQU (AX), Y1
	VPSUBD  Y0, Y1, Y1
	VMOVDQU Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000008, DX
	JMP     int32NegTailLoop

int32NegDone:
	RET

// func int64NegAvx2Asm(x []int64, r []int64)
// Requires: AVX, AVX2
TEXT ·int64NegAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), AX
	MOVQ  r_base+24(FP), CX
	MOVQ  x_len+8(FP), DX
	VPXOR Y0, Y0, Y0

int64NegBlockLoop:
	CMPQ    DX, $0x00000018
	JL      int64NegTailLoop
	VMOVDQU (AX), Y1
	VMOVDQU 32(AX), Y2
	VMOVDQU 64(AX), Y3
	VMOVDQU 96(AX), Y4
	VMOVDQU 128(AX), Y5
	VMOVDQU 160(AX), Y6
	VPSUBQ  Y0, Y1, Y1
	VPSUBQ  Y0, Y2, Y2
	VPSUBQ  Y0, Y3, Y3
	VPSUBQ  Y0, Y4, Y4
	VPSUBQ  Y0, Y5, Y5
	VPSUBQ  Y0, Y6, Y6
	VMOVDQU Y1, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y3, 64(CX)
	VMOVDQU Y4, 96(CX)
	VMOVDQU Y5, 128(CX)
	VMOVDQU Y6, 160(CX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	SUBQ    $0x00000018, DX
	JMP     int64NegBlockLoop

int64NegTailLoop:
	CMPQ    DX, $0x00000004
	JL      int64NegDone
	VMOVDQU (AX), Y1
	VPSUBQ  Y0, Y1, Y1
	VMOVDQU Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000004, DX
	JMP     int64NegTailLoop

int64NegDone:
	RET

// func float32NegAvx2Asm(x []float32, r []float32)
// Requires: AVX, AVX2, SSE2
TEXT ·float32NegAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVL         $0x80000000, BX
	MOVD         BX, X0
	VPBROADCASTD X0, Y0

float32NegBlockLoop:
	CMPQ    DX, $0x00000030
	JL      float32NegTailLoop
	VMOVDQU (AX), Y1
	VMOVDQU 32(AX), Y2
	VMOVDQU 64(AX), Y3
	VMOVDQU 96(AX), Y4
	VMOVDQU 128(AX), Y5
	VMOVDQU 160(AX), Y6
	VPXOR   Y0, Y1, Y1
	VPXOR   Y0, Y2, Y2
	VPXOR   Y0, Y3, Y3
	VPXOR   Y0, Y4, Y4
	VPXOR   Y0, Y5, Y5
	VPXOR   Y0, Y6, Y6
	VMOVDQU Y1, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y3, 64(CX)
	VMOVDQU Y4, 96(CX)
	VMOVDQU Y5, 128(CX)
	VMOVDQU Y6, 160(CX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	SUBQ    $0x00000030, DX
	JMP     float32NegBlockLoop

float32NegTailLoop:
	CMPQ    DX, $0x00000008
	JL      float32NegDone
	VMOVDQU (AX), Y1
	VPXOR   Y0, Y1, Y1
	VMOVDQU Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000008, DX
	JMP     float32NegTailLoop

float32NegDone:
	RET

// func float64NegAvx2Asm(x []float64, r []float64)
// Requires: AVX, AVX2, SSE2
TEXT ·float64NegAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x8000000000000000, BX
	MOVQ         BX, X0
	VPBROADCASTQ X0, Y0

float64NegBlockLoop:
	CMPQ    DX, $0x00000018
	JL      float64NegTailLoop
	VMOVDQU (AX), Y1
	VMOVDQU 32(AX), Y2
	VMOVDQU 64(AX), Y3
	VMOVDQU 96(AX), Y4
	VMOVDQU 128(AX), Y5
	VMOVDQU 160(AX), Y6
	VPXOR   Y0, Y1, Y1
	VPXOR   Y0, Y2, Y2
	VPXOR   Y0, Y3, Y3
	VPXOR   Y0, Y4, Y4
	VPXOR   Y0, Y5, Y5
	VPXOR   Y0, Y6, Y6
	VMOVDQU Y1, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y3, 64(CX)
	VMOVDQU Y4, 96(CX)
	VMOVDQU Y5, 128(CX)
	VMOVDQU Y6, 160(CX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	SUBQ    $0x00000018, DX
	JMP     float64NegBlockLoop

float64NegTailLoop:
	CMPQ    DX, $0x00000004
	JL      float64NegDone
	VMOVDQU (AX), Y1
	VPXOR   Y0, Y1, Y1
	VMOVDQU Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000004, DX
	JMP     float64NegTailLoop

float64NegDone:
	RET
