// Code generated by command: go run avx2.go -out neg/avx2.s -stubs neg/avx2_stubs.go. DO NOT EDIT.

#include "textflag.h"

// func int8NegAvx2Asm(x []int8, r []int8)
// Requires: AVX, AVX2
TEXT ·int8NegAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), AX
	MOVQ  r_base+24(FP), CX
	MOVQ  x_len+8(FP), DX
	VPXOR Y0, Y0, Y0

int8NegBlockLoop:
	CMPQ    DX, $0x000000c0
	JL      int8NegTailLoop
	VPSUBB  (AX), Y0, Y1
	VPSUBB  32(AX), Y0, Y2
	VPSUBB  64(AX), Y0, Y3
	VPSUBB  96(AX), Y0, Y4
	VPSUBB  128(AX), Y0, Y5
	VPSUBB  160(AX), Y0, Y6
	VMOVDQU Y1, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y3, 64(CX)
	VMOVDQU Y4, 96(CX)
	VMOVDQU Y5, 128(CX)
	VMOVDQU Y6, 160(CX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	SUBQ    $0x000000c0, DX
	JMP     int8NegBlockLoop

int8NegTailLoop:
	CMPQ    DX, $0x00000020
	JL      int8NegDone
	VPSUBB  (AX), Y0, Y1
	VMOVDQU Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000020, DX
	JMP     int8NegTailLoop

int8NegDone:
	RET

// func int16NegAvx2Asm(x []int16, r []int16)
// Requires: AVX, AVX2
TEXT ·int16NegAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), AX
	MOVQ  r_base+24(FP), CX
	MOVQ  x_len+8(FP), DX
	VPXOR Y0, Y0, Y0

int16NegBlockLoop:
	CMPQ    DX, $0x00000060
	JL      int16NegTailLoop
	VPSUBW  (AX), Y0, Y1
	VPSUBW  32(AX), Y0, Y2
	VPSUBW  64(AX), Y0, Y3
	VPSUBW  96(AX), Y0, Y4
	VPSUBW  128(AX), Y0, Y5
	VPSUBW  160(AX), Y0, Y6
	VMOVDQU Y1, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y3, 64(CX)
	VMOVDQU Y4, 96(CX)
	VMOVDQU Y5, 128(CX)
	VMOVDQU Y6, 160(CX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	SUBQ    $0x00000060, DX
	JMP     int16NegBlockLoop

int16NegTailLoop:
	CMPQ    DX, $0x00000010
	JL      int16NegDone
	VPSUBW  (AX), Y0, Y1
	VMOVDQU Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000010, DX
	JMP     int16NegTailLoop

int16NegDone:
	RET

// func int32NegAvx2Asm(x []int32, r []int32)
// Requires: AVX, AVX2
TEXT ·int32NegAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), AX
	MOVQ  r_base+24(FP), CX
	MOVQ  x_len+8(FP), DX
	VPXOR Y0, Y0, Y0

int32NegBlockLoop:
	CMPQ    DX, $0x00000030
	JL      int32NegTailLoop
	VPSUBD  (AX), Y0, Y1
	VPSUBD  32(AX), Y0, Y2
	VPSUBD  64(AX), Y0, Y3
	VPSUBD  96(AX), Y0, Y4
	VPSUBD  128(AX), Y0, Y5
	VPSUBD  160(AX), Y0, Y6
	VMOVDQU Y1, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y3, 64(CX)
	VMOVDQU Y4, 96(CX)
	VMOVDQU Y5, 128(CX)
	VMOVDQU Y6, 160(CX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	SUBQ    $0x00000030, DX
	JMP     int32NegBlockLoop

int32NegTailLoop:
	CMPQ    DX, $0x00000008
	JL      int32NegDone
	VPSUBD  (AX), Y0, Y1
	VMOVDQU Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000008, DX
	JMP     int32NegTailLoop

int32NegDone:
	RET

// func int64NegAvx2Asm(x []int64, r []int64)
// Requires: AVX, AVX2
TEXT ·int64NegAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), AX
	MOVQ  r_base+24(FP), CX
	MOVQ  x_len+8(FP), DX
	VPXOR Y0, Y0, Y0

int64NegBlockLoop:
	CMPQ    DX, $0x00000018
	JL      int64NegTailLoop
	VPSUBQ  (AX), Y0, Y1
	VPSUBQ  32(AX), Y0, Y2
	VPSUBQ  64(AX), Y0, Y3
	VPSUBQ  96(AX), Y0, Y4
	VPSUBQ  128(AX), Y0, Y5
	VPSUBQ  160(AX), Y0, Y6
	VMOVDQU Y1, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y3, 64(CX)
	VMOVDQU Y4, 96(CX)
	VMOVDQU Y5, 128(CX)
	VMOVDQU Y6, 160(CX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	SUBQ    $0x00000018, DX
	JMP     int64NegBlockLoop

int64NegTailLoop:
	CMPQ    DX, $0x00000004
	JL      int64NegDone
	VPSUBQ  (AX), Y0, Y1
	VMOVDQU Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000004, DX
	JMP     int64NegTailLoop

int64NegDone:
	RET

// func float32NegAvx2Asm(x []float32, r []float32)
// Requires: AVX, AVX2, SSE2
TEXT ·float32NegAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVL         $0x80000000, BX
	MOVD         BX, X0
	VPBROADCASTD X0, Y0

float32NegBlockLoop:
	CMPQ    DX, $0x00000030
	JL      float32NegTailLoop
	VPXOR   (AX), Y0, Y1
	VPXOR   32(AX), Y0, Y2
	VPXOR   64(AX), Y0, Y3
	VPXOR   96(AX), Y0, Y4
	VPXOR   128(AX), Y0, Y5
	VPXOR   160(AX), Y0, Y6
	VMOVDQU Y1, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y3, 64(CX)
	VMOVDQU Y4, 96(CX)
	VMOVDQU Y5, 128(CX)
	VMOVDQU Y6, 160(CX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	SUBQ    $0x00000030, DX
	JMP     float32NegBlockLoop

float32NegTailLoop:
	CMPQ    DX, $0x00000008
	JL      float32NegDone
	VPXOR   (AX), Y0, Y1
	VMOVDQU Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000008, DX
	JMP     float32NegTailLoop

float32NegDone:
	RET

// func float64NegAvx2Asm(x []float64, r []float64)
// Requires: AVX, AVX2, SSE2
TEXT ·float64NegAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x8000000000000000, BX
	MOVQ         BX, X0
	VPBROADCASTQ X0, Y0

float64NegBlockLoop:
	CMPQ    DX, $0x00000018
	JL      float64NegTailLoop
	VPXOR   (AX), Y0, Y1
	VPXOR   32(AX), Y0, Y2
	VPXOR   64(AX), Y0, Y3
	VPXOR   96(AX), Y0, Y4
	VPXOR   128(AX), Y0, Y5
	VPXOR   160(AX), Y0, Y6
	VMOVDQU Y1, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y3, 64(CX)
	VMOVDQU Y4, 96(CX)
	VMOVDQU Y5, 128(CX)
	VMOVDQU Y6, 160(CX)
	ADDQ    $0x000000c0, AX
	ADDQ    $0x000000c0, CX
	SUBQ    $0x00000018, DX
	JMP     float64NegBlockLoop

float64NegTailLoop:
	CMPQ    DX, $0x00000004
	JL      float64NegDone
	VPXOR   (AX), Y0, Y1
	VMOVDQU Y1, (CX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	SUBQ    $0x00000004, DX
	JMP     float64NegTailLoop

float64NegDone:
	RET
