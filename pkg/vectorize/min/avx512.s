// Code generated by command: go run avx512.go -out avx512.s -stubs avx512_stubs.go. DO NOT EDIT.

#include "textflag.h"

// func int8MinAvx512Asm(x []int8, r []int8)
// Requires: AVX, AVX2, AVX512BW, AVX512F, SSE2
TEXT ·int8MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x000000000000007f, BX
	MOVQ         BX, X0
	VPBROADCASTB X0, Z0
	VMOVDQU64    Z0, Z1
	VMOVDQU64    Z0, Z2
	VMOVDQU64    Z0, Z3
	VMOVDQU64    Z0, Z4
	VMOVDQU64    Z0, Z5
	VMOVDQU64    Z0, Z6
	VMOVDQU64    Z0, Z7
	VMOVDQU64    Z0, Z8
	VMOVDQU64    Z0, Z9
	VMOVDQU64    Z0, Z10
	VMOVDQU64    Z0, Z11
	VMOVDQU64    Z0, Z12
	VMOVDQU64    Z0, Z13
	VMOVDQU64    Z0, Z14
	VMOVDQU64    Z0, Z15
	VMOVDQU64    Z0, Z16
	VMOVDQU64    Z0, Z17
	VMOVDQU64    Z0, Z18
	VMOVDQU64    Z0, Z19
	VMOVDQU64    Z0, Z20
	VMOVDQU64    Z0, Z21
	VMOVDQU64    Z0, Z22
	VMOVDQU64    Z0, Z23
	VMOVDQU64    Z0, Z24
	VMOVDQU64    Z0, Z25
	VMOVDQU64    Z0, Z26
	VMOVDQU64    Z0, Z27
	VMOVDQU64    Z0, Z28
	VMOVDQU64    Z0, Z29
	VMOVDQU64    Z0, Z30
	VMOVDQU64    Z0, Z31

int8MinBlockLoop:
	CMPQ    DX, $0x00000800
	JL      int8MinTailLoop
	VPMINSB (AX), Z0, Z0
	VPMINSB 64(AX), Z1, Z1
	VPMINSB 128(AX), Z2, Z2
	VPMINSB 192(AX), Z3, Z3
	VPMINSB 256(AX), Z4, Z4
	VPMINSB 320(AX), Z5, Z5
	VPMINSB 384(AX), Z6, Z6
	VPMINSB 448(AX), Z7, Z7
	VPMINSB 512(AX), Z8, Z8
	VPMINSB 576(AX), Z9, Z9
	VPMINSB 640(AX), Z10, Z10
	VPMINSB 704(AX), Z11, Z11
	VPMINSB 768(AX), Z12, Z12
	VPMINSB 832(AX), Z13, Z13
	VPMINSB 896(AX), Z14, Z14
	VPMINSB 960(AX), Z15, Z15
	VPMINSB 1024(AX), Z16, Z16
	VPMINSB 1088(AX), Z17, Z17
	VPMINSB 1152(AX), Z18, Z18
	VPMINSB 1216(AX), Z19, Z19
	VPMINSB 1280(AX), Z20, Z20
	VPMINSB 1344(AX), Z21, Z21
	VPMINSB 1408(AX), Z22, Z22
	VPMINSB 1472(AX), Z23, Z23
	VPMINSB 1536(AX), Z24, Z24
	VPMINSB 1600(AX), Z25, Z25
	VPMINSB 1664(AX), Z26, Z26
	VPMINSB 1728(AX), Z27, Z27
	VPMINSB 1792(AX), Z28, Z28
	VPMINSB 1856(AX), Z29, Z29
	VPMINSB 1920(AX), Z30, Z30
	VPMINSB 1984(AX), Z31, Z31
	ADDQ    $0x00000800, AX
	SUBQ    $0x00000800, DX
	JMP     int8MinBlockLoop

int8MinTailLoop:
	CMPQ    DX, $0x00000040
	JL      int8MinDone
	VPMINSB (AX), Z0, Z0
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000040, DX
	JMP     int8MinTailLoop

int8MinDone:
	VPMINSB       Z0, Z1, Z0
	VPMINSB       Z0, Z2, Z0
	VPMINSB       Z0, Z3, Z0
	VPMINSB       Z0, Z4, Z0
	VPMINSB       Z0, Z5, Z0
	VPMINSB       Z0, Z6, Z0
	VPMINSB       Z0, Z7, Z0
	VPMINSB       Z0, Z8, Z0
	VPMINSB       Z0, Z9, Z0
	VPMINSB       Z0, Z10, Z0
	VPMINSB       Z0, Z11, Z0
	VPMINSB       Z0, Z12, Z0
	VPMINSB       Z0, Z13, Z0
	VPMINSB       Z0, Z14, Z0
	VPMINSB       Z0, Z15, Z0
	VPMINSB       Z0, Z16, Z0
	VPMINSB       Z0, Z17, Z0
	VPMINSB       Z0, Z18, Z0
	VPMINSB       Z0, Z19, Z0
	VPMINSB       Z0, Z20, Z0
	VPMINSB       Z0, Z21, Z0
	VPMINSB       Z0, Z22, Z0
	VPMINSB       Z0, Z23, Z0
	VPMINSB       Z0, Z24, Z0
	VPMINSB       Z0, Z25, Z0
	VPMINSB       Z0, Z26, Z0
	VPMINSB       Z0, Z27, Z0
	VPMINSB       Z0, Z28, Z0
	VPMINSB       Z0, Z29, Z0
	VPMINSB       Z0, Z30, Z0
	VPMINSB       Z0, Z31, Z0
	VEXTRACTI64X4 $0x01, Z0, Y1
	VPMINSB       Y1, Y0, Y0
	SUBQ          $0x00000020, DX
	JB            int8MinDone1
	VPMINSB       (AX), Y0, Y0
	ADDQ          $0x00000020, AX

int8MinDone1:
	VEXTRACTI128 $0x01, Y0, X1
	VPMINSB      X1, X0, X0
	CMPQ         DX, $0x00000010
	JL           int8MinDone2
	VPMINSB      (AX), X0, X0
	ADDQ         $0x00000020, AX

int8MinDone2:
	MOVOU X0, (CX)
	RET

// func int16MinAvx512Asm(x []int16, r []int16)
// Requires: AVX, AVX2, AVX512BW, AVX512F, SSE2
TEXT ·int16MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x0000000000007fff, BX
	MOVQ         BX, X0
	VPBROADCASTW X0, Z0
	VMOVDQU64    Z0, Z1
	VMOVDQU64    Z0, Z2
	VMOVDQU64    Z0, Z3
	VMOVDQU64    Z0, Z4
	VMOVDQU64    Z0, Z5
	VMOVDQU64    Z0, Z6
	VMOVDQU64    Z0, Z7
	VMOVDQU64    Z0, Z8
	VMOVDQU64    Z0, Z9
	VMOVDQU64    Z0, Z10
	VMOVDQU64    Z0, Z11
	VMOVDQU64    Z0, Z12
	VMOVDQU64    Z0, Z13
	VMOVDQU64    Z0, Z14
	VMOVDQU64    Z0, Z15
	VMOVDQU64    Z0, Z16
	VMOVDQU64    Z0, Z17
	VMOVDQU64    Z0, Z18
	VMOVDQU64    Z0, Z19
	VMOVDQU64    Z0, Z20
	VMOVDQU64    Z0, Z21
	VMOVDQU64    Z0, Z22
	VMOVDQU64    Z0, Z23
	VMOVDQU64    Z0, Z24
	VMOVDQU64    Z0, Z25
	VMOVDQU64    Z0, Z26
	VMOVDQU64    Z0, Z27
	VMOVDQU64    Z0, Z28
	VMOVDQU64    Z0, Z29
	VMOVDQU64    Z0, Z30
	VMOVDQU64    Z0, Z31

int16MinBlockLoop:
	CMPQ    DX, $0x00000400
	JL      int16MinTailLoop
	VPMINSW (AX), Z0, Z0
	VPMINSW 64(AX), Z1, Z1
	VPMINSW 128(AX), Z2, Z2
	VPMINSW 192(AX), Z3, Z3
	VPMINSW 256(AX), Z4, Z4
	VPMINSW 320(AX), Z5, Z5
	VPMINSW 384(AX), Z6, Z6
	VPMINSW 448(AX), Z7, Z7
	VPMINSW 512(AX), Z8, Z8
	VPMINSW 576(AX), Z9, Z9
	VPMINSW 640(AX), Z10, Z10
	VPMINSW 704(AX), Z11, Z11
	VPMINSW 768(AX), Z12, Z12
	VPMINSW 832(AX), Z13, Z13
	VPMINSW 896(AX), Z14, Z14
	VPMINSW 960(AX), Z15, Z15
	VPMINSW 1024(AX), Z16, Z16
	VPMINSW 1088(AX), Z17, Z17
	VPMINSW 1152(AX), Z18, Z18
	VPMINSW 1216(AX), Z19, Z19
	VPMINSW 1280(AX), Z20, Z20
	VPMINSW 1344(AX), Z21, Z21
	VPMINSW 1408(AX), Z22, Z22
	VPMINSW 1472(AX), Z23, Z23
	VPMINSW 1536(AX), Z24, Z24
	VPMINSW 1600(AX), Z25, Z25
	VPMINSW 1664(AX), Z26, Z26
	VPMINSW 1728(AX), Z27, Z27
	VPMINSW 1792(AX), Z28, Z28
	VPMINSW 1856(AX), Z29, Z29
	VPMINSW 1920(AX), Z30, Z30
	VPMINSW 1984(AX), Z31, Z31
	ADDQ    $0x00000800, AX
	SUBQ    $0x00000400, DX
	JMP     int16MinBlockLoop

int16MinTailLoop:
	CMPQ    DX, $0x00000020
	JL      int16MinDone
	VPMINSW (AX), Z0, Z0
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000020, DX
	JMP     int16MinTailLoop

int16MinDone:
	VPMINSW       Z0, Z1, Z0
	VPMINSW       Z0, Z2, Z0
	VPMINSW       Z0, Z3, Z0
	VPMINSW       Z0, Z4, Z0
	VPMINSW       Z0, Z5, Z0
	VPMINSW       Z0, Z6, Z0
	VPMINSW       Z0, Z7, Z0
	VPMINSW       Z0, Z8, Z0
	VPMINSW       Z0, Z9, Z0
	VPMINSW       Z0, Z10, Z0
	VPMINSW       Z0, Z11, Z0
	VPMINSW       Z0, Z12, Z0
	VPMINSW       Z0, Z13, Z0
	VPMINSW       Z0, Z14, Z0
	VPMINSW       Z0, Z15, Z0
	VPMINSW       Z0, Z16, Z0
	VPMINSW       Z0, Z17, Z0
	VPMINSW       Z0, Z18, Z0
	VPMINSW       Z0, Z19, Z0
	VPMINSW       Z0, Z20, Z0
	VPMINSW       Z0, Z21, Z0
	VPMINSW       Z0, Z22, Z0
	VPMINSW       Z0, Z23, Z0
	VPMINSW       Z0, Z24, Z0
	VPMINSW       Z0, Z25, Z0
	VPMINSW       Z0, Z26, Z0
	VPMINSW       Z0, Z27, Z0
	VPMINSW       Z0, Z28, Z0
	VPMINSW       Z0, Z29, Z0
	VPMINSW       Z0, Z30, Z0
	VPMINSW       Z0, Z31, Z0
	VEXTRACTI64X4 $0x01, Z0, Y1
	VPMINSW       Y1, Y0, Y0
	SUBQ          $0x00000010, DX
	JB            int16MinDone1
	VPMINSW       (AX), Y0, Y0
	ADDQ          $0x00000020, AX

int16MinDone1:
	VEXTRACTI128 $0x01, Y0, X1
	VPMINSW      X1, X0, X0
	CMPQ         DX, $0x00000008
	JL           int16MinDone2
	VPMINSW      (AX), X0, X0
	ADDQ         $0x00000020, AX

int16MinDone2:
	MOVOU X0, (CX)
	RET

// func int32MinAvx512Asm(x []int32, r []int32)
// Requires: AVX, AVX2, AVX512F, SSE2
TEXT ·int32MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x000000007fffffff, BX
	MOVQ         BX, X0
	VPBROADCASTD X0, Z0
	VMOVDQU64    Z0, Z1
	VMOVDQU64    Z0, Z2
	VMOVDQU64    Z0, Z3
	VMOVDQU64    Z0, Z4
	VMOVDQU64    Z0, Z5
	VMOVDQU64    Z0, Z6
	VMOVDQU64    Z0, Z7
	VMOVDQU64    Z0, Z8
	VMOVDQU64    Z0, Z9
	VMOVDQU64    Z0, Z10
	VMOVDQU64    Z0, Z11
	VMOVDQU64    Z0, Z12
	VMOVDQU64    Z0, Z13
	VMOVDQU64    Z0, Z14
	VMOVDQU64    Z0, Z15
	VMOVDQU64    Z0, Z16
	VMOVDQU64    Z0, Z17
	VMOVDQU64    Z0, Z18
	VMOVDQU64    Z0, Z19
	VMOVDQU64    Z0, Z20
	VMOVDQU64    Z0, Z21
	VMOVDQU64    Z0, Z22
	VMOVDQU64    Z0, Z23
	VMOVDQU64    Z0, Z24
	VMOVDQU64    Z0, Z25
	VMOVDQU64    Z0, Z26
	VMOVDQU64    Z0, Z27
	VMOVDQU64    Z0, Z28
	VMOVDQU64    Z0, Z29
	VMOVDQU64    Z0, Z30
	VMOVDQU64    Z0, Z31

int32MinBlockLoop:
	CMPQ    DX, $0x00000200
	JL      int32MinTailLoop
	VPMINSD (AX), Z0, Z0
	VPMINSD 64(AX), Z1, Z1
	VPMINSD 128(AX), Z2, Z2
	VPMINSD 192(AX), Z3, Z3
	VPMINSD 256(AX), Z4, Z4
	VPMINSD 320(AX), Z5, Z5
	VPMINSD 384(AX), Z6, Z6
	VPMINSD 448(AX), Z7, Z7
	VPMINSD 512(AX), Z8, Z8
	VPMINSD 576(AX), Z9, Z9
	VPMINSD 640(AX), Z10, Z10
	VPMINSD 704(AX), Z11, Z11
	VPMINSD 768(AX), Z12, Z12
	VPMINSD 832(AX), Z13, Z13
	VPMINSD 896(AX), Z14, Z14
	VPMINSD 960(AX), Z15, Z15
	VPMINSD 1024(AX), Z16, Z16
	VPMINSD 1088(AX), Z17, Z17
	VPMINSD 1152(AX), Z18, Z18
	VPMINSD 1216(AX), Z19, Z19
	VPMINSD 1280(AX), Z20, Z20
	VPMINSD 1344(AX), Z21, Z21
	VPMINSD 1408(AX), Z22, Z22
	VPMINSD 1472(AX), Z23, Z23
	VPMINSD 1536(AX), Z24, Z24
	VPMINSD 1600(AX), Z25, Z25
	VPMINSD 1664(AX), Z26, Z26
	VPMINSD 1728(AX), Z27, Z27
	VPMINSD 1792(AX), Z28, Z28
	VPMINSD 1856(AX), Z29, Z29
	VPMINSD 1920(AX), Z30, Z30
	VPMINSD 1984(AX), Z31, Z31
	ADDQ    $0x00000800, AX
	SUBQ    $0x00000200, DX
	JMP     int32MinBlockLoop

int32MinTailLoop:
	CMPQ    DX, $0x00000010
	JL      int32MinDone
	VPMINSD (AX), Z0, Z0
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000010, DX
	JMP     int32MinTailLoop

int32MinDone:
	VPMINSD       Z0, Z1, Z0
	VPMINSD       Z0, Z2, Z0
	VPMINSD       Z0, Z3, Z0
	VPMINSD       Z0, Z4, Z0
	VPMINSD       Z0, Z5, Z0
	VPMINSD       Z0, Z6, Z0
	VPMINSD       Z0, Z7, Z0
	VPMINSD       Z0, Z8, Z0
	VPMINSD       Z0, Z9, Z0
	VPMINSD       Z0, Z10, Z0
	VPMINSD       Z0, Z11, Z0
	VPMINSD       Z0, Z12, Z0
	VPMINSD       Z0, Z13, Z0
	VPMINSD       Z0, Z14, Z0
	VPMINSD       Z0, Z15, Z0
	VPMINSD       Z0, Z16, Z0
	VPMINSD       Z0, Z17, Z0
	VPMINSD       Z0, Z18, Z0
	VPMINSD       Z0, Z19, Z0
	VPMINSD       Z0, Z20, Z0
	VPMINSD       Z0, Z21, Z0
	VPMINSD       Z0, Z22, Z0
	VPMINSD       Z0, Z23, Z0
	VPMINSD       Z0, Z24, Z0
	VPMINSD       Z0, Z25, Z0
	VPMINSD       Z0, Z26, Z0
	VPMINSD       Z0, Z27, Z0
	VPMINSD       Z0, Z28, Z0
	VPMINSD       Z0, Z29, Z0
	VPMINSD       Z0, Z30, Z0
	VPMINSD       Z0, Z31, Z0
	VEXTRACTI64X4 $0x01, Z0, Y1
	VPMINSD       Y1, Y0, Y0
	SUBQ          $0x00000008, DX
	JB            int32MinDone1
	VPMINSD       (AX), Y0, Y0
	ADDQ          $0x00000020, AX

int32MinDone1:
	VEXTRACTI128 $0x01, Y0, X1
	VPMINSD      X1, X0, X0
	CMPQ         DX, $0x00000004
	JL           int32MinDone2
	VPMINSD      (AX), X0, X0
	ADDQ         $0x00000020, AX

int32MinDone2:
	MOVOU X0, (CX)
	RET

// func int64MinAvx512Asm(x []int64, r []int64)
// Requires: AVX2, AVX512F, AVX512VL, SSE2
TEXT ·int64MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x7fffffffffffffff, BX
	MOVQ         BX, X0
	VPBROADCASTQ X0, Z0
	VMOVDQU64    Z0, Z1
	VMOVDQU64    Z0, Z2
	VMOVDQU64    Z0, Z3
	VMOVDQU64    Z0, Z4
	VMOVDQU64    Z0, Z5
	VMOVDQU64    Z0, Z6
	VMOVDQU64    Z0, Z7
	VMOVDQU64    Z0, Z8
	VMOVDQU64    Z0, Z9
	VMOVDQU64    Z0, Z10
	VMOVDQU64    Z0, Z11
	VMOVDQU64    Z0, Z12
	VMOVDQU64    Z0, Z13
	VMOVDQU64    Z0, Z14
	VMOVDQU64    Z0, Z15
	VMOVDQU64    Z0, Z16
	VMOVDQU64    Z0, Z17
	VMOVDQU64    Z0, Z18
	VMOVDQU64    Z0, Z19
	VMOVDQU64    Z0, Z20
	VMOVDQU64    Z0, Z21
	VMOVDQU64    Z0, Z22
	VMOVDQU64    Z0, Z23
	VMOVDQU64    Z0, Z24
	VMOVDQU64    Z0, Z25
	VMOVDQU64    Z0, Z26
	VMOVDQU64    Z0, Z27
	VMOVDQU64    Z0, Z28
	VMOVDQU64    Z0, Z29
	VMOVDQU64    Z0, Z30
	VMOVDQU64    Z0, Z31

int64MinBlockLoop:
	CMPQ    DX, $0x00000100
	JL      int64MinTailLoop
	VPMINSQ (AX), Z0, Z0
	VPMINSQ 64(AX), Z1, Z1
	VPMINSQ 128(AX), Z2, Z2
	VPMINSQ 192(AX), Z3, Z3
	VPMINSQ 256(AX), Z4, Z4
	VPMINSQ 320(AX), Z5, Z5
	VPMINSQ 384(AX), Z6, Z6
	VPMINSQ 448(AX), Z7, Z7
	VPMINSQ 512(AX), Z8, Z8
	VPMINSQ 576(AX), Z9, Z9
	VPMINSQ 640(AX), Z10, Z10
	VPMINSQ 704(AX), Z11, Z11
	VPMINSQ 768(AX), Z12, Z12
	VPMINSQ 832(AX), Z13, Z13
	VPMINSQ 896(AX), Z14, Z14
	VPMINSQ 960(AX), Z15, Z15
	VPMINSQ 1024(AX), Z16, Z16
	VPMINSQ 1088(AX), Z17, Z17
	VPMINSQ 1152(AX), Z18, Z18
	VPMINSQ 1216(AX), Z19, Z19
	VPMINSQ 1280(AX), Z20, Z20
	VPMINSQ 1344(AX), Z21, Z21
	VPMINSQ 1408(AX), Z22, Z22
	VPMINSQ 1472(AX), Z23, Z23
	VPMINSQ 1536(AX), Z24, Z24
	VPMINSQ 1600(AX), Z25, Z25
	VPMINSQ 1664(AX), Z26, Z26
	VPMINSQ 1728(AX), Z27, Z27
	VPMINSQ 1792(AX), Z28, Z28
	VPMINSQ 1856(AX), Z29, Z29
	VPMINSQ 1920(AX), Z30, Z30
	VPMINSQ 1984(AX), Z31, Z31
	ADDQ    $0x00000800, AX
	SUBQ    $0x00000100, DX
	JMP     int64MinBlockLoop

int64MinTailLoop:
	CMPQ    DX, $0x00000008
	JL      int64MinDone
	VPMINSQ (AX), Z0, Z0
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000008, DX
	JMP     int64MinTailLoop

int64MinDone:
	VPMINSQ       Z0, Z1, Z0
	VPMINSQ       Z0, Z2, Z0
	VPMINSQ       Z0, Z3, Z0
	VPMINSQ       Z0, Z4, Z0
	VPMINSQ       Z0, Z5, Z0
	VPMINSQ       Z0, Z6, Z0
	VPMINSQ       Z0, Z7, Z0
	VPMINSQ       Z0, Z8, Z0
	VPMINSQ       Z0, Z9, Z0
	VPMINSQ       Z0, Z10, Z0
	VPMINSQ       Z0, Z11, Z0
	VPMINSQ       Z0, Z12, Z0
	VPMINSQ       Z0, Z13, Z0
	VPMINSQ       Z0, Z14, Z0
	VPMINSQ       Z0, Z15, Z0
	VPMINSQ       Z0, Z16, Z0
	VPMINSQ       Z0, Z17, Z0
	VPMINSQ       Z0, Z18, Z0
	VPMINSQ       Z0, Z19, Z0
	VPMINSQ       Z0, Z20, Z0
	VPMINSQ       Z0, Z21, Z0
	VPMINSQ       Z0, Z22, Z0
	VPMINSQ       Z0, Z23, Z0
	VPMINSQ       Z0, Z24, Z0
	VPMINSQ       Z0, Z25, Z0
	VPMINSQ       Z0, Z26, Z0
	VPMINSQ       Z0, Z27, Z0
	VPMINSQ       Z0, Z28, Z0
	VPMINSQ       Z0, Z29, Z0
	VPMINSQ       Z0, Z30, Z0
	VPMINSQ       Z0, Z31, Z0
	VEXTRACTI64X4 $0x01, Z0, Y1
	VPMINSQ       Y1, Y0, Y0
	SUBQ          $0x00000004, DX
	JB            int64MinDone1
	VPMINSQ       (AX), Y0, Y0
	ADDQ          $0x00000020, AX

int64MinDone1:
	VEXTRACTI128 $0x01, Y0, X1
	VPMINSQ      X1, X0, X0
	CMPQ         DX, $0x00000002
	JL           int64MinDone2
	VPMINSQ      (AX), X0, X0
	ADDQ         $0x00000020, AX

int64MinDone2:
	MOVOU X0, (CX)
	RET

// func uint8MinAvx512Asm(x []uint8, r []uint8)
// Requires: AVX, AVX2, AVX512BW, AVX512F, SSE2
TEXT ·uint8MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ       x_base+0(FP), AX
	MOVQ       r_base+24(FP), CX
	MOVQ       x_len+8(FP), DX
	VPTERNLOGD $0xff, Z0, Z0, Z0
	VPTERNLOGD $0xff, Z1, Z1, Z1
	VPTERNLOGD $0xff, Z2, Z2, Z2
	VPTERNLOGD $0xff, Z3, Z3, Z3
	VPTERNLOGD $0xff, Z4, Z4, Z4
	VPTERNLOGD $0xff, Z5, Z5, Z5
	VPTERNLOGD $0xff, Z6, Z6, Z6
	VPTERNLOGD $0xff, Z7, Z7, Z7
	VPTERNLOGD $0xff, Z8, Z8, Z8
	VPTERNLOGD $0xff, Z9, Z9, Z9
	VPTERNLOGD $0xff, Z10, Z10, Z10
	VPTERNLOGD $0xff, Z11, Z11, Z11
	VPTERNLOGD $0xff, Z12, Z12, Z12
	VPTERNLOGD $0xff, Z13, Z13, Z13
	VPTERNLOGD $0xff, Z14, Z14, Z14
	VPTERNLOGD $0xff, Z15, Z15, Z15
	VPTERNLOGD $0xff, Z16, Z16, Z16
	VPTERNLOGD $0xff, Z17, Z17, Z17
	VPTERNLOGD $0xff, Z18, Z18, Z18
	VPTERNLOGD $0xff, Z19, Z19, Z19
	VPTERNLOGD $0xff, Z20, Z20, Z20
	VPTERNLOGD $0xff, Z21, Z21, Z21
	VPTERNLOGD $0xff, Z22, Z22, Z22
	VPTERNLOGD $0xff, Z23, Z23, Z23
	VPTERNLOGD $0xff, Z24, Z24, Z24
	VPTERNLOGD $0xff, Z25, Z25, Z25
	VPTERNLOGD $0xff, Z26, Z26, Z26
	VPTERNLOGD $0xff, Z27, Z27, Z27
	VPTERNLOGD $0xff, Z28, Z28, Z28
	VPTERNLOGD $0xff, Z29, Z29, Z29
	VPTERNLOGD $0xff, Z30, Z30, Z30
	VPTERNLOGD $0xff, Z31, Z31, Z31

uint8MinBlockLoop:
	CMPQ    DX, $0x00000800
	JL      uint8MinTailLoop
	VPMINUB (AX), Z0, Z0
	VPMINUB 64(AX), Z1, Z1
	VPMINUB 128(AX), Z2, Z2
	VPMINUB 192(AX), Z3, Z3
	VPMINUB 256(AX), Z4, Z4
	VPMINUB 320(AX), Z5, Z5
	VPMINUB 384(AX), Z6, Z6
	VPMINUB 448(AX), Z7, Z7
	VPMINUB 512(AX), Z8, Z8
	VPMINUB 576(AX), Z9, Z9
	VPMINUB 640(AX), Z10, Z10
	VPMINUB 704(AX), Z11, Z11
	VPMINUB 768(AX), Z12, Z12
	VPMINUB 832(AX), Z13, Z13
	VPMINUB 896(AX), Z14, Z14
	VPMINUB 960(AX), Z15, Z15
	VPMINUB 1024(AX), Z16, Z16
	VPMINUB 1088(AX), Z17, Z17
	VPMINUB 1152(AX), Z18, Z18
	VPMINUB 1216(AX), Z19, Z19
	VPMINUB 1280(AX), Z20, Z20
	VPMINUB 1344(AX), Z21, Z21
	VPMINUB 1408(AX), Z22, Z22
	VPMINUB 1472(AX), Z23, Z23
	VPMINUB 1536(AX), Z24, Z24
	VPMINUB 1600(AX), Z25, Z25
	VPMINUB 1664(AX), Z26, Z26
	VPMINUB 1728(AX), Z27, Z27
	VPMINUB 1792(AX), Z28, Z28
	VPMINUB 1856(AX), Z29, Z29
	VPMINUB 1920(AX), Z30, Z30
	VPMINUB 1984(AX), Z31, Z31
	ADDQ    $0x00000800, AX
	SUBQ    $0x00000800, DX
	JMP     uint8MinBlockLoop

uint8MinTailLoop:
	CMPQ    DX, $0x00000040
	JL      uint8MinDone
	VPMINUB (AX), Z0, Z0
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000040, DX
	JMP     uint8MinTailLoop

uint8MinDone:
	VPMINUB       Z0, Z1, Z0
	VPMINUB       Z0, Z2, Z0
	VPMINUB       Z0, Z3, Z0
	VPMINUB       Z0, Z4, Z0
	VPMINUB       Z0, Z5, Z0
	VPMINUB       Z0, Z6, Z0
	VPMINUB       Z0, Z7, Z0
	VPMINUB       Z0, Z8, Z0
	VPMINUB       Z0, Z9, Z0
	VPMINUB       Z0, Z10, Z0
	VPMINUB       Z0, Z11, Z0
	VPMINUB       Z0, Z12, Z0
	VPMINUB       Z0, Z13, Z0
	VPMINUB       Z0, Z14, Z0
	VPMINUB       Z0, Z15, Z0
	VPMINUB       Z0, Z16, Z0
	VPMINUB       Z0, Z17, Z0
	VPMINUB       Z0, Z18, Z0
	VPMINUB       Z0, Z19, Z0
	VPMINUB       Z0, Z20, Z0
	VPMINUB       Z0, Z21, Z0
	VPMINUB       Z0, Z22, Z0
	VPMINUB       Z0, Z23, Z0
	VPMINUB       Z0, Z24, Z0
	VPMINUB       Z0, Z25, Z0
	VPMINUB       Z0, Z26, Z0
	VPMINUB       Z0, Z27, Z0
	VPMINUB       Z0, Z28, Z0
	VPMINUB       Z0, Z29, Z0
	VPMINUB       Z0, Z30, Z0
	VPMINUB       Z0, Z31, Z0
	VEXTRACTI64X4 $0x01, Z0, Y1
	VPMINUB       Y1, Y0, Y0
	SUBQ          $0x00000020, DX
	JB            uint8MinDone1
	VPMINUB       (AX), Y0, Y0
	ADDQ          $0x00000020, AX

uint8MinDone1:
	VEXTRACTI128 $0x01, Y0, X1
	VPMINUB      X1, X0, X0
	CMPQ         DX, $0x00000010
	JL           uint8MinDone2
	VPMINUB      (AX), X0, X0
	ADDQ         $0x00000020, AX

uint8MinDone2:
	MOVOU X0, (CX)
	RET

// func uint16MinAvx512Asm(x []uint16, r []uint16)
// Requires: AVX, AVX2, AVX512BW, AVX512F, SSE2
TEXT ·uint16MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ       x_base+0(FP), AX
	MOVQ       r_base+24(FP), CX
	MOVQ       x_len+8(FP), DX
	VPTERNLOGD $0xff, Z0, Z0, Z0
	VPTERNLOGD $0xff, Z1, Z1, Z1
	VPTERNLOGD $0xff, Z2, Z2, Z2
	VPTERNLOGD $0xff, Z3, Z3, Z3
	VPTERNLOGD $0xff, Z4, Z4, Z4
	VPTERNLOGD $0xff, Z5, Z5, Z5
	VPTERNLOGD $0xff, Z6, Z6, Z6
	VPTERNLOGD $0xff, Z7, Z7, Z7
	VPTERNLOGD $0xff, Z8, Z8, Z8
	VPTERNLOGD $0xff, Z9, Z9, Z9
	VPTERNLOGD $0xff, Z10, Z10, Z10
	VPTERNLOGD $0xff, Z11, Z11, Z11
	VPTERNLOGD $0xff, Z12, Z12, Z12
	VPTERNLOGD $0xff, Z13, Z13, Z13
	VPTERNLOGD $0xff, Z14, Z14, Z14
	VPTERNLOGD $0xff, Z15, Z15, Z15
	VPTERNLOGD $0xff, Z16, Z16, Z16
	VPTERNLOGD $0xff, Z17, Z17, Z17
	VPTERNLOGD $0xff, Z18, Z18, Z18
	VPTERNLOGD $0xff, Z19, Z19, Z19
	VPTERNLOGD $0xff, Z20, Z20, Z20
	VPTERNLOGD $0xff, Z21, Z21, Z21
	VPTERNLOGD $0xff, Z22, Z22, Z22
	VPTERNLOGD $0xff, Z23, Z23, Z23
	VPTERNLOGD $0xff, Z24, Z24, Z24
	VPTERNLOGD $0xff, Z25, Z25, Z25
	VPTERNLOGD $0xff, Z26, Z26, Z26
	VPTERNLOGD $0xff, Z27, Z27, Z27
	VPTERNLOGD $0xff, Z28, Z28, Z28
	VPTERNLOGD $0xff, Z29, Z29, Z29
	VPTERNLOGD $0xff, Z30, Z30, Z30
	VPTERNLOGD $0xff, Z31, Z31, Z31

uint16MinBlockLoop:
	CMPQ    DX, $0x00000400
	JL      uint16MinTailLoop
	VPMINUW (AX), Z0, Z0
	VPMINUW 64(AX), Z1, Z1
	VPMINUW 128(AX), Z2, Z2
	VPMINUW 192(AX), Z3, Z3
	VPMINUW 256(AX), Z4, Z4
	VPMINUW 320(AX), Z5, Z5
	VPMINUW 384(AX), Z6, Z6
	VPMINUW 448(AX), Z7, Z7
	VPMINUW 512(AX), Z8, Z8
	VPMINUW 576(AX), Z9, Z9
	VPMINUW 640(AX), Z10, Z10
	VPMINUW 704(AX), Z11, Z11
	VPMINUW 768(AX), Z12, Z12
	VPMINUW 832(AX), Z13, Z13
	VPMINUW 896(AX), Z14, Z14
	VPMINUW 960(AX), Z15, Z15
	VPMINUW 1024(AX), Z16, Z16
	VPMINUW 1088(AX), Z17, Z17
	VPMINUW 1152(AX), Z18, Z18
	VPMINUW 1216(AX), Z19, Z19
	VPMINUW 1280(AX), Z20, Z20
	VPMINUW 1344(AX), Z21, Z21
	VPMINUW 1408(AX), Z22, Z22
	VPMINUW 1472(AX), Z23, Z23
	VPMINUW 1536(AX), Z24, Z24
	VPMINUW 1600(AX), Z25, Z25
	VPMINUW 1664(AX), Z26, Z26
	VPMINUW 1728(AX), Z27, Z27
	VPMINUW 1792(AX), Z28, Z28
	VPMINUW 1856(AX), Z29, Z29
	VPMINUW 1920(AX), Z30, Z30
	VPMINUW 1984(AX), Z31, Z31
	ADDQ    $0x00000800, AX
	SUBQ    $0x00000400, DX
	JMP     uint16MinBlockLoop

uint16MinTailLoop:
	CMPQ    DX, $0x00000020
	JL      uint16MinDone
	VPMINUW (AX), Z0, Z0
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000020, DX
	JMP     uint16MinTailLoop

uint16MinDone:
	VPMINUW       Z0, Z1, Z0
	VPMINUW       Z0, Z2, Z0
	VPMINUW       Z0, Z3, Z0
	VPMINUW       Z0, Z4, Z0
	VPMINUW       Z0, Z5, Z0
	VPMINUW       Z0, Z6, Z0
	VPMINUW       Z0, Z7, Z0
	VPMINUW       Z0, Z8, Z0
	VPMINUW       Z0, Z9, Z0
	VPMINUW       Z0, Z10, Z0
	VPMINUW       Z0, Z11, Z0
	VPMINUW       Z0, Z12, Z0
	VPMINUW       Z0, Z13, Z0
	VPMINUW       Z0, Z14, Z0
	VPMINUW       Z0, Z15, Z0
	VPMINUW       Z0, Z16, Z0
	VPMINUW       Z0, Z17, Z0
	VPMINUW       Z0, Z18, Z0
	VPMINUW       Z0, Z19, Z0
	VPMINUW       Z0, Z20, Z0
	VPMINUW       Z0, Z21, Z0
	VPMINUW       Z0, Z22, Z0
	VPMINUW       Z0, Z23, Z0
	VPMINUW       Z0, Z24, Z0
	VPMINUW       Z0, Z25, Z0
	VPMINUW       Z0, Z26, Z0
	VPMINUW       Z0, Z27, Z0
	VPMINUW       Z0, Z28, Z0
	VPMINUW       Z0, Z29, Z0
	VPMINUW       Z0, Z30, Z0
	VPMINUW       Z0, Z31, Z0
	VEXTRACTI64X4 $0x01, Z0, Y1
	VPMINUW       Y1, Y0, Y0
	SUBQ          $0x00000010, DX
	JB            uint16MinDone1
	VPMINUW       (AX), Y0, Y0
	ADDQ          $0x00000020, AX

uint16MinDone1:
	VEXTRACTI128 $0x01, Y0, X1
	VPMINUW      X1, X0, X0
	CMPQ         DX, $0x00000008
	JL           uint16MinDone2
	VPMINUW      (AX), X0, X0
	ADDQ         $0x00000020, AX

uint16MinDone2:
	MOVOU X0, (CX)
	RET

// func uint32MinAvx512Asm(x []uint32, r []uint32)
// Requires: AVX, AVX2, AVX512F, SSE2
TEXT ·uint32MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ       x_base+0(FP), AX
	MOVQ       r_base+24(FP), CX
	MOVQ       x_len+8(FP), DX
	VPTERNLOGD $0xff, Z0, Z0, Z0
	VPTERNLOGD $0xff, Z1, Z1, Z1
	VPTERNLOGD $0xff, Z2, Z2, Z2
	VPTERNLOGD $0xff, Z3, Z3, Z3
	VPTERNLOGD $0xff, Z4, Z4, Z4
	VPTERNLOGD $0xff, Z5, Z5, Z5
	VPTERNLOGD $0xff, Z6, Z6, Z6
	VPTERNLOGD $0xff, Z7, Z7, Z7
	VPTERNLOGD $0xff, Z8, Z8, Z8
	VPTERNLOGD $0xff, Z9, Z9, Z9
	VPTERNLOGD $0xff, Z10, Z10, Z10
	VPTERNLOGD $0xff, Z11, Z11, Z11
	VPTERNLOGD $0xff, Z12, Z12, Z12
	VPTERNLOGD $0xff, Z13, Z13, Z13
	VPTERNLOGD $0xff, Z14, Z14, Z14
	VPTERNLOGD $0xff, Z15, Z15, Z15
	VPTERNLOGD $0xff, Z16, Z16, Z16
	VPTERNLOGD $0xff, Z17, Z17, Z17
	VPTERNLOGD $0xff, Z18, Z18, Z18
	VPTERNLOGD $0xff, Z19, Z19, Z19
	VPTERNLOGD $0xff, Z20, Z20, Z20
	VPTERNLOGD $0xff, Z21, Z21, Z21
	VPTERNLOGD $0xff, Z22, Z22, Z22
	VPTERNLOGD $0xff, Z23, Z23, Z23
	VPTERNLOGD $0xff, Z24, Z24, Z24
	VPTERNLOGD $0xff, Z25, Z25, Z25
	VPTERNLOGD $0xff, Z26, Z26, Z26
	VPTERNLOGD $0xff, Z27, Z27, Z27
	VPTERNLOGD $0xff, Z28, Z28, Z28
	VPTERNLOGD $0xff, Z29, Z29, Z29
	VPTERNLOGD $0xff, Z30, Z30, Z30
	VPTERNLOGD $0xff, Z31, Z31, Z31

uint32MinBlockLoop:
	CMPQ    DX, $0x00000200
	JL      uint32MinTailLoop
	VPMINUD (AX), Z0, Z0
	VPMINUD 64(AX), Z1, Z1
	VPMINUD 128(AX), Z2, Z2
	VPMINUD 192(AX), Z3, Z3
	VPMINUD 256(AX), Z4, Z4
	VPMINUD 320(AX), Z5, Z5
	VPMINUD 384(AX), Z6, Z6
	VPMINUD 448(AX), Z7, Z7
	VPMINUD 512(AX), Z8, Z8
	VPMINUD 576(AX), Z9, Z9
	VPMINUD 640(AX), Z10, Z10
	VPMINUD 704(AX), Z11, Z11
	VPMINUD 768(AX), Z12, Z12
	VPMINUD 832(AX), Z13, Z13
	VPMINUD 896(AX), Z14, Z14
	VPMINUD 960(AX), Z15, Z15
	VPMINUD 1024(AX), Z16, Z16
	VPMINUD 1088(AX), Z17, Z17
	VPMINUD 1152(AX), Z18, Z18
	VPMINUD 1216(AX), Z19, Z19
	VPMINUD 1280(AX), Z20, Z20
	VPMINUD 1344(AX), Z21, Z21
	VPMINUD 1408(AX), Z22, Z22
	VPMINUD 1472(AX), Z23, Z23
	VPMINUD 1536(AX), Z24, Z24
	VPMINUD 1600(AX), Z25, Z25
	VPMINUD 1664(AX), Z26, Z26
	VPMINUD 1728(AX), Z27, Z27
	VPMINUD 1792(AX), Z28, Z28
	VPMINUD 1856(AX), Z29, Z29
	VPMINUD 1920(AX), Z30, Z30
	VPMINUD 1984(AX), Z31, Z31
	ADDQ    $0x00000800, AX
	SUBQ    $0x00000200, DX
	JMP     uint32MinBlockLoop

uint32MinTailLoop:
	CMPQ    DX, $0x00000010
	JL      uint32MinDone
	VPMINUD (AX), Z0, Z0
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000010, DX
	JMP     uint32MinTailLoop

uint32MinDone:
	VPMINUD       Z0, Z1, Z0
	VPMINUD       Z0, Z2, Z0
	VPMINUD       Z0, Z3, Z0
	VPMINUD       Z0, Z4, Z0
	VPMINUD       Z0, Z5, Z0
	VPMINUD       Z0, Z6, Z0
	VPMINUD       Z0, Z7, Z0
	VPMINUD       Z0, Z8, Z0
	VPMINUD       Z0, Z9, Z0
	VPMINUD       Z0, Z10, Z0
	VPMINUD       Z0, Z11, Z0
	VPMINUD       Z0, Z12, Z0
	VPMINUD       Z0, Z13, Z0
	VPMINUD       Z0, Z14, Z0
	VPMINUD       Z0, Z15, Z0
	VPMINUD       Z0, Z16, Z0
	VPMINUD       Z0, Z17, Z0
	VPMINUD       Z0, Z18, Z0
	VPMINUD       Z0, Z19, Z0
	VPMINUD       Z0, Z20, Z0
	VPMINUD       Z0, Z21, Z0
	VPMINUD       Z0, Z22, Z0
	VPMINUD       Z0, Z23, Z0
	VPMINUD       Z0, Z24, Z0
	VPMINUD       Z0, Z25, Z0
	VPMINUD       Z0, Z26, Z0
	VPMINUD       Z0, Z27, Z0
	VPMINUD       Z0, Z28, Z0
	VPMINUD       Z0, Z29, Z0
	VPMINUD       Z0, Z30, Z0
	VPMINUD       Z0, Z31, Z0
	VEXTRACTI64X4 $0x01, Z0, Y1
	VPMINUD       Y1, Y0, Y0
	SUBQ          $0x00000008, DX
	JB            uint32MinDone1
	VPMINUD       (AX), Y0, Y0
	ADDQ          $0x00000020, AX

uint32MinDone1:
	VEXTRACTI128 $0x01, Y0, X1
	VPMINUD      X1, X0, X0
	CMPQ         DX, $0x00000004
	JL           uint32MinDone2
	VPMINUD      (AX), X0, X0
	ADDQ         $0x00000020, AX

uint32MinDone2:
	MOVOU X0, (CX)
	RET

// func uint64MinAvx512Asm(x []uint64, r []uint64)
// Requires: AVX2, AVX512F, AVX512VL, SSE2
TEXT ·uint64MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ       x_base+0(FP), AX
	MOVQ       r_base+24(FP), CX
	MOVQ       x_len+8(FP), DX
	VPTERNLOGD $0xff, Z0, Z0, Z0
	VPTERNLOGD $0xff, Z1, Z1, Z1
	VPTERNLOGD $0xff, Z2, Z2, Z2
	VPTERNLOGD $0xff, Z3, Z3, Z3
	VPTERNLOGD $0xff, Z4, Z4, Z4
	VPTERNLOGD $0xff, Z5, Z5, Z5
	VPTERNLOGD $0xff, Z6, Z6, Z6
	VPTERNLOGD $0xff, Z7, Z7, Z7
	VPTERNLOGD $0xff, Z8, Z8, Z8
	VPTERNLOGD $0xff, Z9, Z9, Z9
	VPTERNLOGD $0xff, Z10, Z10, Z10
	VPTERNLOGD $0xff, Z11, Z11, Z11
	VPTERNLOGD $0xff, Z12, Z12, Z12
	VPTERNLOGD $0xff, Z13, Z13, Z13
	VPTERNLOGD $0xff, Z14, Z14, Z14
	VPTERNLOGD $0xff, Z15, Z15, Z15
	VPTERNLOGD $0xff, Z16, Z16, Z16
	VPTERNLOGD $0xff, Z17, Z17, Z17
	VPTERNLOGD $0xff, Z18, Z18, Z18
	VPTERNLOGD $0xff, Z19, Z19, Z19
	VPTERNLOGD $0xff, Z20, Z20, Z20
	VPTERNLOGD $0xff, Z21, Z21, Z21
	VPTERNLOGD $0xff, Z22, Z22, Z22
	VPTERNLOGD $0xff, Z23, Z23, Z23
	VPTERNLOGD $0xff, Z24, Z24, Z24
	VPTERNLOGD $0xff, Z25, Z25, Z25
	VPTERNLOGD $0xff, Z26, Z26, Z26
	VPTERNLOGD $0xff, Z27, Z27, Z27
	VPTERNLOGD $0xff, Z28, Z28, Z28
	VPTERNLOGD $0xff, Z29, Z29, Z29
	VPTERNLOGD $0xff, Z30, Z30, Z30
	VPTERNLOGD $0xff, Z31, Z31, Z31

uint64MinBlockLoop:
	CMPQ    DX, $0x00000100
	JL      uint64MinTailLoop
	VPMINUQ (AX), Z0, Z0
	VPMINUQ 64(AX), Z1, Z1
	VPMINUQ 128(AX), Z2, Z2
	VPMINUQ 192(AX), Z3, Z3
	VPMINUQ 256(AX), Z4, Z4
	VPMINUQ 320(AX), Z5, Z5
	VPMINUQ 384(AX), Z6, Z6
	VPMINUQ 448(AX), Z7, Z7
	VPMINUQ 512(AX), Z8, Z8
	VPMINUQ 576(AX), Z9, Z9
	VPMINUQ 640(AX), Z10, Z10
	VPMINUQ 704(AX), Z11, Z11
	VPMINUQ 768(AX), Z12, Z12
	VPMINUQ 832(AX), Z13, Z13
	VPMINUQ 896(AX), Z14, Z14
	VPMINUQ 960(AX), Z15, Z15
	VPMINUQ 1024(AX), Z16, Z16
	VPMINUQ 1088(AX), Z17, Z17
	VPMINUQ 1152(AX), Z18, Z18
	VPMINUQ 1216(AX), Z19, Z19
	VPMINUQ 1280(AX), Z20, Z20
	VPMINUQ 1344(AX), Z21, Z21
	VPMINUQ 1408(AX), Z22, Z22
	VPMINUQ 1472(AX), Z23, Z23
	VPMINUQ 1536(AX), Z24, Z24
	VPMINUQ 1600(AX), Z25, Z25
	VPMINUQ 1664(AX), Z26, Z26
	VPMINUQ 1728(AX), Z27, Z27
	VPMINUQ 1792(AX), Z28, Z28
	VPMINUQ 1856(AX), Z29, Z29
	VPMINUQ 1920(AX), Z30, Z30
	VPMINUQ 1984(AX), Z31, Z31
	ADDQ    $0x00000800, AX
	SUBQ    $0x00000100, DX
	JMP     uint64MinBlockLoop

uint64MinTailLoop:
	CMPQ    DX, $0x00000008
	JL      uint64MinDone
	VPMINUQ (AX), Z0, Z0
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000008, DX
	JMP     uint64MinTailLoop

uint64MinDone:
	VPMINUQ       Z0, Z1, Z0
	VPMINUQ       Z0, Z2, Z0
	VPMINUQ       Z0, Z3, Z0
	VPMINUQ       Z0, Z4, Z0
	VPMINUQ       Z0, Z5, Z0
	VPMINUQ       Z0, Z6, Z0
	VPMINUQ       Z0, Z7, Z0
	VPMINUQ       Z0, Z8, Z0
	VPMINUQ       Z0, Z9, Z0
	VPMINUQ       Z0, Z10, Z0
	VPMINUQ       Z0, Z11, Z0
	VPMINUQ       Z0, Z12, Z0
	VPMINUQ       Z0, Z13, Z0
	VPMINUQ       Z0, Z14, Z0
	VPMINUQ       Z0, Z15, Z0
	VPMINUQ       Z0, Z16, Z0
	VPMINUQ       Z0, Z17, Z0
	VPMINUQ       Z0, Z18, Z0
	VPMINUQ       Z0, Z19, Z0
	VPMINUQ       Z0, Z20, Z0
	VPMINUQ       Z0, Z21, Z0
	VPMINUQ       Z0, Z22, Z0
	VPMINUQ       Z0, Z23, Z0
	VPMINUQ       Z0, Z24, Z0
	VPMINUQ       Z0, Z25, Z0
	VPMINUQ       Z0, Z26, Z0
	VPMINUQ       Z0, Z27, Z0
	VPMINUQ       Z0, Z28, Z0
	VPMINUQ       Z0, Z29, Z0
	VPMINUQ       Z0, Z30, Z0
	VPMINUQ       Z0, Z31, Z0
	VEXTRACTI64X4 $0x01, Z0, Y1
	VPMINUQ       Y1, Y0, Y0
	SUBQ          $0x00000004, DX
	JB            uint64MinDone1
	VPMINUQ       (AX), Y0, Y0
	ADDQ          $0x00000020, AX

uint64MinDone1:
	VEXTRACTI128 $0x01, Y0, X1
	VPMINUQ      X1, X0, X0
	CMPQ         DX, $0x00000002
	JL           uint64MinDone2
	VPMINUQ      (AX), X0, X0
	ADDQ         $0x00000020, AX

uint64MinDone2:
	MOVOU X0, (CX)
	RET

// func float32MinAvx512Asm(x []float32, r []float32)
// Requires: AVX, AVX512DQ, AVX512F, SSE2
TEXT ·float32MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x000000007f7fffff, BX
	MOVQ         BX, X0
	VBROADCASTSS X0, Z0
	VMOVUPS      Z0, Z1
	VMOVUPS      Z0, Z2
	VMOVUPS      Z0, Z3
	VMOVUPS      Z0, Z4
	VMOVUPS      Z0, Z5
	VMOVUPS      Z0, Z6
	VMOVUPS      Z0, Z7
	VMOVUPS      Z0, Z8
	VMOVUPS      Z0, Z9
	VMOVUPS      Z0, Z10
	VMOVUPS      Z0, Z11
	VMOVUPS      Z0, Z12
	VMOVUPS      Z0, Z13
	VMOVUPS      Z0, Z14
	VMOVUPS      Z0, Z15
	VMOVUPS      Z0, Z16
	VMOVUPS      Z0, Z17
	VMOVUPS      Z0, Z18
	VMOVUPS      Z0, Z19
	VMOVUPS      Z0, Z20
	VMOVUPS      Z0, Z21
	VMOVUPS      Z0, Z22
	VMOVUPS      Z0, Z23
	VMOVUPS      Z0, Z24
	VMOVUPS      Z0, Z25
	VMOVUPS      Z0, Z26
	VMOVUPS      Z0, Z27
	VMOVUPS      Z0, Z28
	VMOVUPS      Z0, Z29
	VMOVUPS      Z0, Z30
	VMOVUPS      Z0, Z31

float32MinBlockLoop:
	CMPQ   DX, $0x00000200
	JL     float32MinTailLoop
	VMINPS (AX), Z0, Z0
	VMINPS 64(AX), Z1, Z1
	VMINPS 128(AX), Z2, Z2
	VMINPS 192(AX), Z3, Z3
	VMINPS 256(AX), Z4, Z4
	VMINPS 320(AX), Z5, Z5
	VMINPS 384(AX), Z6, Z6
	VMINPS 448(AX), Z7, Z7
	VMINPS 512(AX), Z8, Z8
	VMINPS 576(AX), Z9, Z9
	VMINPS 640(AX), Z10, Z10
	VMINPS 704(AX), Z11, Z11
	VMINPS 768(AX), Z12, Z12
	VMINPS 832(AX), Z13, Z13
	VMINPS 896(AX), Z14, Z14
	VMINPS 960(AX), Z15, Z15
	VMINPS 1024(AX), Z16, Z16
	VMINPS 1088(AX), Z17, Z17
	VMINPS 1152(AX), Z18, Z18
	VMINPS 1216(AX), Z19, Z19
	VMINPS 1280(AX), Z20, Z20
	VMINPS 1344(AX), Z21, Z21
	VMINPS 1408(AX), Z22, Z22
	VMINPS 1472(AX), Z23, Z23
	VMINPS 1536(AX), Z24, Z24
	VMINPS 1600(AX), Z25, Z25
	VMINPS 1664(AX), Z26, Z26
	VMINPS 1728(AX), Z27, Z27
	VMINPS 1792(AX), Z28, Z28
	VMINPS 1856(AX), Z29, Z29
	VMINPS 1920(AX), Z30, Z30
	VMINPS 1984(AX), Z31, Z31
	ADDQ   $0x00000800, AX
	SUBQ   $0x00000200, DX
	JMP    float32MinBlockLoop

float32MinTailLoop:
	CMPQ   DX, $0x00000010
	JL     float32MinDone
	VMINPS (AX), Z0, Z0
	ADDQ   $0x00000040, AX
	SUBQ   $0x00000010, DX
	JMP    float32MinTailLoop

float32MinDone:
	VMINPS        Z0, Z1, Z0
	VMINPS        Z0, Z2, Z0
	VMINPS        Z0, Z3, Z0
	VMINPS        Z0, Z4, Z0
	VMINPS        Z0, Z5, Z0
	VMINPS        Z0, Z6, Z0
	VMINPS        Z0, Z7, Z0
	VMINPS        Z0, Z8, Z0
	VMINPS        Z0, Z9, Z0
	VMINPS        Z0, Z10, Z0
	VMINPS        Z0, Z11, Z0
	VMINPS        Z0, Z12, Z0
	VMINPS        Z0, Z13, Z0
	VMINPS        Z0, Z14, Z0
	VMINPS        Z0, Z15, Z0
	VMINPS        Z0, Z16, Z0
	VMINPS        Z0, Z17, Z0
	VMINPS        Z0, Z18, Z0
	VMINPS        Z0, Z19, Z0
	VMINPS        Z0, Z20, Z0
	VMINPS        Z0, Z21, Z0
	VMINPS        Z0, Z22, Z0
	VMINPS        Z0, Z23, Z0
	VMINPS        Z0, Z24, Z0
	VMINPS        Z0, Z25, Z0
	VMINPS        Z0, Z26, Z0
	VMINPS        Z0, Z27, Z0
	VMINPS        Z0, Z28, Z0
	VMINPS        Z0, Z29, Z0
	VMINPS        Z0, Z30, Z0
	VMINPS        Z0, Z31, Z0
	VEXTRACTF32X8 $0x01, Z0, Y1
	VMINPS        Y1, Y0, Y0
	SUBQ          $0x00000008, DX
	JB            float32MinDone1
	VMINPS        (AX), Y0, Y0
	ADDQ          $0x00000020, AX

float32MinDone1:
	VEXTRACTF128 $0x01, Y0, X1
	VMINPS       X1, X0, X0
	CMPQ         DX, $0x00000004
	JL           float32MinDone2
	VMINPS       (AX), X0, X0
	ADDQ         $0x00000020, AX

float32MinDone2:
	MOVOU X0, (CX)
	RET

// func float64MinAvx512Asm(x []float64, r []float64)
// Requires: AVX, AVX512F, SSE2
TEXT ·float64MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x7fefffffffffffff, BX
	MOVQ         BX, X0
	VBROADCASTSD X0, Z0
	VMOVUPD      Z0, Z1
	VMOVUPD      Z0, Z2
	VMOVUPD      Z0, Z3
	VMOVUPD      Z0, Z4
	VMOVUPD      Z0, Z5
	VMOVUPD      Z0, Z6
	VMOVUPD      Z0, Z7
	VMOVUPD      Z0, Z8
	VMOVUPD      Z0, Z9
	VMOVUPD      Z0, Z10
	VMOVUPD      Z0, Z11
	VMOVUPD      Z0, Z12
	VMOVUPD      Z0, Z13
	VMOVUPD      Z0, Z14
	VMOVUPD      Z0, Z15
	VMOVUPD      Z0, Z16
	VMOVUPD      Z0, Z17
	VMOVUPD      Z0, Z18
	VMOVUPD      Z0, Z19
	VMOVUPD      Z0, Z20
	VMOVUPD      Z0, Z21
	VMOVUPD      Z0, Z22
	VMOVUPD      Z0, Z23
	VMOVUPD      Z0, Z24
	VMOVUPD      Z0, Z25
	VMOVUPD      Z0, Z26
	VMOVUPD      Z0, Z27
	VMOVUPD      Z0, Z28
	VMOVUPD      Z0, Z29
	VMOVUPD      Z0, Z30
	VMOVUPD      Z0, Z31

float64MinBlockLoop:
	CMPQ   DX, $0x00000100
	JL     float64MinTailLoop
	VMINPD (AX), Z0, Z0
	VMINPD 64(AX), Z1, Z1
	VMINPD 128(AX), Z2, Z2
	VMINPD 192(AX), Z3, Z3
	VMINPD 256(AX), Z4, Z4
	VMINPD 320(AX), Z5, Z5
	VMINPD 384(AX), Z6, Z6
	VMINPD 448(AX), Z7, Z7
	VMINPD 512(AX), Z8, Z8
	VMINPD 576(AX), Z9, Z9
	VMINPD 640(AX), Z10, Z10
	VMINPD 704(AX), Z11, Z11
	VMINPD 768(AX), Z12, Z12
	VMINPD 832(AX), Z13, Z13
	VMINPD 896(AX), Z14, Z14
	VMINPD 960(AX), Z15, Z15
	VMINPD 1024(AX), Z16, Z16
	VMINPD 1088(AX), Z17, Z17
	VMINPD 1152(AX), Z18, Z18
	VMINPD 1216(AX), Z19, Z19
	VMINPD 1280(AX), Z20, Z20
	VMINPD 1344(AX), Z21, Z21
	VMINPD 1408(AX), Z22, Z22
	VMINPD 1472(AX), Z23, Z23
	VMINPD 1536(AX), Z24, Z24
	VMINPD 1600(AX), Z25, Z25
	VMINPD 1664(AX), Z26, Z26
	VMINPD 1728(AX), Z27, Z27
	VMINPD 1792(AX), Z28, Z28
	VMINPD 1856(AX), Z29, Z29
	VMINPD 1920(AX), Z30, Z30
	VMINPD 1984(AX), Z31, Z31
	ADDQ   $0x00000800, AX
	SUBQ   $0x00000100, DX
	JMP    float64MinBlockLoop

float64MinTailLoop:
	CMPQ   DX, $0x00000008
	JL     float64MinDone
	VMINPD (AX), Z0, Z0
	ADDQ   $0x00000040, AX
	SUBQ   $0x00000008, DX
	JMP    float64MinTailLoop

float64MinDone:
	VMINPD        Z0, Z1, Z0
	VMINPD        Z0, Z2, Z0
	VMINPD        Z0, Z3, Z0
	VMINPD        Z0, Z4, Z0
	VMINPD        Z0, Z5, Z0
	VMINPD        Z0, Z6, Z0
	VMINPD        Z0, Z7, Z0
	VMINPD        Z0, Z8, Z0
	VMINPD        Z0, Z9, Z0
	VMINPD        Z0, Z10, Z0
	VMINPD        Z0, Z11, Z0
	VMINPD        Z0, Z12, Z0
	VMINPD        Z0, Z13, Z0
	VMINPD        Z0, Z14, Z0
	VMINPD        Z0, Z15, Z0
	VMINPD        Z0, Z16, Z0
	VMINPD        Z0, Z17, Z0
	VMINPD        Z0, Z18, Z0
	VMINPD        Z0, Z19, Z0
	VMINPD        Z0, Z20, Z0
	VMINPD        Z0, Z21, Z0
	VMINPD        Z0, Z22, Z0
	VMINPD        Z0, Z23, Z0
	VMINPD        Z0, Z24, Z0
	VMINPD        Z0, Z25, Z0
	VMINPD        Z0, Z26, Z0
	VMINPD        Z0, Z27, Z0
	VMINPD        Z0, Z28, Z0
	VMINPD        Z0, Z29, Z0
	VMINPD        Z0, Z30, Z0
	VMINPD        Z0, Z31, Z0
	VEXTRACTF64X4 $0x01, Z0, Y1
	VMINPD        Y1, Y0, Y0
	SUBQ          $0x00000004, DX
	JB            float64MinDone1
	VMINPD        (AX), Y0, Y0
	ADDQ          $0x00000020, AX

float64MinDone1:
	VEXTRACTF128 $0x01, Y0, X1
	VMINPD       X1, X0, X0
	CMPQ         DX, $0x00000002
	JL           float64MinDone2
	VMINPD       (AX), X0, X0
	ADDQ         $0x00000020, AX

float64MinDone2:
	MOVOU X0, (CX)
	RET
