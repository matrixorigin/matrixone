// Code generated by command: go run avx512.go -out avx512.s -stubs avx512_stubs.go. DO NOT EDIT.

#include "textflag.h"

// func int8MinAvx512Asm(x []int8, r []int8)
// Requires: AVX, AVX2, AVX512BW, AVX512F, SSE2, SSE4.1
TEXT ·int8MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x000000000000007f, BX
	MOVQ         BX, X0
	VPBROADCASTB X0, Z0
	VMOVDQU64    Z0, Z1
	VMOVDQU64    Z0, Z2
	VMOVDQU64    Z0, Z3
	VMOVDQU64    Z0, Z4
	VMOVDQU64    Z0, Z5
	VMOVDQU64    Z0, Z6
	VMOVDQU64    Z0, Z7
	VMOVDQU64    Z0, Z8
	VMOVDQU64    Z0, Z9
	VMOVDQU64    Z0, Z10
	VMOVDQU64    Z0, Z11
	VMOVDQU64    Z0, Z0

int8MinBlockLoop:
	CMPQ    DX, $0x00000300
	JL      int8MinTailLoop
	VPMINSB (AX), Z1, Z1
	VPMINSB 64(AX), Z2, Z2
	VPMINSB 128(AX), Z3, Z3
	VPMINSB 192(AX), Z4, Z4
	VPMINSB 256(AX), Z5, Z5
	VPMINSB 320(AX), Z6, Z6
	VPMINSB 384(AX), Z7, Z7
	VPMINSB 448(AX), Z8, Z8
	VPMINSB 512(AX), Z9, Z9
	VPMINSB 576(AX), Z10, Z10
	VPMINSB 640(AX), Z11, Z11
	VPMINSB 704(AX), Z0, Z0
	ADDQ    $0x00000300, AX
	SUBQ    $0x00000300, DX
	JMP     int8MinBlockLoop

int8MinTailLoop:
	CMPQ    DX, $0x00000004
	JL      int8MinDone
	VPMINSB (AX), Z1, Z1
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000040, DX
	JMP     int8MinTailLoop

int8MinDone:
	VPMINSB       Z1, Z2, Z1
	VPMINSB       Z1, Z3, Z1
	VPMINSB       Z1, Z4, Z1
	VPMINSB       Z1, Z5, Z1
	VPMINSB       Z1, Z6, Z1
	VPMINSB       Z1, Z7, Z1
	VPMINSB       Z1, Z8, Z1
	VPMINSB       Z1, Z9, Z1
	VPMINSB       Z1, Z10, Z1
	VPMINSB       Z1, Z11, Z1
	VPMINSB       Z1, Z0, Z1
	VEXTRACTI64X4 $0x01, Z1, Y0
	VPMINSB       Y0, Y1, Y1
	VEXTRACTF128  $0x01, Y1, X0
	PMINSB        X0, X1
	MOVOU         X1, (CX)
	RET

// func int16MinAvx512Asm(x []int16, r []int16)
// Requires: AVX, AVX2, AVX512BW, AVX512F, SSE2
TEXT ·int16MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x0000000000007fff, BX
	MOVQ         BX, X0
	VPBROADCASTW X0, Z0
	VMOVDQU64    Z0, Z1
	VMOVDQU64    Z0, Z2
	VMOVDQU64    Z0, Z3
	VMOVDQU64    Z0, Z4
	VMOVDQU64    Z0, Z5
	VMOVDQU64    Z0, Z6
	VMOVDQU64    Z0, Z7
	VMOVDQU64    Z0, Z8
	VMOVDQU64    Z0, Z9
	VMOVDQU64    Z0, Z10
	VMOVDQU64    Z0, Z11
	VMOVDQU64    Z0, Z0

int16MinBlockLoop:
	CMPQ    DX, $0x00000180
	JL      int16MinTailLoop
	VPMINSW (AX), Z1, Z1
	VPMINSW 64(AX), Z2, Z2
	VPMINSW 128(AX), Z3, Z3
	VPMINSW 192(AX), Z4, Z4
	VPMINSW 256(AX), Z5, Z5
	VPMINSW 320(AX), Z6, Z6
	VPMINSW 384(AX), Z7, Z7
	VPMINSW 448(AX), Z8, Z8
	VPMINSW 512(AX), Z9, Z9
	VPMINSW 576(AX), Z10, Z10
	VPMINSW 640(AX), Z11, Z11
	VPMINSW 704(AX), Z0, Z0
	ADDQ    $0x00000300, AX
	SUBQ    $0x00000180, DX
	JMP     int16MinBlockLoop

int16MinTailLoop:
	CMPQ    DX, $0x00000004
	JL      int16MinDone
	VPMINSW (AX), Z1, Z1
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000020, DX
	JMP     int16MinTailLoop

int16MinDone:
	VPMINSW       Z1, Z2, Z1
	VPMINSW       Z1, Z3, Z1
	VPMINSW       Z1, Z4, Z1
	VPMINSW       Z1, Z5, Z1
	VPMINSW       Z1, Z6, Z1
	VPMINSW       Z1, Z7, Z1
	VPMINSW       Z1, Z8, Z1
	VPMINSW       Z1, Z9, Z1
	VPMINSW       Z1, Z10, Z1
	VPMINSW       Z1, Z11, Z1
	VPMINSW       Z1, Z0, Z1
	VEXTRACTI64X4 $0x01, Z1, Y0
	VPMINSW       Y0, Y1, Y1
	VEXTRACTF128  $0x01, Y1, X0
	PMINSW        X0, X1
	MOVOU         X1, (CX)
	RET

// func int32MinAvx512Asm(x []int32, r []int32)
// Requires: AVX, AVX2, AVX512F, SSE2, SSE4.1
TEXT ·int32MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x000000007fffffff, BX
	MOVQ         BX, X0
	VPBROADCASTD X0, Z0
	VMOVDQU64    Z0, Z1
	VMOVDQU64    Z0, Z2
	VMOVDQU64    Z0, Z3
	VMOVDQU64    Z0, Z4
	VMOVDQU64    Z0, Z5
	VMOVDQU64    Z0, Z6
	VMOVDQU64    Z0, Z7
	VMOVDQU64    Z0, Z8
	VMOVDQU64    Z0, Z9
	VMOVDQU64    Z0, Z10
	VMOVDQU64    Z0, Z11
	VMOVDQU64    Z0, Z0

int32MinBlockLoop:
	CMPQ    DX, $0x000000c0
	JL      int32MinTailLoop
	VPMINSD (AX), Z1, Z1
	VPMINSD 64(AX), Z2, Z2
	VPMINSD 128(AX), Z3, Z3
	VPMINSD 192(AX), Z4, Z4
	VPMINSD 256(AX), Z5, Z5
	VPMINSD 320(AX), Z6, Z6
	VPMINSD 384(AX), Z7, Z7
	VPMINSD 448(AX), Z8, Z8
	VPMINSD 512(AX), Z9, Z9
	VPMINSD 576(AX), Z10, Z10
	VPMINSD 640(AX), Z11, Z11
	VPMINSD 704(AX), Z0, Z0
	ADDQ    $0x00000300, AX
	SUBQ    $0x000000c0, DX
	JMP     int32MinBlockLoop

int32MinTailLoop:
	CMPQ    DX, $0x00000004
	JL      int32MinDone
	VPMINSD (AX), Z1, Z1
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000010, DX
	JMP     int32MinTailLoop

int32MinDone:
	VPMINSD       Z1, Z2, Z1
	VPMINSD       Z1, Z3, Z1
	VPMINSD       Z1, Z4, Z1
	VPMINSD       Z1, Z5, Z1
	VPMINSD       Z1, Z6, Z1
	VPMINSD       Z1, Z7, Z1
	VPMINSD       Z1, Z8, Z1
	VPMINSD       Z1, Z9, Z1
	VPMINSD       Z1, Z10, Z1
	VPMINSD       Z1, Z11, Z1
	VPMINSD       Z1, Z0, Z1
	VEXTRACTI64X4 $0x01, Z1, Y0
	VPMINSD       Y0, Y1, Y1
	VEXTRACTF128  $0x01, Y1, X0
	PMINSD        X0, X1
	MOVOU         X1, (CX)
	RET

// func int64MinAvx512Asm(x []int64, r []int64)
// Requires: AVX, AVX512F, AVX512VL, SSE2
TEXT ·int64MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x7fffffffffffffff, BX
	MOVQ         BX, X0
	VPBROADCASTQ X0, Z0
	VMOVDQU64    Z0, Z1
	VMOVDQU64    Z0, Z2
	VMOVDQU64    Z0, Z3
	VMOVDQU64    Z0, Z4
	VMOVDQU64    Z0, Z5
	VMOVDQU64    Z0, Z6
	VMOVDQU64    Z0, Z7
	VMOVDQU64    Z0, Z8
	VMOVDQU64    Z0, Z9
	VMOVDQU64    Z0, Z10
	VMOVDQU64    Z0, Z11
	VMOVDQU64    Z0, Z0

int64MinBlockLoop:
	CMPQ    DX, $0x00000060
	JL      int64MinTailLoop
	VPMINSQ (AX), Z1, Z1
	VPMINSQ 64(AX), Z2, Z2
	VPMINSQ 128(AX), Z3, Z3
	VPMINSQ 192(AX), Z4, Z4
	VPMINSQ 256(AX), Z5, Z5
	VPMINSQ 320(AX), Z6, Z6
	VPMINSQ 384(AX), Z7, Z7
	VPMINSQ 448(AX), Z8, Z8
	VPMINSQ 512(AX), Z9, Z9
	VPMINSQ 576(AX), Z10, Z10
	VPMINSQ 640(AX), Z11, Z11
	VPMINSQ 704(AX), Z0, Z0
	ADDQ    $0x00000300, AX
	SUBQ    $0x00000060, DX
	JMP     int64MinBlockLoop

int64MinTailLoop:
	CMPQ    DX, $0x00000004
	JL      int64MinDone
	VPMINSQ (AX), Z1, Z1
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000008, DX
	JMP     int64MinTailLoop

int64MinDone:
	VPMINSQ       Z1, Z2, Z1
	VPMINSQ       Z1, Z3, Z1
	VPMINSQ       Z1, Z4, Z1
	VPMINSQ       Z1, Z5, Z1
	VPMINSQ       Z1, Z6, Z1
	VPMINSQ       Z1, Z7, Z1
	VPMINSQ       Z1, Z8, Z1
	VPMINSQ       Z1, Z9, Z1
	VPMINSQ       Z1, Z10, Z1
	VPMINSQ       Z1, Z11, Z1
	VPMINSQ       Z1, Z0, Z1
	VEXTRACTI64X4 $0x01, Z1, Y0
	VPMINSQ       Y0, Y1, Y1
	VEXTRACTF128  $0x01, Y1, X0
	VPMINSQ       X0, X1, X1
	MOVOU         X1, (CX)
	RET

// func uint8MinAvx512Asm(x []uint8, r []uint8)
// Requires: AVX, AVX2, AVX512BW, AVX512F, SSE2
TEXT ·uint8MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ     x_base+0(FP), AX
	MOVQ     r_base+24(FP), CX
	MOVQ     x_len+8(FP), DX
	VPCMPEQQ Z0, Z0, Z0
	VPCMPEQQ Z1, Z1, Z1
	VPCMPEQQ Z2, Z2, Z2
	VPCMPEQQ Z3, Z3, Z3
	VPCMPEQQ Z4, Z4, Z4
	VPCMPEQQ Z5, Z5, Z5
	VPCMPEQQ Z6, Z6, Z6
	VPCMPEQQ Z7, Z7, Z7
	VPCMPEQQ Z8, Z8, Z8
	VPCMPEQQ Z9, Z9, Z9
	VPCMPEQQ Z10, Z10, Z10
	VPCMPEQQ Z11, Z11, Z11

uint8MinBlockLoop:
	CMPQ    DX, $0x00000300
	JL      uint8MinTailLoop
	VPMINUB (AX), Z1, Z1
	VPMINUB 64(AX), Z2, Z2
	VPMINUB 128(AX), Z3, Z3
	VPMINUB 192(AX), Z4, Z4
	VPMINUB 256(AX), Z5, Z5
	VPMINUB 320(AX), Z6, Z6
	VPMINUB 384(AX), Z7, Z7
	VPMINUB 448(AX), Z8, Z8
	VPMINUB 512(AX), Z9, Z9
	VPMINUB 576(AX), Z10, Z10
	VPMINUB 640(AX), Z11, Z11
	VPMINUB 704(AX), Z0, Z0
	ADDQ    $0x00000300, AX
	SUBQ    $0x00000300, DX
	JMP     uint8MinBlockLoop

uint8MinTailLoop:
	CMPQ    DX, $0x00000004
	JL      uint8MinDone
	VPMINUB (AX), Z1, Z1
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000040, DX
	JMP     uint8MinTailLoop

uint8MinDone:
	VPMINUB       Z1, Z2, Z1
	VPMINUB       Z1, Z3, Z1
	VPMINUB       Z1, Z4, Z1
	VPMINUB       Z1, Z5, Z1
	VPMINUB       Z1, Z6, Z1
	VPMINUB       Z1, Z7, Z1
	VPMINUB       Z1, Z8, Z1
	VPMINUB       Z1, Z9, Z1
	VPMINUB       Z1, Z10, Z1
	VPMINUB       Z1, Z11, Z1
	VPMINUB       Z1, Z0, Z1
	VEXTRACTI64X4 $0x01, Z1, Y0
	VPMINUB       Y0, Y1, Y1
	VEXTRACTF128  $0x01, Y1, X0
	PMINUB        X0, X1
	MOVOU         X1, (CX)
	RET

// func uint16MinAvx512Asm(x []uint16, r []uint16)
// Requires: AVX, AVX2, AVX512BW, AVX512F, SSE2, SSE4.1
TEXT ·uint16MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ     x_base+0(FP), AX
	MOVQ     r_base+24(FP), CX
	MOVQ     x_len+8(FP), DX
	VPCMPEQQ Z0, Z0, Z0
	VPCMPEQQ Z1, Z1, Z1
	VPCMPEQQ Z2, Z2, Z2
	VPCMPEQQ Z3, Z3, Z3
	VPCMPEQQ Z4, Z4, Z4
	VPCMPEQQ Z5, Z5, Z5
	VPCMPEQQ Z6, Z6, Z6
	VPCMPEQQ Z7, Z7, Z7
	VPCMPEQQ Z8, Z8, Z8
	VPCMPEQQ Z9, Z9, Z9
	VPCMPEQQ Z10, Z10, Z10
	VPCMPEQQ Z11, Z11, Z11

uint16MinBlockLoop:
	CMPQ    DX, $0x00000180
	JL      uint16MinTailLoop
	VPMINUW (AX), Z1, Z1
	VPMINUW 64(AX), Z2, Z2
	VPMINUW 128(AX), Z3, Z3
	VPMINUW 192(AX), Z4, Z4
	VPMINUW 256(AX), Z5, Z5
	VPMINUW 320(AX), Z6, Z6
	VPMINUW 384(AX), Z7, Z7
	VPMINUW 448(AX), Z8, Z8
	VPMINUW 512(AX), Z9, Z9
	VPMINUW 576(AX), Z10, Z10
	VPMINUW 640(AX), Z11, Z11
	VPMINUW 704(AX), Z0, Z0
	ADDQ    $0x00000300, AX
	SUBQ    $0x00000180, DX
	JMP     uint16MinBlockLoop

uint16MinTailLoop:
	CMPQ    DX, $0x00000004
	JL      uint16MinDone
	VPMINUW (AX), Z1, Z1
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000020, DX
	JMP     uint16MinTailLoop

uint16MinDone:
	VPMINUW       Z1, Z2, Z1
	VPMINUW       Z1, Z3, Z1
	VPMINUW       Z1, Z4, Z1
	VPMINUW       Z1, Z5, Z1
	VPMINUW       Z1, Z6, Z1
	VPMINUW       Z1, Z7, Z1
	VPMINUW       Z1, Z8, Z1
	VPMINUW       Z1, Z9, Z1
	VPMINUW       Z1, Z10, Z1
	VPMINUW       Z1, Z11, Z1
	VPMINUW       Z1, Z0, Z1
	VEXTRACTI64X4 $0x01, Z1, Y0
	VPMINUW       Y0, Y1, Y1
	VEXTRACTF128  $0x01, Y1, X0
	PMINUW        X0, X1
	MOVOU         X1, (CX)
	RET

// func uint32MinAvx512Asm(x []uint32, r []uint32)
// Requires: AVX, AVX2, AVX512F, SSE2, SSE4.1
TEXT ·uint32MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ     x_base+0(FP), AX
	MOVQ     r_base+24(FP), CX
	MOVQ     x_len+8(FP), DX
	VPCMPEQQ Z0, Z0, Z0
	VPCMPEQQ Z1, Z1, Z1
	VPCMPEQQ Z2, Z2, Z2
	VPCMPEQQ Z3, Z3, Z3
	VPCMPEQQ Z4, Z4, Z4
	VPCMPEQQ Z5, Z5, Z5
	VPCMPEQQ Z6, Z6, Z6
	VPCMPEQQ Z7, Z7, Z7
	VPCMPEQQ Z8, Z8, Z8
	VPCMPEQQ Z9, Z9, Z9
	VPCMPEQQ Z10, Z10, Z10
	VPCMPEQQ Z11, Z11, Z11

uint32MinBlockLoop:
	CMPQ    DX, $0x000000c0
	JL      uint32MinTailLoop
	VPMINUD (AX), Z1, Z1
	VPMINUD 64(AX), Z2, Z2
	VPMINUD 128(AX), Z3, Z3
	VPMINUD 192(AX), Z4, Z4
	VPMINUD 256(AX), Z5, Z5
	VPMINUD 320(AX), Z6, Z6
	VPMINUD 384(AX), Z7, Z7
	VPMINUD 448(AX), Z8, Z8
	VPMINUD 512(AX), Z9, Z9
	VPMINUD 576(AX), Z10, Z10
	VPMINUD 640(AX), Z11, Z11
	VPMINUD 704(AX), Z0, Z0
	ADDQ    $0x00000300, AX
	SUBQ    $0x000000c0, DX
	JMP     uint32MinBlockLoop

uint32MinTailLoop:
	CMPQ    DX, $0x00000004
	JL      uint32MinDone
	VPMINUD (AX), Z1, Z1
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000010, DX
	JMP     uint32MinTailLoop

uint32MinDone:
	VPMINUD       Z1, Z2, Z1
	VPMINUD       Z1, Z3, Z1
	VPMINUD       Z1, Z4, Z1
	VPMINUD       Z1, Z5, Z1
	VPMINUD       Z1, Z6, Z1
	VPMINUD       Z1, Z7, Z1
	VPMINUD       Z1, Z8, Z1
	VPMINUD       Z1, Z9, Z1
	VPMINUD       Z1, Z10, Z1
	VPMINUD       Z1, Z11, Z1
	VPMINUD       Z1, Z0, Z1
	VEXTRACTI64X4 $0x01, Z1, Y0
	VPMINUD       Y0, Y1, Y1
	VEXTRACTF128  $0x01, Y1, X0
	PMINUD        X0, X1
	MOVOU         X1, (CX)
	RET

// func uint64MinAvx512Asm(x []uint64, r []uint64)
// Requires: AVX, AVX512F, AVX512VL, SSE2
TEXT ·uint64MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ     x_base+0(FP), AX
	MOVQ     r_base+24(FP), CX
	MOVQ     x_len+8(FP), DX
	VPCMPEQQ Z0, Z0, Z0
	VPCMPEQQ Z1, Z1, Z1
	VPCMPEQQ Z2, Z2, Z2
	VPCMPEQQ Z3, Z3, Z3
	VPCMPEQQ Z4, Z4, Z4
	VPCMPEQQ Z5, Z5, Z5
	VPCMPEQQ Z6, Z6, Z6
	VPCMPEQQ Z7, Z7, Z7
	VPCMPEQQ Z8, Z8, Z8
	VPCMPEQQ Z9, Z9, Z9
	VPCMPEQQ Z10, Z10, Z10
	VPCMPEQQ Z11, Z11, Z11

uint64MinBlockLoop:
	CMPQ    DX, $0x00000060
	JL      uint64MinTailLoop
	VPMINUQ (AX), Z1, Z1
	VPMINUQ 64(AX), Z2, Z2
	VPMINUQ 128(AX), Z3, Z3
	VPMINUQ 192(AX), Z4, Z4
	VPMINUQ 256(AX), Z5, Z5
	VPMINUQ 320(AX), Z6, Z6
	VPMINUQ 384(AX), Z7, Z7
	VPMINUQ 448(AX), Z8, Z8
	VPMINUQ 512(AX), Z9, Z9
	VPMINUQ 576(AX), Z10, Z10
	VPMINUQ 640(AX), Z11, Z11
	VPMINUQ 704(AX), Z0, Z0
	ADDQ    $0x00000300, AX
	SUBQ    $0x00000060, DX
	JMP     uint64MinBlockLoop

uint64MinTailLoop:
	CMPQ    DX, $0x00000004
	JL      uint64MinDone
	VPMINUQ (AX), Z1, Z1
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000008, DX
	JMP     uint64MinTailLoop

uint64MinDone:
	VPMINUQ       Z1, Z2, Z1
	VPMINUQ       Z1, Z3, Z1
	VPMINUQ       Z1, Z4, Z1
	VPMINUQ       Z1, Z5, Z1
	VPMINUQ       Z1, Z6, Z1
	VPMINUQ       Z1, Z7, Z1
	VPMINUQ       Z1, Z8, Z1
	VPMINUQ       Z1, Z9, Z1
	VPMINUQ       Z1, Z10, Z1
	VPMINUQ       Z1, Z11, Z1
	VPMINUQ       Z1, Z0, Z1
	VEXTRACTI64X4 $0x01, Z1, Y0
	VPMINUQ       Y0, Y1, Y1
	VEXTRACTF128  $0x01, Y1, X0
	VPMINUQ       X0, X1, X1
	MOVOU         X1, (CX)
	RET

// func float32MinAvx512Asm(x []float32, r []float32)
// Requires: AVX, AVX512F, SSE, SSE2
TEXT ·float32MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x000000007f7fffff, BX
	MOVQ         BX, X0
	VBROADCASTSS X0, Z0
	VMOVUPS      Z0, Z1
	VMOVUPS      Z0, Z2
	VMOVUPS      Z0, Z3
	VMOVUPS      Z0, Z4
	VMOVUPS      Z0, Z5
	VMOVUPS      Z0, Z6
	VMOVUPS      Z0, Z7
	VMOVUPS      Z0, Z8
	VMOVUPS      Z0, Z9
	VMOVUPS      Z0, Z10
	VMOVUPS      Z0, Z11
	VMOVUPS      Z0, Z0

float32MinBlockLoop:
	CMPQ   DX, $0x000000c0
	JL     float32MinTailLoop
	VMINPS (AX), Z1, Z1
	VMINPS 64(AX), Z2, Z2
	VMINPS 128(AX), Z3, Z3
	VMINPS 192(AX), Z4, Z4
	VMINPS 256(AX), Z5, Z5
	VMINPS 320(AX), Z6, Z6
	VMINPS 384(AX), Z7, Z7
	VMINPS 448(AX), Z8, Z8
	VMINPS 512(AX), Z9, Z9
	VMINPS 576(AX), Z10, Z10
	VMINPS 640(AX), Z11, Z11
	VMINPS 704(AX), Z0, Z0
	ADDQ   $0x00000300, AX
	SUBQ   $0x000000c0, DX
	JMP    float32MinBlockLoop

float32MinTailLoop:
	CMPQ   DX, $0x00000004
	JL     float32MinDone
	VMINPS (AX), Z1, Z1
	ADDQ   $0x00000040, AX
	SUBQ   $0x00000010, DX
	JMP    float32MinTailLoop

float32MinDone:
	VMINPS        Z1, Z2, Z1
	VMINPS        Z1, Z3, Z1
	VMINPS        Z1, Z4, Z1
	VMINPS        Z1, Z5, Z1
	VMINPS        Z1, Z6, Z1
	VMINPS        Z1, Z7, Z1
	VMINPS        Z1, Z8, Z1
	VMINPS        Z1, Z9, Z1
	VMINPS        Z1, Z10, Z1
	VMINPS        Z1, Z11, Z1
	VMINPS        Z1, Z0, Z1
	VEXTRACTI64X4 $0x01, Z1, Y0
	VMINPS        Y0, Y1, Y1
	VEXTRACTF128  $0x01, Y1, X0
	MINPS         X0, X1
	MOVOU         X1, (CX)
	RET

// func float64MinAvx512Asm(x []float64, r []float64)
// Requires: AVX, AVX512F, SSE2
TEXT ·float64MinAvx512Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x7fefffffffffffff, BX
	MOVQ         BX, X0
	VBROADCASTSD X0, Z0
	VMOVUPD      Z0, Z1
	VMOVUPD      Z0, Z2
	VMOVUPD      Z0, Z3
	VMOVUPD      Z0, Z4
	VMOVUPD      Z0, Z5
	VMOVUPD      Z0, Z6
	VMOVUPD      Z0, Z7
	VMOVUPD      Z0, Z8
	VMOVUPD      Z0, Z9
	VMOVUPD      Z0, Z10
	VMOVUPD      Z0, Z11
	VMOVUPD      Z0, Z0

float64MinBlockLoop:
	CMPQ   DX, $0x00000060
	JL     float64MinTailLoop
	VMINPD (AX), Z1, Z1
	VMINPD 64(AX), Z2, Z2
	VMINPD 128(AX), Z3, Z3
	VMINPD 192(AX), Z4, Z4
	VMINPD 256(AX), Z5, Z5
	VMINPD 320(AX), Z6, Z6
	VMINPD 384(AX), Z7, Z7
	VMINPD 448(AX), Z8, Z8
	VMINPD 512(AX), Z9, Z9
	VMINPD 576(AX), Z10, Z10
	VMINPD 640(AX), Z11, Z11
	VMINPD 704(AX), Z0, Z0
	ADDQ   $0x00000300, AX
	SUBQ   $0x00000060, DX
	JMP    float64MinBlockLoop

float64MinTailLoop:
	CMPQ   DX, $0x00000004
	JL     float64MinDone
	VMINPD (AX), Z1, Z1
	ADDQ   $0x00000040, AX
	SUBQ   $0x00000008, DX
	JMP    float64MinTailLoop

float64MinDone:
	VMINPD        Z1, Z2, Z1
	VMINPD        Z1, Z3, Z1
	VMINPD        Z1, Z4, Z1
	VMINPD        Z1, Z5, Z1
	VMINPD        Z1, Z6, Z1
	VMINPD        Z1, Z7, Z1
	VMINPD        Z1, Z8, Z1
	VMINPD        Z1, Z9, Z1
	VMINPD        Z1, Z10, Z1
	VMINPD        Z1, Z11, Z1
	VMINPD        Z1, Z0, Z1
	VEXTRACTI64X4 $0x01, Z1, Y0
	VMINPD        Y0, Y1, Y1
	VEXTRACTF128  $0x01, Y1, X0
	MINPD         X0, X1
	MOVOU         X1, (CX)
	RET
