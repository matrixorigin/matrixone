// Code generated by command: go run avx2.go -out avx2.s -stubs avx2_stubs.go. DO NOT EDIT.

#include "textflag.h"

// func int8MinAvx2Asm(x []int8, r []int8)
// Requires: AVX, AVX2, SSE2
TEXT ·int8MinAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x000000000000007f, BX
	MOVQ         BX, X0
	VPBROADCASTB X0, Y0
	VMOVDQU      Y0, Y1
	VMOVDQU      Y0, Y2
	VMOVDQU      Y0, Y3
	VMOVDQU      Y0, Y4
	VMOVDQU      Y0, Y5
	VMOVDQU      Y0, Y6
	VMOVDQU      Y0, Y7
	VMOVDQU      Y0, Y8
	VMOVDQU      Y0, Y9
	VMOVDQU      Y0, Y10
	VMOVDQU      Y0, Y11
	VMOVDQU      Y0, Y12
	VMOVDQU      Y0, Y13
	VMOVDQU      Y0, Y14
	VMOVDQU      Y0, Y15

int8MinBlockLoop:
	CMPQ    DX, $0x00000200
	JL      int8MinTailLoop
	VPMINSB (AX), Y0, Y0
	VPMINSB 32(AX), Y1, Y1
	VPMINSB 64(AX), Y2, Y2
	VPMINSB 96(AX), Y3, Y3
	VPMINSB 128(AX), Y4, Y4
	VPMINSB 160(AX), Y5, Y5
	VPMINSB 192(AX), Y6, Y6
	VPMINSB 224(AX), Y7, Y7
	VPMINSB 256(AX), Y8, Y8
	VPMINSB 288(AX), Y9, Y9
	VPMINSB 320(AX), Y10, Y10
	VPMINSB 352(AX), Y11, Y11
	VPMINSB 384(AX), Y12, Y12
	VPMINSB 416(AX), Y13, Y13
	VPMINSB 448(AX), Y14, Y14
	VPMINSB 480(AX), Y15, Y15
	ADDQ    $0x00000200, AX
	SUBQ    $0x00000200, DX
	JMP     int8MinBlockLoop

int8MinTailLoop:
	CMPQ    DX, $0x00000020
	JL      int8MinDone
	VPMINSB (AX), Y0, Y0
	ADDQ    $0x00000020, AX
	SUBQ    $0x00000020, DX
	JMP     int8MinTailLoop

int8MinDone:
	VPMINSB      Y0, Y1, Y0
	VPMINSB      Y0, Y2, Y0
	VPMINSB      Y0, Y3, Y0
	VPMINSB      Y0, Y4, Y0
	VPMINSB      Y0, Y5, Y0
	VPMINSB      Y0, Y6, Y0
	VPMINSB      Y0, Y7, Y0
	VPMINSB      Y0, Y8, Y0
	VPMINSB      Y0, Y9, Y0
	VPMINSB      Y0, Y10, Y0
	VPMINSB      Y0, Y11, Y0
	VPMINSB      Y0, Y12, Y0
	VPMINSB      Y0, Y13, Y0
	VPMINSB      Y0, Y14, Y0
	VPMINSB      Y0, Y15, Y0
	VEXTRACTI128 $0x01, Y0, X1
	VPMINSB      X1, X0, X0
	CMPQ         DX, $0x00000010
	JL           int8MinDone1
	VPMINSB      (AX), X0, X0

int8MinDone1:
	MOVOU X0, (CX)
	RET

// func int16MinAvx2Asm(x []int16, r []int16)
// Requires: AVX, AVX2, SSE2
TEXT ·int16MinAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x0000000000007fff, BX
	MOVQ         BX, X0
	VPBROADCASTW X0, Y0
	VMOVDQU      Y0, Y1
	VMOVDQU      Y0, Y2
	VMOVDQU      Y0, Y3
	VMOVDQU      Y0, Y4
	VMOVDQU      Y0, Y5
	VMOVDQU      Y0, Y6
	VMOVDQU      Y0, Y7
	VMOVDQU      Y0, Y8
	VMOVDQU      Y0, Y9
	VMOVDQU      Y0, Y10
	VMOVDQU      Y0, Y11
	VMOVDQU      Y0, Y12
	VMOVDQU      Y0, Y13
	VMOVDQU      Y0, Y14
	VMOVDQU      Y0, Y15

int16MinBlockLoop:
	CMPQ    DX, $0x00000100
	JL      int16MinTailLoop
	VPMINSW (AX), Y0, Y0
	VPMINSW 32(AX), Y1, Y1
	VPMINSW 64(AX), Y2, Y2
	VPMINSW 96(AX), Y3, Y3
	VPMINSW 128(AX), Y4, Y4
	VPMINSW 160(AX), Y5, Y5
	VPMINSW 192(AX), Y6, Y6
	VPMINSW 224(AX), Y7, Y7
	VPMINSW 256(AX), Y8, Y8
	VPMINSW 288(AX), Y9, Y9
	VPMINSW 320(AX), Y10, Y10
	VPMINSW 352(AX), Y11, Y11
	VPMINSW 384(AX), Y12, Y12
	VPMINSW 416(AX), Y13, Y13
	VPMINSW 448(AX), Y14, Y14
	VPMINSW 480(AX), Y15, Y15
	ADDQ    $0x00000200, AX
	SUBQ    $0x00000100, DX
	JMP     int16MinBlockLoop

int16MinTailLoop:
	CMPQ    DX, $0x00000010
	JL      int16MinDone
	VPMINSW (AX), Y0, Y0
	ADDQ    $0x00000020, AX
	SUBQ    $0x00000010, DX
	JMP     int16MinTailLoop

int16MinDone:
	VPMINSW      Y0, Y1, Y0
	VPMINSW      Y0, Y2, Y0
	VPMINSW      Y0, Y3, Y0
	VPMINSW      Y0, Y4, Y0
	VPMINSW      Y0, Y5, Y0
	VPMINSW      Y0, Y6, Y0
	VPMINSW      Y0, Y7, Y0
	VPMINSW      Y0, Y8, Y0
	VPMINSW      Y0, Y9, Y0
	VPMINSW      Y0, Y10, Y0
	VPMINSW      Y0, Y11, Y0
	VPMINSW      Y0, Y12, Y0
	VPMINSW      Y0, Y13, Y0
	VPMINSW      Y0, Y14, Y0
	VPMINSW      Y0, Y15, Y0
	VEXTRACTI128 $0x01, Y0, X1
	VPMINSW      X1, X0, X0
	CMPQ         DX, $0x00000008
	JL           int16MinDone1
	VPMINSW      (AX), X0, X0

int16MinDone1:
	MOVOU X0, (CX)
	RET

// func int32MinAvx2Asm(x []int32, r []int32)
// Requires: AVX, AVX2, SSE2
TEXT ·int32MinAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x000000007fffffff, BX
	MOVQ         BX, X0
	VPBROADCASTD X0, Y0
	VMOVDQU      Y0, Y1
	VMOVDQU      Y0, Y2
	VMOVDQU      Y0, Y3
	VMOVDQU      Y0, Y4
	VMOVDQU      Y0, Y5
	VMOVDQU      Y0, Y6
	VMOVDQU      Y0, Y7
	VMOVDQU      Y0, Y8
	VMOVDQU      Y0, Y9
	VMOVDQU      Y0, Y10
	VMOVDQU      Y0, Y11
	VMOVDQU      Y0, Y12
	VMOVDQU      Y0, Y13
	VMOVDQU      Y0, Y14
	VMOVDQU      Y0, Y15

int32MinBlockLoop:
	CMPQ    DX, $0x00000080
	JL      int32MinTailLoop
	VPMINSD (AX), Y0, Y0
	VPMINSD 32(AX), Y1, Y1
	VPMINSD 64(AX), Y2, Y2
	VPMINSD 96(AX), Y3, Y3
	VPMINSD 128(AX), Y4, Y4
	VPMINSD 160(AX), Y5, Y5
	VPMINSD 192(AX), Y6, Y6
	VPMINSD 224(AX), Y7, Y7
	VPMINSD 256(AX), Y8, Y8
	VPMINSD 288(AX), Y9, Y9
	VPMINSD 320(AX), Y10, Y10
	VPMINSD 352(AX), Y11, Y11
	VPMINSD 384(AX), Y12, Y12
	VPMINSD 416(AX), Y13, Y13
	VPMINSD 448(AX), Y14, Y14
	VPMINSD 480(AX), Y15, Y15
	ADDQ    $0x00000200, AX
	SUBQ    $0x00000080, DX
	JMP     int32MinBlockLoop

int32MinTailLoop:
	CMPQ    DX, $0x00000008
	JL      int32MinDone
	VPMINSD (AX), Y0, Y0
	ADDQ    $0x00000020, AX
	SUBQ    $0x00000008, DX
	JMP     int32MinTailLoop

int32MinDone:
	VPMINSD      Y0, Y1, Y0
	VPMINSD      Y0, Y2, Y0
	VPMINSD      Y0, Y3, Y0
	VPMINSD      Y0, Y4, Y0
	VPMINSD      Y0, Y5, Y0
	VPMINSD      Y0, Y6, Y0
	VPMINSD      Y0, Y7, Y0
	VPMINSD      Y0, Y8, Y0
	VPMINSD      Y0, Y9, Y0
	VPMINSD      Y0, Y10, Y0
	VPMINSD      Y0, Y11, Y0
	VPMINSD      Y0, Y12, Y0
	VPMINSD      Y0, Y13, Y0
	VPMINSD      Y0, Y14, Y0
	VPMINSD      Y0, Y15, Y0
	VEXTRACTI128 $0x01, Y0, X1
	VPMINSD      X1, X0, X0
	CMPQ         DX, $0x00000004
	JL           int32MinDone1
	VPMINSD      (AX), X0, X0

int32MinDone1:
	MOVOU X0, (CX)
	RET

// func uint8MinAvx2Asm(x []uint8, r []uint8)
// Requires: AVX, AVX2, SSE2
TEXT ·uint8MinAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ     x_base+0(FP), AX
	MOVQ     r_base+24(FP), CX
	MOVQ     x_len+8(FP), DX
	VPCMPEQD Y0, Y0, Y0
	VPCMPEQD Y1, Y1, Y1
	VPCMPEQD Y2, Y2, Y2
	VPCMPEQD Y3, Y3, Y3
	VPCMPEQD Y4, Y4, Y4
	VPCMPEQD Y5, Y5, Y5
	VPCMPEQD Y6, Y6, Y6
	VPCMPEQD Y7, Y7, Y7
	VPCMPEQD Y8, Y8, Y8
	VPCMPEQD Y9, Y9, Y9
	VPCMPEQD Y10, Y10, Y10
	VPCMPEQD Y11, Y11, Y11
	VPCMPEQD Y12, Y12, Y12
	VPCMPEQD Y13, Y13, Y13
	VPCMPEQD Y14, Y14, Y14
	VPCMPEQD Y15, Y15, Y15

uint8MinBlockLoop:
	CMPQ    DX, $0x00000200
	JL      uint8MinTailLoop
	VPMINUB (AX), Y0, Y0
	VPMINUB 32(AX), Y1, Y1
	VPMINUB 64(AX), Y2, Y2
	VPMINUB 96(AX), Y3, Y3
	VPMINUB 128(AX), Y4, Y4
	VPMINUB 160(AX), Y5, Y5
	VPMINUB 192(AX), Y6, Y6
	VPMINUB 224(AX), Y7, Y7
	VPMINUB 256(AX), Y8, Y8
	VPMINUB 288(AX), Y9, Y9
	VPMINUB 320(AX), Y10, Y10
	VPMINUB 352(AX), Y11, Y11
	VPMINUB 384(AX), Y12, Y12
	VPMINUB 416(AX), Y13, Y13
	VPMINUB 448(AX), Y14, Y14
	VPMINUB 480(AX), Y15, Y15
	ADDQ    $0x00000200, AX
	SUBQ    $0x00000200, DX
	JMP     uint8MinBlockLoop

uint8MinTailLoop:
	CMPQ    DX, $0x00000020
	JL      uint8MinDone
	VPMINUB (AX), Y0, Y0
	ADDQ    $0x00000020, AX
	SUBQ    $0x00000020, DX
	JMP     uint8MinTailLoop

uint8MinDone:
	VPMINUB      Y0, Y1, Y0
	VPMINUB      Y0, Y2, Y0
	VPMINUB      Y0, Y3, Y0
	VPMINUB      Y0, Y4, Y0
	VPMINUB      Y0, Y5, Y0
	VPMINUB      Y0, Y6, Y0
	VPMINUB      Y0, Y7, Y0
	VPMINUB      Y0, Y8, Y0
	VPMINUB      Y0, Y9, Y0
	VPMINUB      Y0, Y10, Y0
	VPMINUB      Y0, Y11, Y0
	VPMINUB      Y0, Y12, Y0
	VPMINUB      Y0, Y13, Y0
	VPMINUB      Y0, Y14, Y0
	VPMINUB      Y0, Y15, Y0
	VEXTRACTI128 $0x01, Y0, X1
	VPMINUB      X1, X0, X0
	CMPQ         DX, $0x00000010
	JL           uint8MinDone1
	VPMINUB      (AX), X0, X0

uint8MinDone1:
	MOVOU X0, (CX)
	RET

// func uint16MinAvx2Asm(x []uint16, r []uint16)
// Requires: AVX, AVX2, SSE2
TEXT ·uint16MinAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ     x_base+0(FP), AX
	MOVQ     r_base+24(FP), CX
	MOVQ     x_len+8(FP), DX
	VPCMPEQD Y0, Y0, Y0
	VPCMPEQD Y1, Y1, Y1
	VPCMPEQD Y2, Y2, Y2
	VPCMPEQD Y3, Y3, Y3
	VPCMPEQD Y4, Y4, Y4
	VPCMPEQD Y5, Y5, Y5
	VPCMPEQD Y6, Y6, Y6
	VPCMPEQD Y7, Y7, Y7
	VPCMPEQD Y8, Y8, Y8
	VPCMPEQD Y9, Y9, Y9
	VPCMPEQD Y10, Y10, Y10
	VPCMPEQD Y11, Y11, Y11
	VPCMPEQD Y12, Y12, Y12
	VPCMPEQD Y13, Y13, Y13
	VPCMPEQD Y14, Y14, Y14
	VPCMPEQD Y15, Y15, Y15

uint16MinBlockLoop:
	CMPQ    DX, $0x00000100
	JL      uint16MinTailLoop
	VPMINUW (AX), Y0, Y0
	VPMINUW 32(AX), Y1, Y1
	VPMINUW 64(AX), Y2, Y2
	VPMINUW 96(AX), Y3, Y3
	VPMINUW 128(AX), Y4, Y4
	VPMINUW 160(AX), Y5, Y5
	VPMINUW 192(AX), Y6, Y6
	VPMINUW 224(AX), Y7, Y7
	VPMINUW 256(AX), Y8, Y8
	VPMINUW 288(AX), Y9, Y9
	VPMINUW 320(AX), Y10, Y10
	VPMINUW 352(AX), Y11, Y11
	VPMINUW 384(AX), Y12, Y12
	VPMINUW 416(AX), Y13, Y13
	VPMINUW 448(AX), Y14, Y14
	VPMINUW 480(AX), Y15, Y15
	ADDQ    $0x00000200, AX
	SUBQ    $0x00000100, DX
	JMP     uint16MinBlockLoop

uint16MinTailLoop:
	CMPQ    DX, $0x00000010
	JL      uint16MinDone
	VPMINUW (AX), Y0, Y0
	ADDQ    $0x00000020, AX
	SUBQ    $0x00000010, DX
	JMP     uint16MinTailLoop

uint16MinDone:
	VPMINUW      Y0, Y1, Y0
	VPMINUW      Y0, Y2, Y0
	VPMINUW      Y0, Y3, Y0
	VPMINUW      Y0, Y4, Y0
	VPMINUW      Y0, Y5, Y0
	VPMINUW      Y0, Y6, Y0
	VPMINUW      Y0, Y7, Y0
	VPMINUW      Y0, Y8, Y0
	VPMINUW      Y0, Y9, Y0
	VPMINUW      Y0, Y10, Y0
	VPMINUW      Y0, Y11, Y0
	VPMINUW      Y0, Y12, Y0
	VPMINUW      Y0, Y13, Y0
	VPMINUW      Y0, Y14, Y0
	VPMINUW      Y0, Y15, Y0
	VEXTRACTI128 $0x01, Y0, X1
	VPMINUW      X1, X0, X0
	CMPQ         DX, $0x00000008
	JL           uint16MinDone1
	VPMINUW      (AX), X0, X0

uint16MinDone1:
	MOVOU X0, (CX)
	RET

// func uint32MinAvx2Asm(x []uint32, r []uint32)
// Requires: AVX, AVX2, SSE2
TEXT ·uint32MinAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ     x_base+0(FP), AX
	MOVQ     r_base+24(FP), CX
	MOVQ     x_len+8(FP), DX
	VPCMPEQD Y0, Y0, Y0
	VPCMPEQD Y1, Y1, Y1
	VPCMPEQD Y2, Y2, Y2
	VPCMPEQD Y3, Y3, Y3
	VPCMPEQD Y4, Y4, Y4
	VPCMPEQD Y5, Y5, Y5
	VPCMPEQD Y6, Y6, Y6
	VPCMPEQD Y7, Y7, Y7
	VPCMPEQD Y8, Y8, Y8
	VPCMPEQD Y9, Y9, Y9
	VPCMPEQD Y10, Y10, Y10
	VPCMPEQD Y11, Y11, Y11
	VPCMPEQD Y12, Y12, Y12
	VPCMPEQD Y13, Y13, Y13
	VPCMPEQD Y14, Y14, Y14
	VPCMPEQD Y15, Y15, Y15

uint32MinBlockLoop:
	CMPQ    DX, $0x00000080
	JL      uint32MinTailLoop
	VPMINUD (AX), Y0, Y0
	VPMINUD 32(AX), Y1, Y1
	VPMINUD 64(AX), Y2, Y2
	VPMINUD 96(AX), Y3, Y3
	VPMINUD 128(AX), Y4, Y4
	VPMINUD 160(AX), Y5, Y5
	VPMINUD 192(AX), Y6, Y6
	VPMINUD 224(AX), Y7, Y7
	VPMINUD 256(AX), Y8, Y8
	VPMINUD 288(AX), Y9, Y9
	VPMINUD 320(AX), Y10, Y10
	VPMINUD 352(AX), Y11, Y11
	VPMINUD 384(AX), Y12, Y12
	VPMINUD 416(AX), Y13, Y13
	VPMINUD 448(AX), Y14, Y14
	VPMINUD 480(AX), Y15, Y15
	ADDQ    $0x00000200, AX
	SUBQ    $0x00000080, DX
	JMP     uint32MinBlockLoop

uint32MinTailLoop:
	CMPQ    DX, $0x00000008
	JL      uint32MinDone
	VPMINUD (AX), Y0, Y0
	ADDQ    $0x00000020, AX
	SUBQ    $0x00000008, DX
	JMP     uint32MinTailLoop

uint32MinDone:
	VPMINUD      Y0, Y1, Y0
	VPMINUD      Y0, Y2, Y0
	VPMINUD      Y0, Y3, Y0
	VPMINUD      Y0, Y4, Y0
	VPMINUD      Y0, Y5, Y0
	VPMINUD      Y0, Y6, Y0
	VPMINUD      Y0, Y7, Y0
	VPMINUD      Y0, Y8, Y0
	VPMINUD      Y0, Y9, Y0
	VPMINUD      Y0, Y10, Y0
	VPMINUD      Y0, Y11, Y0
	VPMINUD      Y0, Y12, Y0
	VPMINUD      Y0, Y13, Y0
	VPMINUD      Y0, Y14, Y0
	VPMINUD      Y0, Y15, Y0
	VEXTRACTI128 $0x01, Y0, X1
	VPMINUD      X1, X0, X0
	CMPQ         DX, $0x00000004
	JL           uint32MinDone1
	VPMINUD      (AX), X0, X0

uint32MinDone1:
	MOVOU X0, (CX)
	RET

// func float32MinAvx2Asm(x []float32, r []float32)
// Requires: AVX, AVX2, SSE2
TEXT ·float32MinAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x000000007f7fffff, BX
	MOVQ         BX, X0
	VBROADCASTSS X0, Y0
	VMOVUPS      Y0, Y1
	VMOVUPS      Y0, Y2
	VMOVUPS      Y0, Y3
	VMOVUPS      Y0, Y4
	VMOVUPS      Y0, Y5
	VMOVUPS      Y0, Y6
	VMOVUPS      Y0, Y7
	VMOVUPS      Y0, Y8
	VMOVUPS      Y0, Y9
	VMOVUPS      Y0, Y10
	VMOVUPS      Y0, Y11
	VMOVUPS      Y0, Y12
	VMOVUPS      Y0, Y13
	VMOVUPS      Y0, Y14
	VMOVUPS      Y0, Y15

float32MinBlockLoop:
	CMPQ   DX, $0x00000080
	JL     float32MinTailLoop
	VMINPS (AX), Y0, Y0
	VMINPS 32(AX), Y1, Y1
	VMINPS 64(AX), Y2, Y2
	VMINPS 96(AX), Y3, Y3
	VMINPS 128(AX), Y4, Y4
	VMINPS 160(AX), Y5, Y5
	VMINPS 192(AX), Y6, Y6
	VMINPS 224(AX), Y7, Y7
	VMINPS 256(AX), Y8, Y8
	VMINPS 288(AX), Y9, Y9
	VMINPS 320(AX), Y10, Y10
	VMINPS 352(AX), Y11, Y11
	VMINPS 384(AX), Y12, Y12
	VMINPS 416(AX), Y13, Y13
	VMINPS 448(AX), Y14, Y14
	VMINPS 480(AX), Y15, Y15
	ADDQ   $0x00000200, AX
	SUBQ   $0x00000080, DX
	JMP    float32MinBlockLoop

float32MinTailLoop:
	CMPQ   DX, $0x00000008
	JL     float32MinDone
	VMINPS (AX), Y0, Y0
	ADDQ   $0x00000020, AX
	SUBQ   $0x00000008, DX
	JMP    float32MinTailLoop

float32MinDone:
	VMINPS       Y0, Y1, Y0
	VMINPS       Y0, Y2, Y0
	VMINPS       Y0, Y3, Y0
	VMINPS       Y0, Y4, Y0
	VMINPS       Y0, Y5, Y0
	VMINPS       Y0, Y6, Y0
	VMINPS       Y0, Y7, Y0
	VMINPS       Y0, Y8, Y0
	VMINPS       Y0, Y9, Y0
	VMINPS       Y0, Y10, Y0
	VMINPS       Y0, Y11, Y0
	VMINPS       Y0, Y12, Y0
	VMINPS       Y0, Y13, Y0
	VMINPS       Y0, Y14, Y0
	VMINPS       Y0, Y15, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VMINPS       X1, X0, X0
	CMPQ         DX, $0x00000004
	JL           float32MinDone1
	VMINPS       (AX), X0, X0

float32MinDone1:
	MOVOU X0, (CX)
	RET

// func float64MinAvx2Asm(x []float64, r []float64)
// Requires: AVX, AVX2, SSE2
TEXT ·float64MinAvx2Asm(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), AX
	MOVQ         r_base+24(FP), CX
	MOVQ         x_len+8(FP), DX
	MOVQ         $0x7fefffffffffffff, BX
	MOVQ         BX, X0
	VBROADCASTSD X0, Y0
	VMOVUPD      Y0, Y1
	VMOVUPD      Y0, Y2
	VMOVUPD      Y0, Y3
	VMOVUPD      Y0, Y4
	VMOVUPD      Y0, Y5
	VMOVUPD      Y0, Y6
	VMOVUPD      Y0, Y7
	VMOVUPD      Y0, Y8
	VMOVUPD      Y0, Y9
	VMOVUPD      Y0, Y10
	VMOVUPD      Y0, Y11
	VMOVUPD      Y0, Y12
	VMOVUPD      Y0, Y13
	VMOVUPD      Y0, Y14
	VMOVUPD      Y0, Y15

float64MinBlockLoop:
	CMPQ   DX, $0x00000040
	JL     float64MinTailLoop
	VMINPD (AX), Y0, Y0
	VMINPD 32(AX), Y1, Y1
	VMINPD 64(AX), Y2, Y2
	VMINPD 96(AX), Y3, Y3
	VMINPD 128(AX), Y4, Y4
	VMINPD 160(AX), Y5, Y5
	VMINPD 192(AX), Y6, Y6
	VMINPD 224(AX), Y7, Y7
	VMINPD 256(AX), Y8, Y8
	VMINPD 288(AX), Y9, Y9
	VMINPD 320(AX), Y10, Y10
	VMINPD 352(AX), Y11, Y11
	VMINPD 384(AX), Y12, Y12
	VMINPD 416(AX), Y13, Y13
	VMINPD 448(AX), Y14, Y14
	VMINPD 480(AX), Y15, Y15
	ADDQ   $0x00000200, AX
	SUBQ   $0x00000040, DX
	JMP    float64MinBlockLoop

float64MinTailLoop:
	CMPQ   DX, $0x00000004
	JL     float64MinDone
	VMINPD (AX), Y0, Y0
	ADDQ   $0x00000020, AX
	SUBQ   $0x00000004, DX
	JMP    float64MinTailLoop

float64MinDone:
	VMINPD       Y0, Y1, Y0
	VMINPD       Y0, Y2, Y0
	VMINPD       Y0, Y3, Y0
	VMINPD       Y0, Y4, Y0
	VMINPD       Y0, Y5, Y0
	VMINPD       Y0, Y6, Y0
	VMINPD       Y0, Y7, Y0
	VMINPD       Y0, Y8, Y0
	VMINPD       Y0, Y9, Y0
	VMINPD       Y0, Y10, Y0
	VMINPD       Y0, Y11, Y0
	VMINPD       Y0, Y12, Y0
	VMINPD       Y0, Y13, Y0
	VMINPD       Y0, Y14, Y0
	VMINPD       Y0, Y15, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VMINPD       X1, X0, X0
	CMPQ         DX, $0x00000002
	JL           float64MinDone1
	VMINPD       (AX), X0, X0

float64MinDone1:
	MOVOU X0, (CX)
	RET
